import json

# Charger le notebook
with open('ALPHABOT_ML_TRAINING_COLAB.ipynb', 'r', encoding='utf-8') as f:
    nb = json.load(f)

# Trouver et corriger la cellule 5 pour l'erreur d'indentation
for cell in nb['cells']:
    if cell['cell_type'] == 'code' and 'CELLULE 5: Entra√Ænement Pattern Detector' in ''.join(cell['source']):
        source = cell['source']
        
        # Recr√©er compl√®tement la cellule 5 avec une indentation correcte
        new_cell_content = [
            "print(\"üß† Entra√Ænement du Pattern Detector (LSTM + CNN)...\")\n",
            "\n",
            "# Importer les biblioth√®ques n√©cessaires\n",
            "import tensorflow as tf\n",
            "from tensorflow import keras\n",
            "from sklearn.preprocessing import StandardScaler\n",
            "import numpy as np\n",
            "import matplotlib.pyplot as plt\n",
            "import os\n",
            "import pickle\n",
            "from datetime import datetime, timedelta\n",
            "import yfinance as yf\n",
            "\n",
            "# Charger les donn√©es depuis la cellule pr√©c√©dente\n",
            "try:\n",
            "    # Essayer de charger depuis le pickle sauvegard√©\n",
            "    with open(f'{base_path}/data/market_data.pkl', 'rb') as f:\n",
            "        all_data = pickle.load(f)\n",
            "    print(\"‚úÖ Donn√©es charg√©es depuis le pickle\")\n",
            "except:\n",
            "    print(\"üîß Re-t√©l√©chargement des donn√©es...\")\n",
            "    # Re-t√©l√©charger les donn√©es si le pickle n'est pas disponible\n",
            "    symbols = ['AAPL', 'GOOGL', 'MSFT', 'TSLA', 'AMZN']\n",
            "    end_date = datetime.now().strftime('%Y-%m-%d')\n",
            "    start_date = (datetime.now() - timedelta(days=730)).strftime('%Y-%m-%d')\n",
            "    \n",
            "    all_data = {}\n",
            "    for symbol in symbols:\n",
            "        print(f\"üì• T√©l√©chargement des donn√©es pour {symbol}...\")\n",
            "        data = yf.download(symbol, start=start_date, end=end_date, auto_adjust=False)\n",
            "        if not data.empty:\n",
            "            all_data[symbol] = data\n",
            "            print(f\"‚úÖ {symbol}: {len(data)} jours de donn√©es\")\n",
            "        else:\n",
            "            print(f\"‚ö†Ô∏è {symbol}: Pas de donn√©es disponibles\")\n",
            "    \n",
            "    # Sauvegarder pour √©viter de re-t√©l√©charger\n",
            "    try:\n",
            "        os.makedirs(f'{base_path}/data', exist_ok=True)\n",
            "        with open(f'{base_path}/data/market_data.pkl', 'wb') as f:\n",
            "            pickle.dump(all_data, f)\n",
            "        print(f\"üíæ Donn√©es sauvegard√©es dans: {base_path}/data/market_data.pkl\")\n",
            "    except Exception as e:\n",
            "        print(f\"‚ö†Ô∏è Erreur de sauvegarde: {e}\")\n",
            "\n",
            "print(f\"üìä Total symboles: {len(all_data)}\")\n",
            "\n",
            "# Pr√©parer les donn√©es\n",
            "def prepare_pattern_training_data(all_data):\n",
            "    X_train = []\n",
            "    y_train = []\n",
            "    \n",
            "    for symbol, data in all_data.items():\n",
            "        if len(data) < 31:  # Besoin d'au moins 30 jours + 1 pour le label\n",
            "            continue\n",
            "            \n",
            "        # Cr√©er des s√©quences de 30 jours\n",
            "        for i in range(len(data) - 30):\n",
            "            sequence = data.iloc[i:i+30]\n",
            "            \n",
            "            # Features: Close, Volume, High-Low spread\n",
            "            features = []\n",
            "            features.append(sequence['Close'].values)\n",
            "            features.append(sequence['Volume'].values)\n",
            "            features.append((sequence['High'] - sequence['Low']).values)\n",
            "            \n",
            "            X_train.append(np.column_stack(features))\n",
            "            \n",
            "            # Label: tendance des 5 prochains jours\n",
            "            future_prices = data.iloc[i+30:i+35]['Close']\n",
            "            current_price = data.iloc[i+29]['Close']\n",
            "            future_return = (future_prices.mean() - current_price) / current_price\n",
            "            \n",
            "            if future_return > 0.02:  # Hausse > 2%\n",
            "                y_train.append(2)  # Buy\n",
            "            elif future_return < -0.02:  # Baisse > 2%\n",
            "                y_train.append(0)  # Sell\n",
            "            else:\n",
            "                y_train.append(1)  # Hold\n",
            "    \n",
            "    return np.array(X_train), np.array(y_train)\n",
            "\n",
            "X_train, y_train = prepare_pattern_training_data(all_data)\n",
            "print(f\"üìä Donn√©es pr√©par√©es: {X_train.shape[0]} √©chantillons\")\n",
            "\n",
            "# Cr√©er le mod√®le GPU optimis√© pour A100\n",
            "print(\"üîß Cr√©ation du mod√®le GPU optimis√© pour A100...\")\n",
            "\n",
            "# V√©rifier la disponibilit√© du GPU\n",
            "print(f\"üìä GPU disponible: {tf.config.list_physical_devices('GPU')}\")\n",
            "\n",
            "# Configuration simple pour √©viter les crashs\n",
            "# tf.keras.mixed_precision.set_global_policy('mixed_float16')  # D√©sactiv√© pour √©viter les crashs\n",
            "\n",
            "# Cr√©er le mod√®le\n",
            "strategy = tf.distribute.get_strategy()\n",
            "\n",
            "with strategy.scope():\n",
            "    # Utiliser un mod√®le plus simple pour √©viter les crashs GPU\n",
            "    inputs = tf.keras.Input(shape=(30, 3), name='input_layer')\n",
            "    \n",
            "    # Normalisation\n",
            "    x = tf.keras.layers.BatchNormalization()(inputs)\n",
            "    \n",
            "    # Une seule couche LSTM\n",
            "    x = tf.keras.layers.LSTM(\n",
            "        64, \n",
            "        return_sequences=False,\n",
            "        kernel_initializer='glorot_uniform',\n",
            "        recurrent_initializer='orthogonal',\n",
            "        name='lstm_main'\n",
            "    )(x)\n",
            "    x = tf.keras.layers.Dropout(0.3)(x)\n",
            "    \n",
            "    # Couches denses\n",
            "    x = tf.keras.layers.Dense(32, activation='relu')(x)\n",
            "    x = tf.keras.layers.BatchNormalization()(x)\n",
            "    x = tf.keras.layers.Dropout(0.3)(x)\n",
            "    \n",
            "    outputs = tf.keras.layers.Dense(3, activation='softmax', name='output')(x)\n",
            "    \n",
            "    # Cr√©er le mod√®le\n",
            "    model = tf.keras.Model(inputs=inputs, outputs=outputs, name='simplified_gpu_model')\n",
            "    \n",
            "    # Compiler avec des param√®tres simples\n",
            "    model.compile(\n",
            "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
            "        loss='sparse_categorical_crossentropy',\n",
            "        metrics=['accuracy']\n",
            "    )\n",
            "\n",
            "# Afficher le r√©sum√© du mod√®le\n",
            "print(\"‚úÖ Mod√®le GPU optimis√© cr√©√©:\")\n",
            "model.summary()\n",
            "\n",
            "# Callbacks simplifi√©s pour √©viter les crashs\n",
            "callbacks = [\n",
            "    tf.keras.callbacks.EarlyStopping(\n",
            "        patience=10, \n",
            "        restore_best_weights=True,\n",
            "        monitor='val_loss',\n",
            "        verbose=1\n",
            "    ),\n",
            "    tf.keras.callbacks.ReduceLROnPlateau(\n",
            "        factor=0.5, \n",
            "        patience=5, \n",
            "        monitor='val_loss',\n",
            "        verbose=1\n",
            "    )\n",
            "]\n",
            "\n",
            "# V√©rifier et pr√©parer les donn√©es\n",
            "print(f\"üìä V√©rification des donn√©es:\")\n",
            "print(f\"  - X_train shape: {X_train.shape}\")\n",
            "print(f\"  - y_train shape: {y_train.shape}\")\n",
            "print(f\"  - X_train dtype: {X_train.dtype}\")\n",
            "print(f\"  - y_train dtype: {y_train.dtype}\")\n",
            "print(f\"  - Valeurs uniques dans y_train: {np.unique(y_train)}\")\n",
            "\n",
            "# S'assurer que les donn√©es sont du bon type pour GPU\n",
            "X_train = X_train.astype(np.float32)\n",
            "y_train = y_train.astype(np.int32)\n",
            "\n",
            "# Normaliser les donn√©es pour GPU\n",
            "scaler = StandardScaler()\n",
            "X_train_scaled = scaler.fit_transform(X_train.reshape(-1, X_train.shape[-1])).reshape(X_train.shape)\n",
            "\n",
            "# Entra√Æner avec des param√®tres optimis√©s pour A100\n",
            "print(\"üöÄ D√©but de l'entra√Ænement GPU optimis√© pour A100...\")\n",
            "try:\n",
            "    history = model.fit(\n",
            "        X_train_scaled, y_train,\n",
            "        epochs=20,  # R√©duit pour √©viter les crashs\n",
            "        batch_size=64,\n",
            "        validation_split=0.2,\n",
            "        callbacks=callbacks,\n",
            "        verbose=1\n",
            "    )\n",
            "    print(\"‚úÖ Entra√Ænement GPU termin√© avec succ√®s\")\n",
            "    \n",
            "    # Sauvegarder le mod√®le et le scaler\n",
            "    try:\n",
            "        model.save(f'{base_path}/models/simplified_gpu_model.keras')\n",
            "        with open(f'{base_path}/models/simplified_scaler.pkl', 'wb') as f:\n",
            "            pickle.dump(scaler, f)\n",
            "        print(\"‚úÖ Mod√®le GPU et scaler sauvegard√©s\")\n",
            "    except Exception as e:\n",
            "        print(f\"‚ö†Ô∏è Erreur de sauvegarde: {e}\")\n",
            "    \n",
            "    # Afficher les courbes d'apprentissage\n",
            "    try:\n",
            "        plt.figure(figsize=(12, 4))\n",
            "        \n",
            "        plt.subplot(1, 2, 1)\n",
            "        plt.plot(history.history['accuracy'], label='Training')\n",
            "        if 'val_accuracy' in history.history:\n",
            "            plt.plot(history.history['val_accuracy'], label='Validation')\n",
            "        plt.title('Model Accuracy')\n",
            "        plt.legend()\n",
            "        \n",
            "        plt.subplot(1, 2, 2)\n",
            "        plt.plot(history.history['loss'], label='Training')\n",
            "        if 'val_loss' in history.history:\n",
            "            plt.plot(history.history['val_loss'], label='Validation')\n",
            "        plt.title('Model Loss')\n",
            "        plt.legend()\n",
            "        \n",
            "        plt.tight_layout()\n",
            "        plt.show()\n",
            "    except Exception as e:\n",
            "        print(f\"‚ö†Ô∏è Erreur lors de l'affichage des courbes: {e}\")\n",
            "        \n",
            "except Exception as e:\n",
            "    print(f\"‚ùå Erreur lors de l'entra√Ænement GPU: {e}\")\n",
            "    print(\"üîß Analyse de l'erreur:\")\n",
            "    print(f\"  - Type d'erreur: {type(e).__name__}\")\n",
            "    print(f\"  - Message: {str(e)}\")\n",
            "    \n",
            "    # Si erreur CuDNN, essayer une approche CPU\n",
            "    if \"CuDNN\" in str(e) or \"DNN\" in str(e):\n",
            "        print(\"üîß D√©tection d'erreur CuDNN, passage en mode CPU...\")\n",
            "        import os\n",
            "        os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
            "        \n",
            "        # Recr√©er un mod√®le CPU simple\n",
            "        with tf.distribute.get_strategy().scope():\n",
            "            cpu_model = tf.keras.Sequential([\n",
            "                tf.keras.layers.Input(shape=(30, 3)),\n",
            "                tf.keras.layers.Flatten(),\n",
            "                tf.keras.layers.Dense(128, activation='relu'),\n",
            "                tf.keras.layers.Dropout(0.3),\n",
            "                tf.keras.layers.Dense(64, activation='relu'),\n",
            "                tf.keras.layers.Dropout(0.3),\n",
            "                tf.keras.layers.Dense(32, activation='relu'),\n",
            "                tf.keras.layers.Dense(3, activation='softmax')\n",
            "            ])\n",
            "            \n",
            "            cpu_model.compile(\n",
            "                optimizer='adam',\n",
            "                loss='sparse_categorical_crossentropy',\n",
            "                metrics=['accuracy']\n",
            "            )\n",
            "        \n",
            "        # Entra√Æner le mod√®le CPU\n",
            "        history = cpu_model.fit(\n",
            "            X_train_scaled, y_train,\n",
            "            epochs=15,  # R√©duit pour √©viter les crashs\n",
            "            batch_size=32,\n",
            "            validation_split=0.2,\n",
            "            verbose=1\n",
            "        )\n",
            "        \n",
            "        model = cpu_model\n",
            "        print(\"‚úÖ Mod√®le CPU de secours entra√Æn√©\")\n",
            "    else:\n",
            "        raise e\n"
        ]
        
        # Remplacer tout le contenu de la cellule
        cell['source'] = new_cell_content
        break

# Sauvegarder le notebook corrig√©
with open('ALPHABOT_ML_TRAINING_COLAB.ipynb', 'w', encoding='utf-8') as f:
    json.dump(nb, f, indent=1, ensure_ascii=False)

print("Notebook corrig√© avec succ√®s - erreur d'indentation r√©solue!")
