{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Ajustement du path pour que Colab trouve le module alphabot\n",
    "import sys\n",
    "sys.path.append('/content')\n",
    "sys.path.append('/content/alphabot')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ AlphaBot ML/DL Training - Google Colab\n",
    "\n",
    "## üìã Vue d'ensemble\n",
    "\n",
    "Ce notebook entra√Æne les mod√®les Machine Learning et Deep Learning d'AlphaBot :\n",
    "- Pattern Detector (LSTM + CNN)\n",
    "- Sentiment Analyzer (FinBERT + RoBERTa)\n",
    "- RAG Integrator (Embeddings + FAISS)\n",
    "\n",
    "## ‚ö° Optimisations\n",
    "- GPU/TPU acceleration\n",
    "- Mixed precision training\n",
    "- Memory management\n",
    "- Automatic checkpoints\n",
    "- Timeout protection"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# CELLULE 1: Setup GPU/TPU optimis√©\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "import json\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Configuration logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# D√©tection GPU/TPU\n",
    "try:\n",
    "    # D√©tecter TPU\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    strategy = tf.distribute.TPUStrategy(tpu)\n",
    "    print('‚úÖ TPU d√©tect√©e et configur√©e')\n",
    "except:\n",
    "    try:\n",
    "        # D√©tecter GPU\n",
    "        gpus = tf.config.list_physical_devices('GPU')\n",
    "        if gpus:\n",
    "            for gpu in gpus:\n",
    "                tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            strategy = tf.distribute.MirroredStrategy()\n",
    "            print(f'‚úÖ {len(gpus)} GPU(s) d√©tect√©e(s)')\n",
    "        else:\n",
    "            strategy = tf.distribute.get_strategy()\n",
    "            print('‚ö†Ô∏è Aucun GPU/TPU d√©tect√©, utilisation du CPU')\n",
    "    except Exception as e:\n",
    "        strategy = tf.distribute.get_strategy()\n",
    "        print(f'‚ö†Ô∏è Erreur de configuration GPU: {e}')\n",
    "\n",
    "# Activer mixed precision\n",
    "try:\n",
    "    policy = tf.keras.mixed_precision.Policy('mixed_float16')\n",
    "    tf.keras.mixed_precision.set_global_policy(policy)\n",
    "    print('‚úÖ Mixed precision activ√©e')\n",
    "except:\n",
    "    print('‚ö†Ô∏è Mixed precision non disponible')\n",
    "\n",
    "# Afficher les infos\n",
    "print(f\"\\nüìä Configuration:\")\n",
    "print(f\"- TensorFlow: {tf.__version__}\")\n",
    "print(f\"- PyTorch: {torch.__version__}\")\n",
    "print(f\"- Strategy: {strategy}\")\n",
    "print(f\"- GPUs disponibles: {tf.config.list_physical_devices('GPU')}\")\n",
    "\n",
    "# V√©rifier CUDA\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"- CUDA disponible: {torch.version.cuda}\")\n",
    "    print(f\"- GPU courant: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"- M√©moire GPU: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# CELLULE 2: Google Drive setup\n",
    "from google.colab import drive\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Monter Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Cr√©er la structure de dossiers\n",
    "base_path = '/content/drive/MyDrive/AlphaBot_ML_Training'\n",
    "Path(base_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Sous-dossiers\n",
    "subdirs = ['models', 'data', 'logs', 'checkpoints', 'exports', 'configs']\n",
    "for subdir in subdirs:\n",
    "    Path(f\"{base_path}/{subdir}\").mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"‚úÖ Google Drive mont√©\")\n",
    "print(f\"üìÅ Base path: {base_path}\")\n",
    "print(f\"üìÇ Sous-dossiers cr√©√©s: {', '.join(subdirs)}\")\n",
    "\n",
    "# V√©rifier l'espace disponible\n",
    "import shutil\n",
    "total, used, free = shutil.disk_usage(base_path)\n",
    "print(f\"üíæ Espace disponible: {free / (1024**3):.1f} GB\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# CELLULE 3: Code AlphaBot setup\n",
    "import subprocess\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Cloner le d√©p√¥t AlphaBot\n",
    "if not Path('/content/alphabot').exists():\n",
    "    print(\"üì• Clonage du d√©p√¥t AlphaBot...\")\n",
    "    subprocess.run(['git', 'clone', 'https://github.com/thomy03/alphabot.git', '/content/alphabot'], check=True)\n",
    "else:\n",
    "    print(\"üìÇ D√©p√¥t AlphaBot d√©j√† pr√©sent\")\n",
    "\n",
    "# Installer les d√©pendances\n",
    "print(\"üì¶ Installation des d√©pendances...\")\n",
    "subprocess.run(['pip', 'install', '-r', '/content/alphabot/requirements_colab.txt'], check=True)\n",
    "\n",
    "# Importer les modules AlphaBot\n",
    "sys.path.append('/content')\n",
    "sys.path.append('/content/alphabot')\n",
    "\n",
    "try:\n",
    "    from alphabot.ml.pattern_detector import MLPatternDetector\n",
    "    from alphabot.ml.sentiment_analyzer import SentimentAnalyzer\n",
    "    from alphabot.ml.rag_integrator import RAGIntegrator\n",
    "    from colab_utils import ColabMemoryMonitor, create_colab_callbacks\n",
    "    from drive_manager import DriveManager\n",
    "    print(\"‚úÖ Modules AlphaBot import√©s avec succ√®s\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erreur d'import: {e}\")\n",
    "    print(\"üîß V√©rifiez que le path est correct et les d√©pendances install√©es\")\n",
    "\n",
    "# Initialiser les composants\n",
    "try:\n",
    "    pattern_detector = MLPatternDetector()\n",
    "    sentiment_analyzer = SentimentAnalyzer()\n",
    "    rag_integrator = RAGIntegrator()\n",
    "    drive_manager = DriveManager(base_path)\n",
    "    memory_monitor = ColabMemoryMonitor()\n",
    "    print(\"‚úÖ Composants ML initialis√©s\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erreur d'initialisation: {e}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# CELLULE 4: T√©l√©chargement des donn√©es\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "\n",
    "# Configuration\n",
    "symbols = ['AAPL', 'GOOGL', 'MSFT', 'TSLA', 'AMZN']\n",
    "end_date = datetime.now()\n",
    "start_date = end_date - timedelta(days=365*2)  # 2 ans de donn√©es\n",
    "\n",
    "print(f\"üì• T√©l√©chargement des donn√©es pour {symbols}...\")\n",
    "print(f\"üìÖ P√©riode: {start_date.strftime('%Y-%m-%d')} √† {end_date.strftime('%Y-%m-%d')}\")\n",
    "\n",
    "# T√©l√©charger les donn√©es\n",
    "all_data = {}\n",
    "for symbol in symbols:\n",
    "    try:\n",
    "        data = yf.download(symbol, start=start_date, end=end_date)\n",
    "        if not data.empty:\n",
    "            all_data[symbol] = data\n",
    "            print(f\"‚úÖ {symbol}: {len(data)} jours de donn√©es\")\n",
    "        else:\n",
    "            print(f\"‚ùå {symbol}: Pas de donn√©es disponibles\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå {symbol}: Erreur de t√©l√©chargement - {e}\")\n",
    "\n",
    "# Sauvegarder les donn√©es\n",
    "import pickle\n",
    "data_path = f\"{base_path}/data/market_data.pkl\"\n",
    "with open(data_path, 'wb') as f:\n",
    "    pickle.dump(all_data, f)\n",
    "\n",
    "print(f\"\\nüíæ Donn√©es sauvegard√©es dans: {data_path}\")\n",
    "print(f\"üìä Total symboles: {len(all_data)}\")\n",
    "\n",
    "# Afficher des statistiques\n",
    "if all_data:\n",
    "    sample_symbol = list(all_data.keys())[0]\n",
    "    sample_data = all_data[sample_symbol]\n",
    "    print(f\"\\nüìà Exemple pour {sample_symbol}:\")\n",
    "    print(f\"- Premi√®re date: {sample_data.index[0].strftime('%Y-%m-%d')}\")\n",
    "    print(f\"- Derni√®re date: {sample_data.index[-1].strftime('%Y-%m-%d')}\")\n",
    "    print(f\"- Prix moyen: ${sample_data['Close'].mean():.2f}\")\n",
    "    print(f\"- Volatilit√©: {sample_data['Close'].pct_change().std()*100:.2f}%\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# CELLULE 5: Entra√Ænement Pattern Detector\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"üß† Entra√Ænement du Pattern Detector (LSTM + CNN)...\")\n",
    "\n",
    "# Pr√©parer les donn√©es d'entra√Ænement\n",
    "def prepare_pattern_training_data(all_data):\n",
    "    X_train, y_train = [], []\n",
    "    \n",
    "    for symbol, data in all_data.items():\n",
    "        if len(data) < 50:\n",
    "            continue\n",
    "            \n",
    "        # Cr√©er des s√©quences temporelles\n",
    "        prices = data['Close'].values\n",
    "        returns = np.diff(prices) / prices[:-1]\n",
    "        \n",
    "        # Labels: 0=DOWN, 1=SIDEWAYS, 2=UP\n",
    "        for i in range(30, len(returns)-5):\n",
    "            # Features: prix normalis√©s, returns, volume\n",
    "            seq_prices = prices[i-30:i] / prices[i-30]\n",
    "            seq_returns = returns[i-30:i]\n",
    "            seq_volume = data['Volume'].values[i-30:i] / np.mean(data['Volume'].values[i-30:i])\n",
    "            \n",
    "            # Combiner features\n",
    "            features = np.column_stack([seq_prices, seq_returns, seq_volume])\n",
    "            \n",
    "            # Label bas√© sur le mouvement futur\n",
    "            future_return = np.mean(returns[i:i+5])\n",
    "            if future_return < -0.02:\n",
    "                label = 0  # DOWN\n",
    "            elif future_return > 0.02:\n",
    "                label = 2  # UP\n",
    "            else:\n",
    "                label = 1  # SIDEWAYS\n",
    "            \n",
    "            X_train.append(features)\n",
    "            y_train.append(label)\n",
    "    \n",
    "    return np.array(X_train), np.array(y_train)\n",
    "\n",
    "# Pr√©parer les donn√©es\n",
    "X_train, y_train = prepare_pattern_training_data(all_data)\n",
    "print(f\"üìä Donn√©es pr√©par√©es: {X_train.shape[0]} √©chantillons\")\n",
    "\n",
    "# Cr√©er le mod√®le LSTM\n",
    "with strategy.scope():\n",
    "    lstm_model = tf.keras.Sequential([\n",
    "        tf.keras.layers.LSTM(64, return_sequences=True, input_shape=(30, 3)),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.LSTM(32, return_sequences=False),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(16, activation='relu'),\n",
    "        tf.keras.layers.Dense(3, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    lstm_model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "# Callbacks\n",
    "callbacks = [\n",
    "    EarlyStopping(patience=10, restore_best_weights=True),\n",
    "    ReduceLROnPlateau(factor=0.5, patience=5),\n",
    "    ModelCheckpoint(\n",
    "        f'{base_path}/checkpoints/lstm_pattern_best.h5',\n",
    "        save_best_only=True\n",
    "    )\n",
    "]\n",
    "\n",
    "# Entra√Æner\n",
    "print(\"üöÄ D√©but de l'entra√Ænement LSTM...\")\n",
    "history = lstm_model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Sauvegarder le mod√®le\n",
    "lstm_model.save(f'{base_path}/models/lstm_pattern_model.h5')\n",
    "print(\"‚úÖ Mod√®le LSTM sauvegard√©\")\n",
    "\n",
    "# Afficher les courbes d'apprentissage\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Training')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation')\n",
    "plt.title('LSTM Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Training')\n",
    "plt.plot(history.history['val_loss'], label='Validation')\n",
    "plt.title('LSTM Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# CELLULE 6: Entra√Ænement Sentiment Analyzer\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "print(\"üí≠ Entra√Ænement du Sentiment Analyzer (FinBERT + RoBERTa)...\")\n",
    "\n",
    "# Cr√©er un dataset de d√©monstration\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Donn√©es de d√©monstration\n",
    "demo_texts = [\n",
    "    \"Apple reports record quarterly earnings\",\n",
    "    \"Google stock drops on regulatory concerns\",\n",
    "    \"Tesla announces new battery technology\",\n",
    "    \"Microsoft cloud growth exceeds expectations\",\n",
    "    \"Amazon faces antitrust investigation\",\n",
    "    \"Meta launches new VR platform\",\n",
    "    \"NVIDIA chips power AI revolution\",\n",
    "    "Bitcoin reaches new all-time high\",\n",
    "    \"Federal Reserve raises interest rates\",\n",
    "    \"Oil prices surge on supply concerns\"\n",
    "]\n",
    "\n",
    "demo_labels = [2, 0, 2, 2, 0, 2, 2, 2, 0, 0]  # 0=n√©gatif, 1=neutre, 2=positif\n",
    "\n",
    "# Initialiser FinBERT\n",
    "model_name = \"yiyanghkust/finbert-tone\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# Cr√©er le dataset\n",
    "dataset = SentimentDataset(demo_texts, demo_labels, tokenizer)\n",
    "\n",
    "# Configuration de l'entra√Ænement\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f'{base_path}/checkpoints/sentiment',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    warmup_steps=10,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=f'{base_path}/logs/sentiment',\n",
    "    logging_steps=1,\n",
    "    save_steps=10,\n",
    "    evaluation_strategy=\"no\",\n",
    "    save_strategy=\"epoch\"\n",
    ")\n",
    "\n",
    "# Cr√©er le trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# Entra√Æner\n",
    "print(\"üöÄ D√©but de l'entra√Ænement FinBERT...\")\n",
    "trainer.train()\n",
    "\n",
    "# Sauvegarder le mod√®le\n",
    "model.save_pretrained(f'{base_path}/models/finbert_sentiment')\n",
    "tokenizer.save_pretrained(f'{base_path}/models/finbert_sentiment')\n",
    "print(\"‚úÖ Mod√®le FinBERT sauvegard√©\")\n",
    "\n",
    "# Tester le mod√®le\n",
    "print(\"\\nüß™ Test du mod√®le:\")\n",
    "test_texts = [\n",
    "    \"Strong earnings report drives stock higher\",\n",
    "    \"Company faces bankruptcy concerns\",\n",
    "    \"Stable performance in challenging market\"\n",
    "]\n",
    "\n",
    "for text in test_texts:\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    outputs = model(**inputs)\n",
    "    predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "    sentiment_idx = torch.argmax(predictions).item()\n",
    "    sentiment_map = {0: \"N√©gatif\", 1: \"Neutre\", 2: \"Positif\"}\n",
    "    confidence = predictions[0][sentiment_idx].item()\n",
    "    print(f\"'{text[:50]}...' -> {sentiment_map[sentiment_idx]} ({confidence:.2f})\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# CELLULE 7: Entra√Ænement RAG Integrator\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"üîç Entra√Ænement du RAG Integrator (Embeddings + FAISS)...\")\n",
    "\n",
    "# Cr√©er des documents de d√©monstration\n",
    "demo_documents = [\n",
    "    \"Apple Inc. is a technology company that designs, manufactures, and markets smartphones, personal computers, tablets, wearables, and accessories worldwide.\",\n",
    "    \"Alphabet Inc. provides various products and platforms in the United States, Europe, the Middle East, Africa, the Asia-Pacific, Canada, and Latin America.\",\n",
    "    \"Microsoft Corporation develops, licenses, and supports software, services, devices, and solutions worldwide.\",\n",
    "    \"Amazon.com, Inc. engages in the retail sale of consumer products and subscriptions in North America and internationally.\",\n",
    "    \"Tesla, Inc. designs, develops, manufactures, leases, and sells electric vehicles, and energy generation and storage systems.\",\n",
    "    \"Meta Platforms, Inc. develops products that enable people to connect and share with friends and family through mobile devices, personal computers, virtual reality headsets, and wearables.\",\n",
    "    \"NVIDIA Corporation provides graphics, computing, and networking solutions in the United States, Taiwan, China, and internationally.\",\n",
    "    \"Netflix, Inc. provides entertainment services worldwide. It offers a TV shows, movies, and games.\",\n",
    "    \"The Goldman Sachs Group, Inc. provides a range of financial services worldwide.\",\n",
    "    "JPMorgan Chase & Co. provides financial services worldwide.\",\n",
    "    \"The Bank of America Corporation provides banking and financial products and services for consumers, small businesses, and institutions.\",\n",
    "    \"Walmart Inc. engages in the operation of retail, wholesale, and other units worldwide.\",\n",
    "    \"The Procter & Gamble Company provides branded consumer packaged goods worldwide.\",\n",
    "    \"The Coca-Cola Company is a beverage company.\",\n",
    "    "PepsiCo, Inc. manufactures, markets, and distributes various beverages and convenient foods worldwide.\",\n",
    "    \"The Home Depot, Inc. operates as a home improvement retailer.\",\n",
    "    \"The Boeing Company operates in the aerospace industry.\",\n",
    "    \"The Exxon Mobil Corporation explores for and produces crude oil and natural gas.\",\n",
    "    \"The Chevron Corporation operates through its Upstream and Downstream segments.\",\n",
    "    \"The Johnson & Johnson engages in the research and development, manufacture, and sale of various products in the healthcare field.\",\n",
    "    \"The Visa Inc. operates as a payments technology company worldwide.\"\n",
    "]\n",
    "\n",
    "# Initialiser le mod√®le d'embeddings\n",
    "print(\"üì• Chargement du mod√®le d'embeddings...\")\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Cr√©er les embeddings\n",
    "print(\"üî¢ Cr√©ation des embeddings...\")\n",
    "document_embeddings = embedding_model.encode(demo_documents)\n",
    "print(f\"‚úÖ Embeddings cr√©√©s: {document_embeddings.shape}\")\n",
    "\n",
    "# Cr√©er l'index FAISS\n",
    "dimension = document_embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(document_embeddings)\n",
    "\n",
    "print(f\"‚úÖ Index FAISS cr√©√© avec {index.ntotal} documents\")\n",
    "\n",
    "# Sauvegarder l'index et les documents\n",
    "faiss.write_index(index, f'{base_path}/models/faiss_index.bin')\n",
    "\n",
    "import pickle\n",
    "with open(f'{base_path}/models/documents.pkl', 'wb') as f:\n",
    "    pickle.dump(demo_documents, f)\n",
    "\n",
    "print(\"üíæ Index et documents sauvegard√©s\")\n",
    "\n",
    "# Tester la recherche s√©mantique\n",
    "print(\"\\nüß™ Test de recherche s√©mantique:\")\n",
    "test_queries = [\n",
    "    \"electric vehicle company\",\n",
    "    \"social media platform\",\n",
    "    \"banking services\",\n",
    "    \"beverage company\",\n",
    "    \"technology giant\"\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    # Cr√©er l'embedding de la requ√™te\n",
    "    query_embedding = embedding_model.encode([query])[0].reshape(1, -1)\n",
    "    \n",
    "    # Rechercher les documents les plus similaires\n",
    "    k = 3\n",
    "    distances, indices = index.search(query_embedding, k)\n",
    "    \n",
    "    print(f\"\\nüîç Requ√™te: '{query}'\")\n",
    "    for i, (dist, idx) in enumerate(zip(distances[0], indices[0])):\n",
    "        doc_preview = demo_documents[idx][:80] + \"...\"\n",
    "        print(f\"  {i+1}. {doc_preview} (distance: {dist:.4f})\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# CELLULE 8: Int√©gration et tests\n",
    "import json\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "\n",
    "print(\"üîß Int√©gration des mod√®les et tests finaux...\")\n",
    "\n",
    "# Charger tous les mod√®les\n",
    "try:\n",
    "    # Charger le mod√®le LSTM\n",
    "    lstm_model = tf.keras.models.load_model(f'{base_path}/models/lstm_pattern_model.h5')\n",
    "    print(\"‚úÖ Mod√®le LSTM charg√©\")\n",
    "    \n",
    "    # Charger le mod√®le FinBERT\n",
    "    from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "    sentiment_tokenizer = AutoTokenizer.from_pretrained(f'{base_path}/models/finbert_sentiment')\n",
    "    sentiment_model = AutoModelForSequenceClassification.from_pretrained(f'{base_path}/models/finbert_sentiment')\n",
    "    print(\"‚úÖ Mod√®le FinBERT charg√©\")\n",
    "    \n",
    "    # Charger l'index FAISS et les documents\n",
    "    faiss_index = faiss.read_index(f'{base_path}/models/faiss_index.bin')\n",
    "    with open(f'{base_path}/models/documents.pkl', 'rb') as f:\n",
    "        documents = pickle.load(f)\n",
    "    print(\"‚úÖ Index FAISS et documents charg√©s\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erreur de chargement des mod√®les: {e}\")\n",
    "\n",
    "# Test d'int√©gration complet\n",
    "print(\"\\nüß™ Test d'int√©gration complet:\")\n",
    "\n",
    "# 1. Test de d√©tection de patterns\n",
    "print(\"\\n1. Test Pattern Detector:\")\n",
    "test_symbol = 'AAPL'\n",
    "if test_symbol in all_data:\n",
    "    test_data = all_data[test_symbol].tail(50)\n",
    "    \n",
    "    # Pr√©parer les features\n",
    "    prices = test_data['Close'].values\n",
    "    returns = np.diff(prices) / prices[:-1]\n",
    "    \n",
    "    seq_prices = prices[-30:] / prices[-30]\n",
    "    seq_returns = returns[-30:]\n",
    "    seq_volume = test_data['Volume'].values[-30:] / np.mean(test_data['Volume'].values[-30:])\n",
    "    \n",
    "    features = np.column_stack([seq_prices, seq_returns, seq_volume]).reshape(1, 30, 3)\n",
    "    \n",
    "    # Pr√©diction\n",
    "    prediction = lstm_model.predict(features, verbose=0)\n",
    "    predicted_class = np.argmax(prediction[0])\n",
    "    confidence = np.max(prediction[0])\n",
    "    \n",
    "    class_map = {0: 'DOWN', 1: 'SIDEWAYS', 2: 'UP'}\n",
    "    print(f\"   Pr√©diction: {class_map[predicted_class]} (confiance: {confidence:.2f})\")\n",
    "\n",
    "# 2. Test d'analyse de sentiment\n",
    "print(\"\\n2. Test Sentiment Analyzer:\")\n",
    "test_news = [\n",
    "    \"Apple announces breakthrough AI technology\",\n",
    "    \"Market volatility concerns investors\",\n",
    "    \"Tech sector shows steady growth\"\n",
    "]\n",
    "\n",
    "for news in test_news:\n",
    "    inputs = sentiment_tokenizer(news, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    outputs = sentiment_model(**inputs)\n",
    "    predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "    sentiment_idx = torch.argmax(predictions).item()\n",
    "    sentiment_map = {0: \"N√©gatif\", 1: \"Neutre\", 2: \"Positif\"}\n",
    "    confidence = predictions[0][sentiment_idx].item()\n",
    "    print(f\"   '{news[:40]}...' -> {sentiment_map[sentiment_idx]} ({confidence:.2f})\")\n",
    "\n",
    "# 3. Test RAG\n",
    "print(\"\\n3. Test RAG Integrator:\")\n",
    "rag_queries = [\n",
    "    \"Which company focuses on electric vehicles?\",\n",
    "    \"Find information about social media companies\",\n",
    "    \"What companies are in the banking sector?\"\n",
    "]\n",
    "\n",
    "for query in rag_queries:\n",
    "    query_embedding = embedding_model.encode([query])[0].reshape(1, -1)\n",
    "    k = 2\n",
    "    distances, indices = faiss_index.search(query_embedding, k)\n",
    "    \n",
    "    print(f\"   Query: '{query}'\")\n",
    "    for i, (dist, idx) in enumerate(zip(distances[0], indices[0])):\n",
    "        doc_preview = documents[idx][:60] + \"...\"\n",
    "        print(f\"     Result {i+1}: {doc_preview}\")\n",
    "\n",
    "# Cr√©er un rapport de performance\n",
    "performance_report = {\n",
    "    \"timestamp\": datetime.now().isoformat(),\n",
    "    \"models_trained\": {\n",
    "        \"lstm_pattern_detector\": {\n",
    "            \"status\": \"success\",\n",
    "            \"accuracy\": float(history.history['val_accuracy'][-1]) if 'history' in locals() else 0.0,\n",
    "            \"epochs_trained\": len(history.history['accuracy']) if 'history' in locals() else 0\n",
    "        },\n",
    "        \"finbert_sentiment\": {\n",
    "            \"status\": \"success\",\n",
    "            \"epochs_trained\": 3,\n",
    "            \"model_size\": \"base\"\n",
    "        },\n",
    "        \"rag_integrator\": {\n",
    "            \"status\": \"success\",\n",
    "            \"documents_indexed\": len(documents),\n",
    "            \"embedding_dimension\": document_embeddings.shape[1]\n",
    "        }\n",
    "    },\n",
    "    \"system_info\": {\n",
    "        \"tensorflow_version\": tf.__version__,\n",
    "        \"torch_version\": torch.__version__,\n",
    "        \"gpu_available\": len(tf.config.list_physical_devices('GPU')) > 0,\n",
    "        \"tpu_available\": 'tpu' in locals()\n",
    "    },\n",
    "    \"training_duration\": \"N/A\",\n",
    "    \"data_used\": {\n",
    "        \"symbols\": list(all_data.keys()),\n",
    "        \"total_data_points\": sum(len(data) for data in all_data.values())\n",
    "    }\n",
    "}\n",
    "\n",
    "# Sauvegarder le rapport\n",
    "report_path = f'{base_path}/exports/performance_report.json'\n",
    "with open(report_path, 'w') as f:\n",
    "    json.dump(performance_report, f, indent=2)\n",
    "\n",
    "print(f\"\\nüìä Rapport de performance sauvegard√©: {report_path}\")\n",
    "print(\"‚úÖ Entra√Ænement ML/DL termin√© avec succ√®s!\")\n",
    "\n",
    "# R√©sum√© final\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"üéâ R√âSUM√â DE L'ENTRA√éNEMENT\")\n",
    "print(\"=\"*50)\n",
    "print(f\"üìÖ Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"üß† Mod√®les entra√Æn√©s: 3 (LSTM, FinBERT, RAG)\")\n",
    "print(f\"üìä Donn√©es utilis√©es: {len(all_data)} symboles\")\n",
    "print(f\"üíæ Mod√®les sauvegard√©s dans: {base_path}/models/\")\n",
    "print(f\"üìã Rapport disponible: {report_path}\")\n",
    "print(\"\\nüöÄ Prochaines √©tapes:\")\n",
    "print(\"   1. Pousser les modifications sur GitHub\")\n",
    "print(\"   2. Lancer les tests locaux\")\n",
    "print(\"   3. D√©ployer en production\")\n",
    "print(\"=\"*50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
