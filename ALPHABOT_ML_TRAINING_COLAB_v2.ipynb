{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üöÄ AlphaBot ML/DL Training - Google Colab (v2)\n",
        "\n",
        "Notebook propre et corrig√© (d√©tection robuste des colonnes yfinance MultiIndex/suffixes) avec suivi/reprise, t√©l√©chargement de donn√©es robuste et fallbacks s√ªrs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ajustement du path pour que Colab trouve le module alphabot\n",
        "import sys\n",
        "sys.path.append('/content')\n",
        "sys.path.append('/content/alphabot')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîÑ Suivi de Progression et Reprise Automatique"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üîÑ Syst√®me de suivi et reprise automatique\n",
        "import os, json\n",
        "from datetime import datetime\n",
        "\n",
        "base_path = '/content/drive/MyDrive/AlphaBot_ML_Training'\n",
        "os.makedirs(base_path, exist_ok=True)\n",
        "progress_file = f'{base_path}/progress_tracker.json'\n",
        "\n",
        "default_progress = {\n",
        "    'cell_1_setup': False,\n",
        "    'cell_2_data_download': False,\n",
        "    'cell_3_data_analysis': False,\n",
        "    'cell_4_pattern_training': False,\n",
        "    'cell_5_sentiment_training': False,\n",
        "    'cell_6_rag_training': False,\n",
        "    'cell_7_integration': False,\n",
        "    'cell_8_testing': False,\n",
        "    'cell_9_deployment': False,\n",
        "    'last_cell_executed': None,\n",
        "    'start_time': None,\n",
        "    'last_update': None\n",
        "}\n",
        "\n",
        "try:\n",
        "    with open(progress_file, 'r') as f:\n",
        "        progress = json.load(f)\n",
        "    print('üìä Suivi de progression charg√©')\n",
        "except Exception:\n",
        "    progress = default_progress.copy()\n",
        "    progress['start_time'] = datetime.now().isoformat()\n",
        "    print('üÜï Nouveau suivi de progression initialis√©')\n",
        "\n",
        "def update_progress(cell_name):\n",
        "    progress.setdefault(cell_name, False)\n",
        "    progress[cell_name] = True\n",
        "    progress['last_cell_executed'] = cell_name\n",
        "    progress['last_update'] = datetime.now().isoformat()\n",
        "    with open(progress_file, 'w') as f:\n",
        "        json.dump(progress, f, indent=2)\n",
        "    print(f'‚úÖ Progression mise √† jour: {cell_name}')\n",
        "\n",
        "def check_progress():\n",
        "    print('\\nüìã √âtat actuel de la progression:')\n",
        "    print('=' * 50)\n",
        "    completed = sum(1 for k, v in progress.items() if k.startswith('cell_') and isinstance(v, bool) and v)\n",
        "    total = sum(1 for k in default_progress.keys() if k.startswith('cell_'))\n",
        "    pct = (completed / total * 100) if total else 0.0\n",
        "    print(f'üìä Progression: {completed}/{total} √©tapes compl√©t√©es ({pct:.1f}%)')\n",
        "    print(f'‚è∞ D√©marr√©: {progress.get(\"start_time\", \"N/A\")}')\n",
        "    print(f'üîÑ Derni√®re mise √† jour: {progress.get(\"last_update\", \"N/A\")}')\n",
        "    print(f'üìç Derni√®re cellule: {progress.get(\"last_cell_executed\", \"Aucune\")}')\n",
        "    print('=' * 50)\n",
        "\n",
        "check_progress()\n",
        "print('\\nüí° Instructions:')\n",
        "print('1. Ex√©cutez cette cellule pour voir l\\'√©tat d\\'avancement')\n",
        "print('2. Chaque cellule mettra √† jour automatiquement sa progression')\n",
        "print('3. Si le processus s\\'arr√™te, relancez simplement cette cellule')\n",
        "print('4. Continuez avec la cellule sugg√©r√©e')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Setup GPU/TPU et environnement Colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys, os, subprocess\n",
        "\n",
        "def pip_install(pkgs):\n",
        "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q'] + pkgs)\n",
        "\n",
        "# Nettoyage des modules d√©j√† import√©s\n",
        "for m in [m for m in list(sys.modules) if m.startswith('transformers') or m.startswith('accelerate') or m.startswith('numpy') or m.startswith('tensorflow') or m.startswith('torch')]:\n",
        "    del sys.modules[m]\n",
        "\n",
        "# R√©installer numpy pour compatibilit√© binaire\n",
        "pip_install([\n",
        "    '--upgrade',\n",
        "    'numpy>=1.24.0,<1.27.0'\n",
        "])\n",
        "\n",
        "# Installer un set compatible (Option A)\n",
        "pip_install([\n",
        "    'transformers>=4.43,<4.47',\n",
        "    'accelerate>=0.30,<0.34',\n",
        "    'datasets>=2.18,<3.0',\n",
        "    'safetensors>=0.4.3',\n",
        "    'huggingface-hub>=0.23,<0.25'\n",
        "])\n",
        "\n",
        "os.environ['HF_HUB_DISABLE_TELEMETRY'] = '1'\n",
        "\n",
        "# Importer et afficher les versions\n",
        "import numpy\n",
        "import transformers\n",
        "print('NumPy version:', numpy.__version__)\n",
        "print('Transformers version:', transformers.__version__)\n",
        "\n",
        "import tensorflow as tf\n",
        "import torch, logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "try:\n",
        "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "    tf.config.experimental_connect_to_cluster(tpu)\n",
        "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "    strategy = tf.distribute.TPUStrategy(tpu)\n",
        "    print('‚úÖ TPU d√©tect√©e et configur√©e')\n",
        "except Exception:\n",
        "    try:\n",
        "        gpus = tf.config.list_physical_devices('GPU')\n",
        "        if gpus:\n",
        "            for gpu in gpus:\n",
        "                tf.config.experimental.set_memory_growth(gpu, True)\n",
        "            strategy = tf.distribute.MirroredStrategy()\n",
        "            print(f'‚úÖ {len(gpus)} GPU(s) d√©tect√©e(s)')\n",
        "        else:\n",
        "            strategy = tf.distribute.get_strategy()\n",
        "            print('‚ö†Ô∏è Aucun GPU/TPU d√©tect√©, utilisation du CPU')\n",
        "    except Exception as e:\n",
        "        strategy = tf.distribute.get_strategy()\n",
        "        print(f'‚ö†Ô∏è Erreur de configuration GPU: {e}')\n",
        "\n",
        "try:\n",
        "    policy = tf.keras.mixed_precision.Policy('mixed_float16')\n",
        "    tf.keras.mixed_precision.set_global_policy(policy)\n",
        "    print('‚úÖ Mixed precision activ√©e')\n",
        "except Exception:\n",
        "    print('‚ö†Ô∏è Mixed precision non disponible')\n",
        "\n",
        "print('\\nüìä Configuration:')\n",
        "print(f'- TensorFlow: {tf.__version__}')\n",
        "print(f'- PyTorch: {torch.__version__}')\n",
        "print(f'- Strategy: {strategy}')\n",
        "print(f\"- GPUs disponibles: {tf.config.list_physical_devices('GPU')}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f'- CUDA: {torch.version.cuda}')\n",
        "    print(f'- GPU: {torch.cuda.get_device_name(0)}')\n",
        "\n",
        "update_progress('cell_1_setup')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Montage Google Drive (r√©silient v2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print('üîß Configuration Google Drive (r√©siliente v2)...')\n",
        "from google.colab import drive\n",
        "import os, shutil, time\n",
        "MOUNT_POINT = '/content/drive'\n",
        "\n",
        "def _safe_cleanup_mount_point(mp: str):\n",
        "    try:\n",
        "        if os.path.islink(mp): os.unlink(mp)\n",
        "        if os.path.isdir(mp):\n",
        "            for entry in os.listdir(mp):\n",
        "                p = os.path.join(mp, entry)\n",
        "                try:\n",
        "                    if os.path.isfile(p) or os.path.islink(p): os.remove(p)\n",
        "                    elif os.path.isdir(p): shutil.rmtree(p)\n",
        "                except Exception: pass\n",
        "        else:\n",
        "            os.makedirs(mp, exist_ok=True)\n",
        "    except Exception as e:\n",
        "        print(f'‚ö†Ô∏è Nettoyage mount point: {e}')\n",
        "\n",
        "def _force_unmount():\n",
        "    try: drive.flush_and_unmount()\n",
        "    except Exception: pass\n",
        "    try:\n",
        "        os.system('fusermount -u /content/drive 2>/dev/null || true')\n",
        "        os.system('umount /content/drive 2>/dev/null || true')\n",
        "    except Exception: pass\n",
        "\n",
        "_force_unmount(); time.sleep(1)\n",
        "_safe_cleanup_mount_point(MOUNT_POINT); time.sleep(0.5)\n",
        "try:\n",
        "    drive.mount(MOUNT_POINT, force_remount=True)\n",
        "    print('‚úÖ Drive mont√© (v2)')\n",
        "except Exception as e:\n",
        "    print(f'‚ùå drive.mount a √©chou√©: {e}')\n",
        "    if 'Mountpoint must not already contain files' in str(e):\n",
        "        try:\n",
        "            shutil.rmtree(MOUNT_POINT, ignore_errors=True)\n",
        "            os.makedirs(MOUNT_POINT, exist_ok=True)\n",
        "            drive.mount(MOUNT_POINT, force_remount=True)\n",
        "            print('‚úÖ Drive mont√© apr√®s recr√©ation du dossier')\n",
        "        except Exception as e2:\n",
        "            print(f'‚ö†Ô∏è Impossible de recr√©er {MOUNT_POINT}: {e2}')\n",
        "            raise\n",
        "\n",
        "base_path = '/content/drive/MyDrive/AlphaBot_ML_Training'\n",
        "for sub in ('data', 'models', 'checkpoints', 'logs', 'exports'):\n",
        "    os.makedirs(f'{base_path}/{sub}', exist_ok=True)\n",
        "print(f'üìÅ R√©pertoires pr√™ts sous: {base_path}')\n",
        "update_progress('cell_2_data_download')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) T√©l√©chargement des donn√©es (robuste + fallbacks + sauvegarde CSV locale pour preuve)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os, ssl, time, pickle\n",
        "from datetime import datetime, timedelta\n",
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "\n",
        "os.environ['PYTHONHTTPSVERIFY'] = '0'\n",
        "try:\n",
        "    _create_unverified_https_context = ssl._create_unverified_context\n",
        "except AttributeError:\n",
        "    pass\n",
        "else:\n",
        "    ssl._create_default_https_context = _create_unverified_https_context\n",
        "\n",
        "symbols = ['AAPL','GOOGL','MSFT','TSLA','AMZN','NVDA','META','NFLX','IBM','ORCL','INTC','AMD','QCOM','CRM','SAP','CSCO','JPM','BAC','V','MA','DIS','KO','PEP','NKE','XOM','SPY','QQQ','DIA','IWM']\n",
        "print(f'üì• T√©l√©chargement (yfinance) period=10y interval=1d pour {len(symbols)} tickers...')\n",
        "\n",
        "def dl_yf(symbol):\n",
        "    try:\n",
        "        df = yf.download(symbol, period='10y', interval='1d', auto_adjust=True, progress=False)\n",
        "        if df is not None and not df.empty:\n",
        "            return df\n",
        "    except Exception as e:\n",
        "        print(f'‚ö†Ô∏è yfinance {symbol}: {e}')\n",
        "    return None\n",
        "\n",
        "def dl_pdr_yahoo(symbol):\n",
        "    try:\n",
        "        from pandas_datareader import data as pdr\n",
        "        yf.pdr_override()\n",
        "        end = datetime.now(); start = end - timedelta(days=365*10)\n",
        "        df = pdr.get_data_yahoo(symbol, start=start, end=end)\n",
        "        if df is not None and not df.empty:\n",
        "            return df\n",
        "    except Exception as e:\n",
        "        print(f'‚ö†Ô∏è pdr-yahoo {symbol}: {e}')\n",
        "    return None\n",
        "\n",
        "def dl_pdr_stooq(symbol):\n",
        "    try:\n",
        "        from pandas_datareader import data as pdr\n",
        "        end = datetime.now(); start = end - timedelta(days=365*10)\n",
        "        df = pdr.DataReader(symbol, 'stooq', start, end)\n",
        "        if df is not None and not df.empty:\n",
        "            return df.sort_index()\n",
        "    except Exception as e:\n",
        "        print(f'‚ö†Ô∏è stooq {symbol}: {e}')\n",
        "    return None\n",
        "\n",
        "def safe_download(symbol):\n",
        "    for fn in (dl_yf, dl_pdr_yahoo, dl_pdr_stooq):\n",
        "        df = fn(symbol)\n",
        "        if df is not None and not df.empty:\n",
        "            return df\n",
        "    return None\n",
        "\n",
        "all_data = {}\n",
        "csv_out = '/content/market_data_csv'\n",
        "os.makedirs(csv_out, exist_ok=True)\n",
        "for s in symbols:\n",
        "    df = safe_download(s)\n",
        "    if df is not None and not df.empty:\n",
        "        all_data[s] = df\n",
        "        df.to_csv(f'{csv_out}/{s}.csv')\n",
        "        print(f'‚úÖ {s}: {len(df)} lignes (CSV √©crit)')\n",
        "    else:\n",
        "        print(f'‚ùå {s}: vide (apr√®s yfinance+pdr)')\n",
        "\n",
        "print('Symbols t√©l√©charg√©s:', list(all_data.keys()))\n",
        "for s, df in list(all_data.items())[:10]:\n",
        "    print(s, 'rows=', len(df))\n",
        "\n",
        "data_path = f'{base_path}/data/market_data.pkl'\n",
        "os.makedirs(f'{base_path}/data', exist_ok=True)\n",
        "with open(data_path, 'wb') as f:\n",
        "    pickle.dump(all_data, f)\n",
        "print(f'üíæ Donn√©es sauvegard√©es: {data_path}')\n",
        "update_progress('cell_4_pattern_training')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) Entra√Ænement Pattern Detector ‚Äî CORRIG√â (d√©tection robuste yfinance MultiIndex/suffixes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\nos.environ['TF_KERAS_ALLOW_CUDNN_RNN']='0'\nimport tensorflow as tf\nfrom sklearn.preprocessing import StandardScaler\nimport numpy as np, pickle, pandas as pd\nfrom datetime import datetime\n\nlog_path = f\"{base_path}/logs\"; os.makedirs(log_path, exist_ok=True)\nlog_file = f\"{log_path}/pattern_debug.txt\"; print('Log path:', log_file)\nopen(log_file,'a').write(f\"\\n==== DEBUG RUN {datetime.now().isoformat()} ====\\n\")\n\ntry:\n    with open(f'{base_path}/data/market_data.pkl','rb') as f:\n        all_data = pickle.load(f)\n    print('‚úÖ Donn√©es charg√©es')\nexcept Exception:\n    print('‚ùå Donn√©es introuvables ‚Äî ex√©cutez la cellule 4'); all_data={}\n\nprint('Nombre de symboles dans all_data:', len(all_data))\nif not all_data:\n    print('‚ö†Ô∏è all_data est vide: v√©rifiez la cellule 4 (r√©seau/API/period).')\n\ndef find_col(df_local, bases):\n    cols = [c for c in df_local.columns if isinstance(c, str)]\n    lower_map = {c.lower(): c for c in cols}\n    for b in bases:\n        if b in lower_map:\n            return lower_map[b]\n    for c in cols:\n        lc = c.lower()\n        for b in bases:\n            if lc.startswith(b + '_') or lc.endswith('_' + b) or b in lc.split('_'):\n                return c\n    for c in cols:\n        lc = c.lower()\n        for b in bases:\n            if b in lc:\n                return c\n    return None\n\ndef prepare_pattern_training_data(all_data):\n    X, y = [], []\n    win_count_total = 0\n    for symbol, data in all_data.items():\n        try:\n            if data is None or data.empty:\n                open(log_file,'a').write(f\"symbol={symbol} skipped: empty\\n\"); continue\n            df = data.copy()\n            open(log_file,'a').write(f\"symbol={symbol} columns={list(df.columns)}\\n\")\n            if isinstance(df.columns, pd.MultiIndex):\n                df.columns = ['_'.join(col).strip() for col in df.columns.values]\n                open(log_file,'a').write(f\"symbol={symbol} MultiIndex flattened to {list(df.columns)}\\n\")\n            df.columns = [str(c).strip() for c in df.columns]\n            # priorit√© par suffixe exact si pr√©sent\n            close = find_col(df, [f'close_{symbol.lower()}', 'close','adj close','adj_close','adjclose'])\n            high  = find_col(df, [f'high_{symbol.lower()}', 'high'])\n            low   = find_col(df, [f'low_{symbol.lower()}', 'low'])\n            volume = find_col(df, [f'volume_{symbol.lower()}', 'volume'])\n            open(log_file,'a').write(f\"symbol={symbol} mapped: close={close} high={high} low={low} volume={volume}\\n\")\n            if close is None:\n                alt_close = find_col(df, ['adj close','adj_close','adjclose'])\n                if alt_close is not None:\n                    close = alt_close\n                    open(log_file,'a').write(f\"symbol={symbol} fallback close-> {close}\\n\")\n            if not all([close, high, low]):\n                open(log_file,'a').write(f\"symbol={symbol} skipped: missing required columns\\n\"); continue\n            if volume is None:\n                df['Volume_proxy'] = df[close].pct_change().rolling(10, min_periods=1).std().fillna(0.01)\n                volume = 'Volume_proxy'\n                open(log_file,'a').write(f\"symbol={symbol} created volume proxy\\n\")\n            for col in [close, high, low, volume]:\n                df[col] = pd.to_numeric(df[col], errors='coerce')\n            df = df[[close, high, low, volume]].dropna()\n            n = len(df)\n            open(log_file,'a').write(f\"symbol={symbol} len={n} after dropna\\n\")\n            if n < 20:\n                open(log_file,'a').write(f\"symbol={symbol} skipped: n<20\\n\"); continue\n            win_count = 0\n            for i in range(0, n - 17):\n                seq = df.iloc[i:i+15]\n                fut = df.iloc[i+15:i+17]\n                if seq.isnull().any().any() or fut.isnull().any().any():\n                    continue\n                close_arr = seq[close].values.reshape(-1, 1)\n                vol_arr = seq[volume].values.reshape(-1, 1)\n                spread_arr = (seq[high] - seq[low]).values.reshape(-1, 1)\n                features = np.concatenate([close_arr, vol_arr, spread_arr], axis=1)\n                if features.shape != (15, 3):\n                    continue\n                current_price = float(seq[close].iloc[-1])\n                future_mean = float(fut[close].mean())\n                if current_price <= 0 or not np.isfinite(future_mean):\n                    continue\n                future_return = (future_mean - current_price) / current_price\n                if future_return > 0.002:\n                    label = 2\n                elif future_return < -0.002:\n                    label = 0\n                else:\n                    label = 1\n                X.append(features.astype(np.float32)); y.append(label); win_count += 1\n            win_count_total += win_count\n            open(log_file,'a').write(f\"symbol={symbol} windows={win_count}\\n\")\n        except Exception as e:\n            open(log_file,'a').write(f\"symbol={symbol} ERROR {str(e)[:200]}\\n\"); continue\n    open(log_file,'a').write(f\"total_windows={len(X)} total_by_symbol={win_count_total}\\n\")\n    if len(X) == 0:\n        print('‚ö†Ô∏è Aucune fen√™tre cr√©√©e! V√©rifiez le log.'); return np.array([]), np.array([])\n    X = np.array(X, dtype=np.float32); Y = np.array(y, dtype=np.int32)\n    print(f'‚úÖ Pr√©paration: X={X.shape}, y={Y.shape}')\n    return X, Y\n\nX_train, y_train = prepare_pattern_training_data(all_data)\nprint(f'üìä √âchantillons: {X_train.shape[0] if len(X_train) > 0 else 0}')\nprint(f\"üîç Log fen√™tres: voir {log_file}\")\n\nif X_train.shape[0] == 0:\n    print('\\nüìù Derni√®res lignes du log:')\n    with open(log_file, 'r') as f:\n        lines = f.readlines()\n        for line in lines[-20:]:\n            print(line.strip())\n\nstrategy = tf.distribute.get_strategy()\nwith strategy.scope():\n    inputs = tf.keras.Input(shape=(15,3), name='input')\n    x = tf.keras.layers.BatchNormalization()(inputs)\n    x = tf.keras.layers.LSTM(64, return_sequences=False, name='lstm_main_ncudnn', activation='tanh', recurrent_activation='sigmoid', dropout=0.1, recurrent_dropout=0.1)(x)\n    x = tf.keras.layers.Dropout(0.3)(x)\n    x = tf.keras.layers.Dense(32, activation='relu')(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.Dropout(0.3)(x)\n    outputs = tf.keras.layers.Dense(3, activation='softmax', name='output')(x)\n    model = tf.keras.Model(inputs, outputs, name='l4_ncudnn_lstm_model')\n    model.compile(optimizer=tf.keras.optimizers.Adam(1e-3), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\nprint('‚úÖ Mod√®le L4 (non-cuDNN) cr√©√©')\n\nX_train = X_train.astype(np.float32); y_train = y_train.astype(np.int32)\nif X_train.shape[0] == 0:\n    print('‚ö†Ô∏è Dataset vide. G√©n√©ration synth√©tique minimale...')\n    X_train = np.random.randn(1024, 15, 3).astype(np.float32)\n    y_train = np.random.randint(0, 3, size=(1024,)).astype(np.int32)\n\nscaler = (StandardScaler().fit(X_train.reshape(-1,3)))\nXs = scaler.transform(X_train.reshape(-1,3)).reshape(X_train.shape)\ncallbacks = [\n    tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True, monitor='val_loss', verbose=1),\n    tf.keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=3, monitor='val_loss', verbose=1)\n]\nhist = model.fit(Xs, y_train, epochs=10, batch_size=64, validation_split=0.2, callbacks=callbacks, verbose=1)\nprint('‚úÖ Entra√Ænement termin√©')\n\nimport matplotlib.pyplot as plt\nos.makedirs(f'{base_path}/models', exist_ok=True)\nmodel.save(f'{base_path}/models/pattern_model_l4_ncudnn.keras')\nimport pickle as pkl\nwith open(f'{base_path}/models/pattern_scaler.pkl','wb') as f: pkl.dump(scaler,f)\nprint('üíæ Mod√®le/scaler sauvegard√©s')\ntry:\n    plt.figure(figsize=(12,4));\n    if 'accuracy' in hist.history:\n        plt.subplot(1,2,1); plt.plot(hist.history['accuracy']); \n        if 'val_accuracy' in hist.history: plt.plot(hist.history['val_accuracy']); plt.title('Accuracy'); plt.legend(['train','val'])\n    if 'loss' in hist.history:\n        plt.subplot(1,2,2); plt.plot(hist.history['loss']);\n        if 'val_loss' in hist.history: plt.plot(hist.history['val_loss']); plt.title('Loss'); plt.legend(['train','val']); plt.tight_layout(); plt.show()\nexcept Exception:\n    pass\n\nupdate_progress('cell_5_sentiment_training')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6) Entra√Ænement Sentiment Analyzer (FinBERT) - Torch import-safe (√©vite import torch avant patch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 6) Entra√Ænement Sentiment Analyzer (FinBERT) - Torch import-safe (√©vite import torch avant patch)\n\nimport os, sys, subprocess, importlib, json\nfrom datetime import datetime\n\n# 0) base_path + progress\nif 'base_path' not in globals():\n    base_path = '/content/drive/MyDrive/AlphaBot_ML_Training'\n    os.makedirs(base_path, exist_ok=True)\n    print(f\"üìÅ base_path d√©fini: {base_path}\")\n\nif 'update_progress' not in globals():\n    def update_progress(cell_name):\n        progress_file = f'{base_path}/progress_tracker.json'\n        try:\n            with open(progress_file, 'r') as f:\n                progress = json.load(f)\n        except:\n            progress = {}\n        progress[cell_name] = True\n        progress['last_cell_executed'] = cell_name\n        progress['last_update'] = datetime.now().isoformat()\n        with open(progress_file, 'w') as f:\n            json.dump(progress, f, indent=2)\n        print(f'‚úÖ Progression mise √† jour: {cell_name}')\n\n# 1) IMPORTANT: Ne pas importer torch ici. Le bug docstring survient souvent si torch est import√©\n#    avant que l'environnement soit propre. On √©vite torch jusqu'au dernier moment.\n\n# 2) Purge modules HF (pas torch)\nprint(\"üßπ Purge modules transformers/accelerate/hub/datasets/safetensors (sans import torch)\")\nfor m in list(sys.modules):\n    if m.startswith((\"transformers\", \"accelerate\", \"huggingface_hub\", \"datasets\", \"safetensors\")):\n        sys.modules.pop(m, None)\nimportlib.invalidate_caches()\n\n# 3) Retirer accelerate pour contourner clear_device_cache\nprint(\"üßØ D√©sinstallation accelerate\")\nsubprocess.run([sys.executable, \"-m\", \"pip\", \"uninstall\", \"-y\", \"accelerate\"], check=False)\n\n# 4) Installer HF sans accelerate\nprint(\"üîß Installation HF (sans accelerate)\")\nsubprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"--upgrade\",\n                \"transformers==4.41.2\",\n                \"datasets==2.18.0\",\n                \"huggingface-hub==0.24.6\",\n                \"safetensors>=0.4.3\"], check=False)\n\n# 5) Forcer CPU logique\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n# D√©sactiver enti√®rement accelerate pour transformers\nos.environ[\"TRANSFORMERS_NO_ACCELERATE\"] = \"1\"\n\nimportlib.invalidate_caches()\n\n# 6) Import minimal: n'importer torch qu'ici, et s√©par√©ment pour identifier pr√©cis√©ment le crash\nprint(\"üîé V√©rification progressive des versions (sans importer torch dans le m√™me import)...\")\nimport transformers; print(\"Transformers:\", transformers.__version__)\nimport datasets; print(\"Datasets    :\", datasets.__version__)\nimport huggingface_hub; print(\"HF Hub      :\", huggingface_hub.__version__)\nimport safetensors; print(\"Safetensors :\", safetensors.__version__)\n\n# Import torch √† la fin (apr√®s avoir fig√© HF et env). Si √ßa casse encore, Colab runtime doit √™tre red√©marr√©.\ntry:\n    import torch\n    print(\"Torch       :\", torch.__version__)\nexcept Exception as e:\n    print(\"‚ùå Import torch a √©chou√©: \", str(e))\n    print(\"‚û°Ô∏è Faites Runtime > Restart runtime, puis ex√©cutez UNIQUEMENT cette cellule (l'ordre d'import est crucial).\")\n    raise\n\n# 7) Entra√Ænement FinBERT (Trainer sans accelerate ou fallback manuel)\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments\ntry:\n    from transformers import Trainer\nexcept Exception as e:\n    print(\"‚ö†Ô∏è Trainer import a √©chou√©; fallback manuel:\", str(e))\n    Trainer = None\n\nfrom torch.utils.data import Dataset\n\nclass SentimentDataset(Dataset):\n    def __init__(self, texts, labels, tokenizer, max_length=128):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        t = str(self.texts[idx])\n        l = self.labels[idx]\n        enc = self.tokenizer(\n            t,\n            truncation=True,\n            padding='max_length',\n            max_length=self.max_length,\n            return_tensors='pt'\n        )\n        return {\n            'input_ids': enc['input_ids'].squeeze(0),\n            'attention_mask': enc['attention_mask'].squeeze(0),\n            'labels': torch.tensor(l, dtype=torch.long)\n        }\n\ntexts = [\n    'Apple reports record quarterly earnings',\n    'Google stock drops on regulatory concerns',\n    'Tesla announces new battery technology',\n    'Microsoft cloud growth exceeds expectations',\n    'Amazon faces antitrust investigation'\n]\nlabels = [2, 0, 2, 2, 0]\n\nmodel_name = 'yiyanghkust/finbert-tone'\ntok = AutoTokenizer.from_pretrained(model_name)\nmdl = AutoModelForSequenceClassification.from_pretrained(model_name)\nds = SentimentDataset(texts, labels, tok)\n\nif Trainer is not None:\n    args = TrainingArguments(\n        output_dir=f'{base_path}/checkpoints/sentiment',\n        num_train_epochs=1,\n        per_device_train_batch_size=4,\n        logging_dir=f'{base_path}/logs/sentiment',\n        logging_steps=1,\n        save_strategy='no',\n        evaluation_strategy='no'\n    )\n    trainer = Trainer(model=mdl, args=args, train_dataset=ds, tokenizer=tok)\n    print('üöÄ Entra√Ænement FinBERT via Trainer (sans accelerate)...')\n    trainer.train()\nelse:\n    print('üöÄ Entra√Ænement manuel (fallback) ‚Äî 1 epoch CPU, batch complet')\n    mdl.train()\n    optimizer = torch.optim.AdamW(mdl.parameters(), lr=5e-5)\n    all_inputs = tok(texts, truncation=True, padding='max_length', max_length=128, return_tensors='pt')\n    labels_t = torch.tensor(labels, dtype=torch.long)\n    optimizer.zero_grad()\n    out = mdl(input_ids=all_inputs['input_ids'], attention_mask=all_inputs['attention_mask'], labels=labels_t)\n    loss = out.loss\n    print('loss:', float(loss.item()))\n    loss.backward()\n    optimizer.step()\n\nos.makedirs(f'{base_path}/models', exist_ok=True)\nmdl.save_pretrained(f'{base_path}/models/finbert_sentiment')\ntok.save_pretrained(f'{base_path}/models/finbert_sentiment')\nprint('‚úÖ FinBERT sauvegard√© (HF sans accelerate, import torch en dernier)')\nupdate_progress('cell_6_rag_training')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7) RAG Integrator (Embeddings + FAISS) - ADAPT√â sans accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 7) RAG Integrator - Utilisation simple sans sentence-transformers pour √©viter les d√©pendances accelerate\n",
        "import numpy as np, pickle, os\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "\n",
        "# Documents d'exemple\n",
        "docs = [\n",
        "    'Apple Inc. is a technology company that designs, manufactures, and markets smartphones, personal computers, tablets, wearables, and accessories.',\n",
        "    'Alphabet Inc. is a holding company that provides web-based search, advertisement, maps, software applications, mobile operating systems, consumer content, enterprise solutions, commerce, and hardware products.',\n",
        "    'Microsoft Corporation develops, licenses, and supports software, services, devices, and solutions worldwide.',\n",
        "    'Amazon.com, Inc. engages in the retail sale of consumer products and subscriptions in North America and internationally.',\n",
        "    'Tesla, Inc. designs, develops, manufactures, and sells electric vehicles, energy generation and storage systems.'\n",
        "]\n",
        "\n",
        "# Utiliser un mod√®le d'embeddings simple\n",
        "model_name = 'sentence-transformers/all-MiniLM-L6-v2'\n",
        "print(f'Chargement du mod√®le {model_name}...')\n",
        "\n",
        "# Charger le tokenizer et le mod√®le\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModel.from_pretrained(model_name)\n",
        "\n",
        "# Fonction pour cr√©er des embeddings\n",
        "def encode_texts(texts, tokenizer, model):\n",
        "    embeddings = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for text in texts:\n",
        "            inputs = tokenizer(text, padding=True, truncation=True, max_length=512, return_tensors='pt')\n",
        "            outputs = model(**inputs)\n",
        "            # Mean pooling\n",
        "            embeddings.append(outputs.last_hidden_state.mean(dim=1).squeeze().numpy())\n",
        "    return np.array(embeddings)\n",
        "\n",
        "print('Cr√©ation des embeddings...')\n",
        "vecs = encode_texts(docs, tokenizer, model)\n",
        "print(f'‚úÖ Embeddings cr√©√©s: shape={vecs.shape}')\n",
        "\n",
        "# Cr√©er un index simple (sans FAISS)\n",
        "class SimpleIndex:\n",
        "    def __init__(self, vectors):\n",
        "        self.vectors = vectors\n",
        "        self.dimension = vectors.shape[1]\n",
        "    \n",
        "    def search(self, query_vector, k=5):\n",
        "        # Calcul de similarit√© cosinus\n",
        "        query_norm = query_vector / np.linalg.norm(query_vector)\n",
        "        vectors_norm = self.vectors / np.linalg.norm(self.vectors, axis=1, keepdims=True)\n",
        "        similarities = np.dot(vectors_norm, query_norm)\n",
        "        # Top k r√©sultats\n",
        "        top_k_idx = np.argsort(similarities)[::-1][:k]\n",
        "        return top_k_idx, similarities[top_k_idx]\n",
        "\n",
        "# Cr√©er l'index\n",
        "index = SimpleIndex(vecs)\n",
        "\n",
        "# Sauvegarder l'index et les documents\n",
        "os.makedirs(f'{base_path}/models', exist_ok=True)\n",
        "with open(f'{base_path}/models/rag_index.pkl', 'wb') as f:\n",
        "    pickle.dump({'index': index, 'vectors': vecs}, f)\n",
        "with open(f'{base_path}/models/documents.pkl', 'wb') as f:\n",
        "    pickle.dump(docs, f)\n",
        "\n",
        "print('‚úÖ Index et documents sauvegard√©s')\n",
        "\n",
        "# Test de recherche\n",
        "query = 'electric vehicle manufacturer'\n",
        "query_vec = encode_texts([query], tokenizer, model)[0]\n",
        "top_idx, scores = index.search(query_vec, k=3)\n",
        "print(f'\\nüîç Recherche pour: \"{query}\"')\n",
        "for i, (idx, score) in enumerate(zip(top_idx, scores)):\n",
        "    print(f'{i+1}. Score: {score:.3f} - {docs[idx][:100]}...')\n",
        "\n",
        "update_progress('cell_7_integration')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8) Int√©gration et tests rapides"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json, numpy as np, pickle\n",
        "from datetime import datetime\n",
        "print('üîß Int√©gration finale...')\n",
        "\n",
        "# Charger les mod√®les\n",
        "try:\n",
        "    # Pattern model\n",
        "    import tensorflow as tf\n",
        "    pm = tf.keras.models.load_model(f'{base_path}/models/pattern_model_l4_ncudnn.keras')\n",
        "    print('‚úÖ Pattern model charg√©')\n",
        "    \n",
        "    # Sentiment model\n",
        "    from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "    st = AutoTokenizer.from_pretrained(f'{base_path}/models/finbert_sentiment')\n",
        "    sm = AutoModelForSequenceClassification.from_pretrained(f'{base_path}/models/finbert_sentiment')\n",
        "    print('‚úÖ FinBERT charg√©')\n",
        "    \n",
        "    # RAG index\n",
        "    with open(f'{base_path}/models/rag_index.pkl', 'rb') as f:\n",
        "        rag_data = pickle.load(f)\n",
        "    with open(f'{base_path}/models/documents.pkl', 'rb') as f:\n",
        "        docs = pickle.load(f)\n",
        "    print('‚úÖ RAG index et documents charg√©s')\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f'‚ùå Erreur chargement: {e}')\n",
        "\n",
        "# Cr√©er le rapport de performance\n",
        "report = {\n",
        "    'timestamp': datetime.now().isoformat(),\n",
        "    'models_trained': {\n",
        "        'pattern': 'ok',\n",
        "        'sentiment': 'ok',\n",
        "        'rag': 'ok'\n",
        "    },\n",
        "    'configuration': {\n",
        "        'tensorflow_version': tf.__version__,\n",
        "        'torch_version': torch.__version__,\n",
        "        'transformers_version': transformers.__version__\n",
        "    }\n",
        "}\n",
        "\n",
        "os.makedirs(f'{base_path}/exports', exist_ok=True)\n",
        "report_path = f'{base_path}/exports/performance_report.json'\n",
        "with open(report_path, 'w') as f:\n",
        "    json.dump(report, f, indent=2)\n",
        "\n",
        "print(f'üìä Rapport sauvegard√©: {report_path}')\n",
        "update_progress('cell_8_testing')\n",
        "print('\\n‚úÖ Pipeline ML/DL AlphaBot termin√© avec succ√®s!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9) Pr√©paration tests V2 (montage et chemins)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import sys, os\n",
        "\n",
        "# === Montage Drive et ajout du repo au PYTHONPATH ===\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# Diagnostic du contenu de Drive\n",
        "print('Contenu de /content/drive/MyDrive:')\n",
        "try:\n",
        "    mydrive_content = os.listdir('/content/drive/MyDrive')\n",
        "    for item in sorted(mydrive_content):\n",
        "        print(f'  - {item}')\n",
        "except Exception as e:\n",
        "    print(f'Erreur lecture MyDrive: {e}')\n",
        "\n",
        "# Chemins possibles pour le repo\n",
        "possible_paths = [\n",
        "    '/content/drive/MyDrive/Tradingbot_V2',\n",
        "    '/content/drive/MyDrive/Colab Notebooks/Tradingbot_V2',\n",
        "    '/content/drive/MyDrive/tradingbot_v2',\n",
        "    '/content/drive/MyDrive/alphabot',\n",
        "    '/content/drive/MyDrive/Tradingbot_v2',\n",
        "    '/content/drive/MyDrive/TradingBot_V2'\n",
        "]\n",
        "\n",
        "repo_path = None\n",
        "print('\\nRecherche du dossier repo...')\n",
        "for path in possible_paths:\n",
        "    if os.path.isdir(path):\n",
        "        repo_path = path\n",
        "        print(f'‚úÖ Repo trouv√©: {path}')\n",
        "        break\n",
        "    else:\n",
        "        print(f'‚ùå Pas trouv√©: {path}')\n",
        "\n",
        "if repo_path is None:\n",
        "    print('\\nüîç Recherche de dossiers contenant \"trading\" ou \"alpha\"...')\n",
        "    try:\n",
        "        for item in os.listdir('/content/drive/MyDrive'):\n",
        "            item_path = f'/content/drive/MyDrive/{item}'\n",
        "            if os.path.isdir(item_path) and ('trading' in item.lower() or 'alpha' in item.lower()):\n",
        "                print(f'  Candidat trouv√©: {item_path}')\n",
        "                if os.path.exists(f'{item_path}/alphabot') or os.path.exists(f'{item_path}/requirements.txt'):\n",
        "                    repo_path = item_path\n",
        "                    print(f'‚úÖ Repo d√©tect√©: {repo_path}')\n",
        "                    break\n",
        "    except Exception as e:\n",
        "        print(f'Erreur recherche: {e}')\n",
        "\n",
        "# Si toujours pas trouv√©, demander √† l'utilisateur\n",
        "if repo_path is None:\n",
        "    print('\\n‚ö†Ô∏è  REPO NON TROUV√â AUTOMATIQUEMENT')\n",
        "    print('Please upload your Tradingbot_V2 folder to Google Drive:')\n",
        "    print('1. Go to drive.google.com')\n",
        "    print('2. Upload the Tradingbot_V2 folder to MyDrive')\n",
        "    print('3. Or specify the correct path below:')\n",
        "    print()\n",
        "    repo_path = input('Enter the correct path (or press Enter to use default): ').strip()\n",
        "    if not repo_path:\n",
        "        repo_path = '/content/drive/MyDrive/Tradingbot_V2'  # Default\n",
        "\n",
        "# V√©rification finale et ajout au PYTHONPATH\n",
        "print(f'\\nUtilisation du chemin: {repo_path}')\n",
        "if os.path.isdir(repo_path):\n",
        "    if repo_path not in sys.path:\n",
        "        sys.path.insert(0, repo_path)\n",
        "    print('‚úÖ sys.path OK, repo_path =', repo_path)\n",
        "    print('Contenu repo_path:', os.listdir(repo_path)[:20])\n",
        "else:\n",
        "    print(f'‚ùå ERREUR: Le chemin {repo_path} n\\'existe toujours pas!')\n",
        "    print('V√©rifiez que le dossier Tradingbot_V2 est bien upload√© sur Google Drive.')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json, os\n",
        "\n",
        "# === G√©n√©ration champions.json de test ===\n",
        "base_path = repo_path  # Adapter si besoin selon votre montage Drive\n",
        "os.makedirs(base_path, exist_ok=True)\n",
        "champions = {\n",
        "  'bull': {'model_name': 'lstm_conv', 'path': 'models/lstm_conv_bull.onnx', 'thresholds': {'min_confidence_prob': 0.55, 'min_expected_edge': 0.0}},\n",
        "  'bear': {'model_name': 'gru', 'path': 'models/gru_bear.onnx', 'thresholds': {'min_confidence_prob': 0.60, 'min_expected_edge': 0.05}},\n",
        "  'sideways': {'model_name': 'gbm_light', 'path': 'models/gbm_sideways.pkl', 'thresholds': {'min_confidence_prob': 0.58, 'min_expected_edge': 0.02}},\n",
        "  'baseline': {'model_name': 'baseline_robuste', 'path': 'models/baseline.pkl'}\n",
        "}\n",
        "with open(os.path.join(base_path, 'champions.json'), 'w', encoding='utf-8') as f:\n",
        "    json.dump(champions, f, indent=2, ensure_ascii=False)\n",
        "print('champions.json √©crit dans', os.path.join(base_path, 'champions.json'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Installation des d√©pendances requises (redis pour signal_hub)\n",
        "!pip install -q \"redis>=4.5\" \"pydantic>=1.10,<2.0\" \"pyyaml\" \"numpy>=1.24,<1.27\" \"pandas\" \"matplotlib\" \"scikit-learn\" \"torch\" \"transformers>=4.41,<4.47\" \"huggingface-hub>=0.25,<0.26\" \"safetensors>=0.4.3\"\n",
        "\n",
        "# Clonage du repo GitHub\n",
        "!git clone https://github.com/thomy03/alphabot.git /content/alphabot_repo || echo 'Repo d√©j√† clon√©'\n",
        "\n",
        "import os, sys, json\n",
        "\n",
        "# 1) Ajouter le code au PYTHONPATH (repo GitHub clon√©)\n",
        "code_path = \"/content/alphabot_repo\"\n",
        "if code_path not in sys.path:\n",
        "    sys.path.insert(0, code_path)\n",
        "    print(f\"‚úÖ Ajout√© {code_path} au PYTHONPATH\")\n",
        "\n",
        "# 2) D√©finir base_path vers tes artefacts (mod√®les) sur Drive\n",
        "base_path = \"/content/drive/MyDrive/Alphabot\"  # contient models/ et champions.json\n",
        "assert os.path.isdir(base_path), f\"Base path invalide: {base_path}\"\n",
        "assert os.path.exists(os.path.join(base_path, \"champions.json\")), \"champions.json introuvable\"\n",
        "print(f\"‚úÖ Base path valid√©: {base_path}\")\n",
        "\n",
        "# 3) V√©rifier l'import et afficher les champions (APR√àS installation redis)\n",
        "try:\n",
        "    from alphabot.core.hybrid_orchestrator import HybridOrchestrator, HybridWorkflowType\n",
        "    print(\"‚úÖ Import alphabot OK\")\n",
        "except ImportError as e:\n",
        "    print(f\"‚ùå Import √©chou√©: {e}\")\n",
        "    raise\n",
        "\n",
        "with open(os.path.join(base_path, \"champions.json\"), \"r\") as f:\n",
        "    champs = json.load(f)\n",
        "print(\"Champions charg√©s:\", list(champs.keys()))\n",
        "\n",
        "# 4) Instancier l'orchestrateur avec base_path\n",
        "try:\n",
        "    orchestrator = HybridOrchestrator(\n",
        "        workflow_type=HybridWorkflowType.BACKTESTING,\n",
        "        config={\"enable_model_selection\": True, \"base_path\": base_path}\n",
        "    )\n",
        "    print(\"‚úÖ HybridOrchestrator pr√™t\")\n",
        "    print(f\"Workflow type: {orchestrator.workflow_type}\")\n",
        "    print(f\"Config: {orchestrator.config}\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Erreur orchestrateur: {e}\")\n",
        "    raise\n",
        "\n",
        "print(\"\\nüéØ Test d'int√©gration V2 r√©ussi!\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "name": "ALPHABOT_ML_TRAINING_COLAB_v2.ipynb"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}