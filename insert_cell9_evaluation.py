import json
import copy

NB_PATH = "ALPHABOT_ML_TRAINING_COLAB_v2.ipynb"

cell9_markdown = {
  "cell_type": "markdown",
  "metadata": {},
  "source": [
    "## 9) √âvaluation & Visualisations (Pattern / FinBERT / RAG)\n",
    "\n",
    "Cette section calcule des m√©triques quantitatives et g√©n√®re des visualisations rapides:\n",
    "- Pattern: accuracy + matrice de confusion sur un holdout\n",
    "- FinBERT: F1 macro/micro sur un √©chantillon de phrases annot√©es (mini-benchmark)\n",
    "- RAG: Recall@k sur quelques requ√™tes de test\n",
    "Les r√©sultats sont export√©s sous /exports/ (JSON + PNG)\n"
  ]
}

cell9_code = {
  "cell_type": "code",
  "execution_count": None,
  "metadata": {},
  "outputs": [],
  "source": [
    "import os, json, pickle, numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, f1_score\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoTokenizer, AutoModel\n",
    "\n",
    "os.makedirs(f\"{base_path}/exports\", exist_ok=True)\n",
    "\n",
    "def save_json(obj, path):\n",
    "    with open(path, 'w') as f:\n",
    "        json.dump(obj, f, indent=2)\n",
    "    print(f\"üíæ JSON sauvegard√©: {path}\")\n",
    "\n",
    "# ==== 9.A Pattern Evaluation ====\n",
    "pattern_metrics = {}\n",
    "try:\n",
    "    # Charger mod√®le/scaler\n",
    "    pattern_model_path = f\"{base_path}/models/pattern_model_l4_ncudnn.keras\"\n",
    "    scaler_path = f\"{base_path}/models/pattern_scaler.pkl\"\n",
    "    model = tf.keras.models.load_model(pattern_model_path)\n",
    "    scaler = pickle.load(open(scaler_path, 'rb'))\n",
    "\n",
    "    # Recharger donn√©es brutes\n",
    "    with open(f'{base_path}/data/market_data.pkl','rb') as f:\n",
    "        all_data = pickle.load(f)\n",
    "\n",
    "    # Fonction utilitaire (similaire √† la pr√©paration d'entra√Ænement)\n",
    "    import pandas as pd\n",
    "    def find_col(df_local, bases):\n",
    "        cols = [c for c in df_local.columns if isinstance(c, str)]\n",
    "        lower_map = {c.lower(): c for c in cols}\n",
    "        for b in bases:\n",
    "            if b in lower_map:\n",
    "                return lower_map[b]\n",
    "        for c in cols:\n",
    "            lc = c.lower()\n",
    "            for b in bases:\n",
    "                if lc.startswith(b + '_') or lc.endswith('_' + b) or b in lc.split('_'):\n",
    "                    return c\n",
    "        for c in cols:\n",
    "            lc = c.lower()\n",
    "            for b in bases:\n",
    "                if b in lc:\n",
    "                    return c\n",
    "        return None\n",
    "\n",
    "    def build_dataset(all_data):\n",
    "        X, y = [], []\n",
    "        for symbol, data in all_data.items():\n",
    "            try:\n",
    "                if data is None or data.empty:\n",
    "                    continue\n",
    "                df = data.copy()\n",
    "                if isinstance(df.columns, pd.MultiIndex):\n",
    "                    df.columns = ['_'.join(col).strip() for col in df.columns.values]\n",
    "                df.columns = [str(c).strip() for c in df.columns]\n",
    "                close = find_col(df, [f'close_{symbol.lower()}', 'close','adj close','adj_close','adjclose'])\n",
    "                high  = find_col(df, [f'high_{symbol.lower()}', 'high'])\n",
    "                low   = find_col(df, [f'low_{symbol.lower()}', 'low'])\n",
    "                volume = find_col(df, [f'volume_{symbol.lower()}', 'volume'])\n",
    "                if close is None:\n",
    "                    alt_close = find_col(df, ['adj close','adj_close','adjclose'])\n",
    "                    if alt_close is not None:\n",
    "                        close = alt_close\n",
    "                if not all([close, high, low]):\n",
    "                    continue\n",
    "                if volume is None:\n",
    "                    df['Volume_proxy'] = df[close].pct_change().rolling(10, min_periods=1).std().fillna(0.01)\n",
    "                    volume = 'Volume_proxy'\n",
    "                for col in [close, high, low, volume]:\n",
    "                    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "                df = df[[close, high, low, volume]].dropna()\n",
    "                n = len(df)\n",
    "                if n < 20:\n",
    "                    continue\n",
    "                for i in range(0, n - 17):\n",
    "                    seq = df.iloc[i:i+15]\n",
    "                    fut = df.iloc[i+15:i+17]\n",
    "                    if seq.isnull().any().any() or fut.isnull().any().any():\n",
    "                        continue\n",
    "                    close_arr = seq[close].values.reshape(-1, 1)\n",
    "                    vol_arr = seq[volume].values.reshape(-1, 1)\n",
    "                    spread_arr = (seq[high] - seq[low]).values.reshape(-1, 1)\n",
    "                    features = np.concatenate([close_arr, vol_arr, spread_arr], axis=1)\n",
    "                    if features.shape != (15, 3):\n",
    "                        continue\n",
    "                    current_price = float(seq[close].iloc[-1])\n",
    "                    future_mean = float(fut[close].mean())\n",
    "                    if current_price <= 0 or not np.isfinite(future_mean):\n",
    "                        continue\n",
    "                    future_return = (future_mean - current_price) / current_price\n",
    "                    if future_return > 0.002:\n",
    "                        label = 2\n",
    "                    elif future_return < -0.002:\n",
    "                        label = 0\n",
    "                    else:\n",
    "                        label = 1\n",
    "                    X.append(features.astype(np.float32))\n",
    "                    y.append(label)\n",
    "            except Exception:\n",
    "                continue\n",
    "        return np.array(X, dtype=np.float32), np.array(y, dtype=np.int32)\n",
    "\n",
    "    X_all, y_all = build_dataset(all_data)\n",
    "    if len(X_all) == 0:\n",
    "        # fallback: donn√©es synth√©tiques\n",
    "        X_all = np.random.randn(2048, 15, 3).astype(np.float32)\n",
    "        y_all = np.random.randint(0, 3, size=(2048,)).astype(np.int32)\n",
    "\n",
    "    # Holdout 20%\n",
    "    n = len(X_all)\n",
    "    idx = np.arange(n)\n",
    "    np.random.seed(42)\n",
    "    np.random.shuffle(idx)\n",
    "    split = int(0.8 * n)\n",
    "    test_idx = idx[split:]\n",
    "    X_test, y_test = X_all[test_idx], y_all[test_idx]\n",
    "\n",
    "    # Scale\n",
    "    Xs = scaler.transform(X_test.reshape(-1, 3)).reshape(X_test.shape)\n",
    "\n",
    "    # Predict\n",
    "    y_pred = model.predict(Xs, verbose=0)\n",
    "    y_hat = np.argmax(y_pred, axis=1).astype(np.int32)\n",
    "\n",
    "    acc = float(accuracy_score(y_test, y_hat))\n",
    "    cm = confusion_matrix(y_test, y_hat).tolist()\n",
    "    cls_report = classification_report(y_test, y_hat, output_dict=True)\n",
    "\n",
    "    pattern_metrics = {\n",
    "        'accuracy': acc,\n",
    "        'confusion_matrix': cm,\n",
    "        'classification_report': cls_report,\n",
    "        'samples': int(len(y_test))\n",
    "    }\n",
    "\n",
    "    # Plot CM\n",
    "    fig, ax = plt.subplots(figsize=(4,4))\n",
    "    ax.imshow(confusion_matrix(y_test, y_hat), cmap='Blues')\n",
    "    ax.set_title('Pattern - Confusion Matrix')\n",
    "    ax.set_xlabel('Pred')\n",
    "    ax.set_ylabel('True')\n",
    "    for (i, j), val in np.ndenumerate(confusion_matrix(y_test, y_hat)):\n",
    "        ax.text(j, i, int(val), ha='center', va='center', color='black')\n",
    "    plt.tight_layout()\n",
    "    cm_path = f\"{base_path}/exports/pattern_confusion_matrix.png\"\n",
    "    plt.savefig(cm_path)\n",
    "    plt.close(fig)\n",
    "    print(f\"üñºÔ∏è Matrice de confusion sauvegard√©e: {cm_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Pattern eval erreur: {e}\")\n",
    "\n",
    "# ==== 9.B FinBERT Evaluation (mini-benchmark) ====\n",
    "finbert_metrics = {}\n",
    "try:\n",
    "    finbert_dir = f\"{base_path}/models/finbert_sentiment\"\n",
    "    tok = AutoTokenizer.from_pretrained(finbert_dir)\n",
    "    mdl = AutoModelForSequenceClassification.from_pretrained(finbert_dir)\n",
    "    mdl.eval()\n",
    "    # Mini-benchmark simple (√† adapter avec vos donn√©es):\n",
    "    samples = [\n",
    "        (\"The company's earnings beat expectations and the outlook is positive\", 2),\n",
    "        (\"The firm announced layoffs due to declining sales\", 0),\n",
    "        (\"Market remains uncertain with mixed indicators\", 1)\n",
    "    ]\n",
    "    y_true, y_pred = [], []\n",
    "    with torch.no_grad():\n",
    "        for text, label in samples:\n",
    "            inputs = tok(text, return_tensors='pt', truncation=True, padding=True, max_length=128)\n",
    "            outputs = mdl(**inputs)\n",
    "            pred = int(torch.argmax(outputs.logits, dim=1).item())\n",
    "            y_true.append(label); y_pred.append(pred)\n",
    "    f1_macro = float(f1_score(y_true, y_pred, average='macro'))\n",
    "    f1_micro = float(f1_score(y_true, y_pred, average='micro'))\n",
    "    finbert_metrics = {\n",
    "        'f1_macro': f1_macro,\n",
    "        'f1_micro': f1_micro,\n",
    "        'samples': len(samples)\n",
    "    }\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è FinBERT eval erreur: {e}\")\n",
    "\n",
    "# ==== 9.C RAG Evaluation (Recall@k) ====\n",
    "rag_metrics = {}\n",
    "try:\n",
    "    with open(f'{base_path}/models/rag_index.pkl', 'rb') as f:\n",
    "        rag_data = pickle.load(f)\n",
    "    with open(f'{base_path}/models/documents.pkl', 'rb') as f:\n",
    "        docs = pickle.load(f)\n",
    "    vecs = rag_data.get('vectors')\n",
    "    # Recr√©er un encodeur simple\n",
    "    model_name = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "    tok2 = AutoTokenizer.from_pretrained(model_name)\n",
    "    mdl2 = AutoModel.from_pretrained(model_name)\n",
    "    mdl2.eval()\n",
    "    def encode_texts(texts):\n",
    "        embs = []\n",
    "        with torch.no_grad():\n",
    "            for t in texts:\n",
    "                inp = tok2(t, return_tensors='pt', truncation=True, padding=True, max_length=256)\n",
    "                out = mdl2(**inp)\n",
    "                embs.append(out.last_hidden_state.mean(dim=1).squeeze().numpy())\n",
    "        return np.array(embs)\n",
    "    # Queries de test rudimentaires (doivent correspondre √† l'esprit des docs)\n",
    "    queries = [\n",
    "        (\"electric vehicles manufacturer\", 4),\n",
    "        (\"cloud software company\", 2),\n",
    "        (\"online retail and subscriptions\", 3)\n",
    "    ]\n",
    "    def recall_at_k(query, target_doc_idx, k=3):\n",
    "        qv = encode_texts([query])[0]\n",
    "        qn = qv / np.linalg.norm(qv)\n",
    "        dn = vecs / np.linalg.norm(vecs, axis=1, keepdims=True)\n",
    "        sims = (dn @ qn)\n",
    "        top = np.argsort(sims)[::-1][:k]\n",
    "        return float(1.0 if target_doc_idx in top else 0.0)\n",
    "    results = {}\n",
    "    for k in (3,5,10):\n",
    "        scores = [recall_at_k(q, idx, k=k) for (q, idx) in queries]\n",
    "        results[f'recall@{k}'] = float(np.mean(scores))\n",
    "    rag_metrics = results\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è RAG eval erreur: {e}\")\n",
    "\n",
    "# ==== Sauvegarde des m√©triques ====\n",
    "metrics = {\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'pattern': pattern_metrics,\n",
    "    'finbert': finbert_metrics,\n",
    "    'rag': rag_metrics\n",
    "}\n",
    "out_path = f\"{base_path}/exports/eval_metrics.json\"\n",
    "save_json(metrics, out_path)\n",
    "print(\"\\n‚úÖ √âvaluation Cellule 9 termin√©e.\")\n"
  ]
}

def main():
    with open(NB_PATH, "r", encoding="utf-8") as f:
        nb = json.load(f)
    # Ins√©rer Cellule 9 √† la fin (si non d√©j√† pr√©sente)
    cells = nb.get("cells", [])
    # D√©tecter si une cellule markdown titre contient '## 9) √âvaluation'
    already = any(
        c.get("cell_type") == "markdown" and any("## 9) √âvaluation" in line for line in c.get("source", []))
        for c in cells
    )
    if not already:
        cells.append(copy.deepcopy(cell9_markdown))
        cells.append(copy.deepcopy(cell9_code))
        nb["cells"] = cells
        with open(NB_PATH, "w", encoding="utf-8") as f:
            json.dump(nb, f, indent=2, ensure_ascii=False)
        print("‚úÖ Cellule 9 ajout√©e au notebook.")
    else:
        print("‚ÑπÔ∏è Cellule 9 d√©j√† pr√©sente. Aucun changement.")

if __name__ == "__main__":
    main()
