import json

# Lire le contenu de cellule_6_code.txt
with open('cellule_6_code.txt', 'r', encoding='utf-8') as f:
    cell6_content = f.read()

# Créer le notebook avec toutes les cellules
notebook = {
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 🚀 AlphaBot ML/DL Training - Google Colab (v2)\n",
                "\n",
                "Notebook propre et corrigé (détection robuste des colonnes yfinance MultiIndex/suffixes) avec suivi/reprise, téléchargement de données robuste et fallbacks sûrs."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Ajustement du path pour que Colab trouve le module alphabot\n",
                "import sys\n",
                "sys.path.append('/content')\n",
                "sys.path.append('/content/alphabot')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": ["## 🔄 Suivi de Progression et Reprise Automatique"]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 🔄 Système de suivi et reprise automatique\n",
                "import os, json\n",
                "from datetime import datetime\n",
                "\n",
                "base_path = '/content/drive/MyDrive/AlphaBot_ML_Training'\n",
                "os.makedirs(base_path, exist_ok=True)\n",
                "progress_file = f'{base_path}/progress_tracker.json'\n",
                "\n",
                "default_progress = {\n",
                "    'cell_1_setup': False,\n",
                "    'cell_2_data_download': False,\n",
                "    'cell_3_data_analysis': False,\n",
                "    'cell_4_pattern_training': False,\n",
                "    'cell_5_sentiment_training': False,\n",
                "    'cell_6_rag_training': False,\n",
                "    'cell_7_integration': False,\n",
                "    'cell_8_testing': False,\n",
                "    'cell_9_deployment': False,\n",
                "    'last_cell_executed': None,\n",
                "    'start_time': None,\n",
                "    'last_update': None\n",
                "}\n",
                "\n",
                "try:\n",
                "    with open(progress_file, 'r') as f:\n",
                "        progress = json.load(f)\n",
                "    print('📊 Suivi de progression chargé')\n",
                "except Exception:\n",
                "    progress = default_progress.copy()\n",
                "    progress['start_time'] = datetime.now().isoformat()\n",
                "    print('🆕 Nouveau suivi de progression initialisé')\n",
                "\n",
                "def update_progress(cell_name):\n",
                "    progress.setdefault(cell_name, False)\n",
                "    progress[cell_name] = True\n",
                "    progress['last_cell_executed'] = cell_name\n",
                "    progress['last_update'] = datetime.now().isoformat()\n",
                "    with open(progress_file, 'w') as f:\n",
                "        json.dump(progress, f, indent=2)\n",
                "    print(f'✅ Progression mise à jour: {cell_name}')\n",
                "\n",
                "def check_progress():\n",
                "    print('\\n📋 État actuel de la progression:')\n",
                "    print('=' * 50)\n",
                "    completed = sum(1 for k, v in progress.items() if k.startswith('cell_') and isinstance(v, bool) and v)\n",
                "    total = sum(1 for k in default_progress.keys() if k.startswith('cell_'))\n",
                "    pct = (completed / total * 100) if total else 0.0\n",
                "    print(f'📊 Progression: {completed}/{total} étapes complétées ({pct:.1f}%)')\n",
                "    print(f'⏰ Démarré: {progress.get(\"start_time\", \"N/A\")}')\n",
                "    print(f'🔄 Dernière mise à jour: {progress.get(\"last_update\", \"N/A\")}')\n",
                "    print(f'📍 Dernière cellule: {progress.get(\"last_cell_executed\", \"Aucune\")}')\n",
                "    print('=' * 50)\n",
                "\n",
                "check_progress()\n",
                "print('\\n💡 Instructions:')\n",
                "print('1. Exécutez cette cellule pour voir l\\'état d\\'avancement')\n",
                "print('2. Chaque cellule mettra à jour automatiquement sa progression')\n",
                "print('3. Si le processus s\\'arrête, relancez simplement cette cellule')\n",
                "print('4. Continuez avec la cellule suggérée')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": ["## 1) Setup GPU/TPU et environnement Colab"]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys, os, subprocess\n",
                "\n",
                "def pip_install(pkgs):\n",
                "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q'] + pkgs)\n",
                "\n",
                "# Nettoyage des modules déjà importés\n",
                "for m in [m for m in list(sys.modules) if m.startswith('transformers') or m.startswith('accelerate') or m.startswith('numpy') or m.startswith('tensorflow') or m.startswith('torch')]:\n",
                "    del sys.modules[m]\n",
                "\n",
                "# Réinstaller numpy pour compatibilité binaire\n",
                "pip_install([\n",
                "    '--upgrade',\n",
                "    'numpy>=1.24.0,<1.27.0'\n",
                "])\n",
                "\n",
                "# Installer un set compatible (Option A)\n",
                "pip_install([\n",
                "    'transformers>=4.43,<4.47',\n",
                "    'accelerate>=0.30,<0.34',\n",
                "    'datasets>=2.18,<3.0',\n",
                "    'safetensors>=0.4.3',\n",
                "    'huggingface-hub>=0.23,<0.25'\n",
                "])\n",
                "\n",
                "os.environ['HF_HUB_DISABLE_TELEMETRY'] = '1'\n",
                "\n",
                "# Importer et afficher les versions\n",
                "import numpy\n",
                "import transformers\n",
                "print('NumPy version:', numpy.__version__)\n",
                "print('Transformers version:', transformers.__version__)\n",
                "\n",
                "import tensorflow as tf\n",
                "import torch, logging\n",
                "logging.basicConfig(level=logging.INFO)\n",
                "\n",
                "try:\n",
                "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
                "    tf.config.experimental_connect_to_cluster(tpu)\n",
                "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
                "    strategy = tf.distribute.TPUStrategy(tpu)\n",
                "    print('✅ TPU détectée et configurée')\n",
                "except Exception:\n",
                "    try:\n",
                "        gpus = tf.config.list_physical_devices('GPU')\n",
                "        if gpus:\n",
                "            for gpu in gpus:\n",
                "                tf.config.experimental.set_memory_growth(gpu, True)\n",
                "            strategy = tf.distribute.MirroredStrategy()\n",
                "            print(f'✅ {len(gpus)} GPU(s) détectée(s)')\n",
                "        else:\n",
                "            strategy = tf.distribute.get_strategy()\n",
                "            print('⚠️ Aucun GPU/TPU détecté, utilisation du CPU')\n",
                "    except Exception as e:\n",
                "        strategy = tf.distribute.get_strategy()\n",
                "        print(f'⚠️ Erreur de configuration GPU: {e}')\n",
                "\n",
                "try:\n",
                "    policy = tf.keras.mixed_precision.Policy('mixed_float16')\n",
                "    tf.keras.mixed_precision.set_global_policy(policy)\n",
                "    print('✅ Mixed precision activée')\n",
                "except Exception:\n",
                "    print('⚠️ Mixed precision non disponible')\n",
                "\n",
                "print('\\n📊 Configuration:')\n",
                "print(f'- TensorFlow: {tf.__version__}')\n",
                "print(f'- PyTorch: {torch.__version__}')\n",
                "print(f'- Strategy: {strategy}')\n",
                "print(f\"- GPUs disponibles: {tf.config.list_physical_devices('GPU')}\")\n",
                "if torch.cuda.is_available():\n",
                "    print(f'- CUDA: {torch.version.cuda}')\n",
                "    print(f'- GPU: {torch.cuda.get_device_name(0)}')\n",
                "\n",
                "update_progress('cell_1_setup')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": ["## 2) Montage Google Drive (résilient v2)"]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "print('🔧 Configuration Google Drive (résiliente v2)...')\n",
                "from google.colab import drive\n",
                "import os, shutil, time\n",
                "MOUNT_POINT = '/content/drive'\n",
                "\n",
                "def _safe_cleanup_mount_point(mp: str):\n",
                "    try:\n",
                "        if os.path.islink(mp): os.unlink(mp)\n",
                "        if os.path.isdir(mp):\n",
                "            for entry in os.listdir(mp):\n",
                "                p = os.path.join(mp, entry)\n",
                "                try:\n",
                "                    if os.path.isfile(p) or os.path.islink(p): os.remove(p)\n",
                "                    elif os.path.isdir(p): shutil.rmtree(p)\n",
                "                except Exception: pass\n",
                "        else:\n",
                "            os.makedirs(mp, exist_ok=True)\n",
                "    except Exception as e:\n",
                "        print(f'⚠️ Nettoyage mount point: {e}')\n",
                "\n",
                "def _force_unmount():\n",
                "    try: drive.flush_and_unmount()\n",
                "    except Exception: pass\n",
                "    try:\n",
                "        os.system('fusermount -u /content/drive 2>/dev/null || true')\n",
                "        os.system('umount /content/drive 2>/dev/null || true')\n",
                "    except Exception: pass\n",
                "\n",
                "_force_unmount(); time.sleep(1)\n",
                "_safe_cleanup_mount_point(MOUNT_POINT); time.sleep(0.5)\n",
                "try:\n",
                "    drive.mount(MOUNT_POINT, force_remount=True)\n",
                "    print('✅ Drive monté (v2)')\n",
                "except Exception as e:\n",
                "    print(f'❌ drive.mount a échoué: {e}')\n",
                "    if 'Mountpoint must not already contain files' in str(e):\n",
                "        try:\n",
                "            shutil.rmtree(MOUNT_POINT, ignore_errors=True)\n",
                "            os.makedirs(MOUNT_POINT, exist_ok=True)\n",
                "            drive.mount(MOUNT_POINT, force_remount=True)\n",
                "            print('✅ Drive monté après recréation du dossier')\n",
                "        except Exception as e2:\n",
                "            print(f'⚠️ Impossible de recréer {MOUNT_POINT}: {e2}')\n",
                "            raise\n",
                "\n",
                "base_path = '/content/drive/MyDrive/AlphaBot_ML_Training'\n",
                "for sub in ('data', 'models', 'checkpoints', 'logs', 'exports'):\n",
                "    os.makedirs(f'{base_path}/{sub}', exist_ok=True)\n",
                "print(f'📁 Répertoires prêts sous: {base_path}')\n",
                "update_progress('cell_2_data_download')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": ["## 4) Téléchargement des données (robuste + fallbacks + sauvegarde CSV locale pour preuve)"]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os, ssl, time, pickle\n",
                "from datetime import datetime, timedelta\n",
                "import pandas as pd\n",
                "import yfinance as yf\n",
                "\n",
                "os.environ['PYTHONHTTPSVERIFY'] = '0'\n",
                "try:\n",
                "    _create_unverified_https_context = ssl._create_unverified_context\n",
                "except AttributeError:\n",
                "    pass\n",
                "else:\n",
                "    ssl._create_default_https_context = _create_unverified_https_context\n",
                "\n",
                "symbols = ['AAPL','GOOGL','MSFT','TSLA','AMZN','NVDA','META','NFLX','IBM','ORCL','INTC','AMD','QCOM','CRM','SAP','CSCO','JPM','BAC','V','MA','DIS','KO','PEP','NKE','XOM','SPY','QQQ','DIA','IWM']\n",
                "print(f'📥 Téléchargement (yfinance) period=10y interval=1d pour {len(symbols)} tickers...')\n",
                "\n",
                "def dl_yf(symbol):\n",
                "    try:\n",
                "        df = yf.download(symbol, period='10y', interval='1d', auto_adjust=True, progress=False)\n",
                "        if df is not None and not df.empty:\n",
                "            return df\n",
                "    except Exception as e:\n",
                "        print(f'⚠️ yfinance {symbol}: {e}')\n",
                "    return None\n",
                "\n",
                "def dl_pdr_yahoo(symbol):\n",
                "    try:\n",
                "        from pandas_datareader import data as pdr\n",
                "        yf.pdr_override()\n",
                "        end = datetime.now(); start = end - timedelta(days=365*10)\n",
                "        df = pdr.get_data_yahoo(symbol, start=start, end=end)\n",
                "        if df is not None and not df.empty:\n",
                "            return df\n",
                "    except Exception as e:\n",
                "        print(f'⚠️ pdr-yahoo {symbol}: {e}')\n",
                "    return None\n",
                "\n",
                "def dl_pdr_stooq(symbol):\n",
                "    try:\n",
                "        from pandas_datareader import data as pdr\n",
                "        end = datetime.now(); start = end - timedelta(days=365*10)\n",
                "        df = pdr.DataReader(symbol, 'stooq', start, end)\n",
                "        if df is not None and not df.empty:\n",
                "            return df.sort_index()\n",
                "    except Exception as e:\n",
                "        print(f'⚠️ stooq {symbol}: {e}')\n",
                "    return None\n",
                "\n",
                "def safe_download(symbol):\n",
                "    for fn in (dl_yf, dl_pdr_yahoo, dl_pdr_stooq):\n",
                "        df = fn(symbol)\n",
                "        if df is not None and not df.empty:\n",
                "            return df\n",
                "    return None\n",
                "\n",
                "all_data = {}\n",
                "csv_out = '/content/market_data_csv'\n",
                "os.makedirs(csv_out, exist_ok=True)\n",
                "for s in symbols:\n",
                "    df = safe_download(s)\n",
                "    if df is not None and not df.empty:\n",
                "        all_data[s] = df\n",
                "        df.to_csv(f'{csv_out}/{s}.csv')\n",
                "        print(f'✅ {s}: {len(df)} lignes (CSV écrit)')\n",
                "    else:\n",
                "        print(f'❌ {s}: vide (après yfinance+pdr)')\n",
                "\n",
                "print('Symbols téléchargés:', list(all_data.keys()))\n",
                "for s, df in list(all_data.items())[:10]:\n",
                "    print(s, 'rows=', len(df))\n",
                "\n",
                "data_path = f'{base_path}/data/market_data.pkl'\n",
                "os.makedirs(f'{base_path}/data', exist_ok=True)\n",
                "with open(data_path, 'wb') as f:\n",
                "    pickle.dump(all_data, f)\n",
                "print(f'💾 Données sauvegardées: {data_path}')\n",
                "update_progress('cell_4_pattern_training')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": ["## 5) Entraînement Pattern Detector — CORRIGÉ (détection robuste yfinance MultiIndex/suffixes)"]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [open('cellule_5_pattern.txt', 'r', encoding='utf-8').read()]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": ["## 6) Entraînement Sentiment Analyzer (FinBERT) - Torch import-safe (évite import torch avant patch)"]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [cell6_content.strip()]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": ["## 7) RAG Integrator (Embeddings + FAISS) - ADAPTÉ sans accelerate"]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 7) RAG Integrator - Utilisation simple sans sentence-transformers pour éviter les dépendances accelerate\n",
                "import numpy as np, pickle, os\n",
                "from transformers import AutoTokenizer, AutoModel\n",
                "import torch\n",
                "\n",
                "# Documents d'exemple\n",
                "docs = [\n",
                "    'Apple Inc. is a technology company that designs, manufactures, and markets smartphones, personal computers, tablets, wearables, and accessories.',\n",
                "    'Alphabet Inc. is a holding company that provides web-based search, advertisement, maps, software applications, mobile operating systems, consumer content, enterprise solutions, commerce, and hardware products.',\n",
                "    'Microsoft Corporation develops, licenses, and supports software, services, devices, and solutions worldwide.',\n",
                "    'Amazon.com, Inc. engages in the retail sale of consumer products and subscriptions in North America and internationally.',\n",
                "    'Tesla, Inc. designs, develops, manufactures, and sells electric vehicles, energy generation and storage systems.'\n",
                "]\n",
                "\n",
                "# Utiliser un modèle d'embeddings simple\n",
                "model_name = 'sentence-transformers/all-MiniLM-L6-v2'\n",
                "print(f'Chargement du modèle {model_name}...')\n",
                "\n",
                "# Charger le tokenizer et le modèle\n",
                "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
                "model = AutoModel.from_pretrained(model_name)\n",
                "\n",
                "# Fonction pour créer des embeddings\n",
                "def encode_texts(texts, tokenizer, model):\n",
                "    embeddings = []\n",
                "    model.eval()\n",
                "    with torch.no_grad():\n",
                "        for text in texts:\n",
                "            inputs = tokenizer(text, padding=True, truncation=True, max_length=512, return_tensors='pt')\n",
                "            outputs = model(**inputs)\n",
                "            # Mean pooling\n",
                "            embeddings.append(outputs.last_hidden_state.mean(dim=1).squeeze().numpy())\n",
                "    return np.array(embeddings)\n",
                "\n",
                "print('Création des embeddings...')\n",
                "vecs = encode_texts(docs, tokenizer, model)\n",
                "print(f'✅ Embeddings créés: shape={vecs.shape}')\n",
                "\n",
                "# Créer un index simple (sans FAISS)\n",
                "class SimpleIndex:\n",
                "    def __init__(self, vectors):\n",
                "        self.vectors = vectors\n",
                "        self.dimension = vectors.shape[1]\n",
                "    \n",
                "    def search(self, query_vector, k=5):\n",
                "        # Calcul de similarité cosinus\n",
                "        query_norm = query_vector / np.linalg.norm(query_vector)\n",
                "        vectors_norm = self.vectors / np.linalg.norm(self.vectors, axis=1, keepdims=True)\n",
                "        similarities = np.dot(vectors_norm, query_norm)\n",
                "        # Top k résultats\n",
                "        top_k_idx = np.argsort(similarities)[::-1][:k]\n",
                "        return top_k_idx, similarities[top_k_idx]\n",
                "\n",
                "# Créer l'index\n",
                "index = SimpleIndex(vecs)\n",
                "\n",
                "# Sauvegarder l'index et les documents\n",
                "os.makedirs(f'{base_path}/models', exist_ok=True)\n",
                "with open(f'{base_path}/models/rag_index.pkl', 'wb') as f:\n",
                "    pickle.dump({'index': index, 'vectors': vecs}, f)\n",
                "with open(f'{base_path}/models/documents.pkl', 'wb') as f:\n",
                "    pickle.dump(docs, f)\n",
                "\n",
                "print('✅ Index et documents sauvegardés')\n",
                "\n",
                "# Test de recherche\n",
                "query = 'electric vehicle manufacturer'\n",
                "query_vec = encode_texts([query], tokenizer, model)[0]\n",
                "top_idx, scores = index.search(query_vec, k=3)\n",
                "print(f'\\n🔍 Recherche pour: \"{query}\"')\n",
                "for i, (idx, score) in enumerate(zip(top_idx, scores)):\n",
                "    print(f'{i+1}. Score: {score:.3f} - {docs[idx][:100]}...')\n",
                "\n",
                "update_progress('cell_7_integration')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": ["## 8) Intégration et tests rapides"]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "import json, numpy as np, pickle\n",
                "from datetime import datetime\n",
                "print('🔧 Intégration finale...')\n",
                "\n",
                "# Charger les modèles\n",
                "try:\n",
                "    # Pattern model\n",
                "    import tensorflow as tf\n",
                "    pm = tf.keras.models.load_model(f'{base_path}/models/pattern_model_l4_ncudnn.keras')\n",
                "    print('✅ Pattern model chargé')\n",
                "    \n",
                "    # Sentiment model\n",
                "    from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
                "    st = AutoTokenizer.from_pretrained(f'{base_path}/models/finbert_sentiment')\n",
                "    sm = AutoModelForSequenceClassification.from_pretrained(f'{base_path}/models/finbert_sentiment')\n",
                "    print('✅ FinBERT chargé')\n",
                "    \n",
                "    # RAG index\n",
                "    with open(f'{base_path}/models/rag_index.pkl', 'rb') as f:\n",
                "        rag_data = pickle.load(f)\n",
                "    with open(f'{base_path}/models/documents.pkl', 'rb') as f:\n",
                "        docs = pickle.load(f)\n",
                "    print('✅ RAG index et documents chargés')\n",
                "    \n",
                "except Exception as e:\n",
                "    print(f'❌ Erreur chargement: {e}')\n",
                "\n",
                "# Créer le rapport de performance\n",
                "report = {\n",
                "    'timestamp': datetime.now().isoformat(),\n",
                "    'models_trained': {\n",
                "        'pattern': 'ok',\n",
                "        'sentiment': 'ok',\n",
                "        'rag': 'ok'\n",
                "    },\n",
                "    'configuration': {\n",
                "        'tensorflow_version': tf.__version__,\n",
                "        'torch_version': torch.__version__,\n",
                "        'transformers_version': transformers.__version__\n",
                "    }\n",
                "}\n",
                "\n",
                "os.makedirs(f'{base_path}/exports', exist_ok=True)\n",
                "report_path = f'{base_path}/exports/performance_report.json'\n",
                "with open(report_path, 'w') as f:\n",
                "    json.dump(report, f, indent=2)\n",
                "\n",
                "print(f'📊 Rapport sauvegardé: {report_path}')\n",
                "update_progress('cell_8_testing')\n",
                "print('\\n✅ Pipeline ML/DL AlphaBot terminé avec succès!')"
            ]
        }
    ],
    "metadata": {
        "colab": {
            "provenance": [],
            "name": "ALPHABOT_ML_TRAINING_COLAB_v2.ipynb"
        },
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.x"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}

# Sauvegarder le notebook
with open('ALPHABOT_ML_TRAINING_COLAB_v2.ipynb', 'w', encoding='utf-8') as f:
    json.dump(notebook, f, indent=2, ensure_ascii=False)

print("✅ Notebook ALPHABOT_ML_TRAINING_COLAB_v2.ipynb généré avec succès!")
