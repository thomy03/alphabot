import os
os.environ['TF_KERAS_ALLOW_CUDNN_RNN']='0'
import tensorflow as tf
from sklearn.preprocessing import StandardScaler
import numpy as np, pickle, pandas as pd
from datetime import datetime

log_path = f"{base_path}/logs"; os.makedirs(log_path, exist_ok=True)
log_file = f"{log_path}/pattern_debug.txt"; print('Log path:', log_file)
open(log_file,'a').write(f"\n==== DEBUG RUN {datetime.now().isoformat()} ====\n")

try:
    with open(f'{base_path}/data/market_data.pkl','rb') as f:
        all_data = pickle.load(f)
    print('‚úÖ Donn√©es charg√©es')
except Exception:
    print('‚ùå Donn√©es introuvables ‚Äî ex√©cutez la cellule 4'); all_data={}

print('Nombre de symboles dans all_data:', len(all_data))
if not all_data:
    print('‚ö†Ô∏è all_data est vide: v√©rifiez la cellule 4 (r√©seau/API/period).')

def find_col(df_local, bases):
    cols = [c for c in df_local.columns if isinstance(c, str)]
    lower_map = {c.lower(): c for c in cols}
    for b in bases:
        if b in lower_map:
            return lower_map[b]
    for c in cols:
        lc = c.lower()
        for b in bases:
            if lc.startswith(b + '_') or lc.endswith('_' + b) or b in lc.split('_'):
                return c
    for c in cols:
        lc = c.lower()
        for b in bases:
            if b in lc:
                return c
    return None

def prepare_pattern_training_data(all_data):
    X, y = [], []
    win_count_total = 0
    for symbol, data in all_data.items():
        try:
            if data is None or data.empty:
                open(log_file,'a').write(f"symbol={symbol} skipped: empty\n"); continue
            df = data.copy()
            open(log_file,'a').write(f"symbol={symbol} columns={list(df.columns)}\n")
            if isinstance(df.columns, pd.MultiIndex):
                df.columns = ['_'.join(col).strip() for col in df.columns.values]
                open(log_file,'a').write(f"symbol={symbol} MultiIndex flattened to {list(df.columns)}\n")
            df.columns = [str(c).strip() for c in df.columns]
            # priorit√© par suffixe exact si pr√©sent
            close = find_col(df, [f'close_{symbol.lower()}', 'close','adj close','adj_close','adjclose'])
            high  = find_col(df, [f'high_{symbol.lower()}', 'high'])
            low   = find_col(df, [f'low_{symbol.lower()}', 'low'])
            volume = find_col(df, [f'volume_{symbol.lower()}', 'volume'])
            open(log_file,'a').write(f"symbol={symbol} mapped: close={close} high={high} low={low} volume={volume}\n")
            if close is None:
                alt_close = find_col(df, ['adj close','adj_close','adjclose'])
                if alt_close is not None:
                    close = alt_close
                    open(log_file,'a').write(f"symbol={symbol} fallback close-> {close}\n")
            if not all([close, high, low]):
                open(log_file,'a').write(f"symbol={symbol} skipped: missing required columns\n"); continue
            if volume is None:
                df['Volume_proxy'] = df[close].pct_change().rolling(10, min_periods=1).std().fillna(0.01)
                volume = 'Volume_proxy'
                open(log_file,'a').write(f"symbol={symbol} created volume proxy\n")
            for col in [close, high, low, volume]:
                df[col] = pd.to_numeric(df[col], errors='coerce')
            df = df[[close, high, low, volume]].dropna()
            n = len(df)
            open(log_file,'a').write(f"symbol={symbol} len={n} after dropna\n")
            if n < 20:
                open(log_file,'a').write(f"symbol={symbol} skipped: n<20\n"); continue
            win_count = 0
            for i in range(0, n - 17):
                seq = df.iloc[i:i+15]
                fut = df.iloc[i+15:i+17]
                if seq.isnull().any().any() or fut.isnull().any().any():
                    continue
                close_arr = seq[close].values.reshape(-1, 1)
                vol_arr = seq[volume].values.reshape(-1, 1)
                spread_arr = (seq[high] - seq[low]).values.reshape(-1, 1)
                features = np.concatenate([close_arr, vol_arr, spread_arr], axis=1)
                if features.shape != (15, 3):
                    continue
                current_price = float(seq[close].iloc[-1])
                future_mean = float(fut[close].mean())
                if current_price <= 0 or not np.isfinite(future_mean):
                    continue
                future_return = (future_mean - current_price) / current_price
                if future_return > 0.002:
                    label = 2
                elif future_return < -0.002:
                    label = 0
                else:
                    label = 1
                X.append(features.astype(np.float32)); y.append(label); win_count += 1
            win_count_total += win_count
            open(log_file,'a').write(f"symbol={symbol} windows={win_count}\n")
        except Exception as e:
            open(log_file,'a').write(f"symbol={symbol} ERROR {str(e)[:200]}\n"); continue
    open(log_file,'a').write(f"total_windows={len(X)} total_by_symbol={win_count_total}\n")
    if len(X) == 0:
        print('‚ö†Ô∏è Aucune fen√™tre cr√©√©e! V√©rifiez le log.'); return np.array([]), np.array([])
    X = np.array(X, dtype=np.float32); Y = np.array(y, dtype=np.int32)
    print(f'‚úÖ Pr√©paration: X={X.shape}, y={Y.shape}')
    return X, Y

X_train, y_train = prepare_pattern_training_data(all_data)
print(f'üìä √âchantillons: {X_train.shape[0] if len(X_train) > 0 else 0}')
print(f"üîç Log fen√™tres: voir {log_file}")

if X_train.shape[0] == 0:
    print('\nüìù Derni√®res lignes du log:')
    with open(log_file, 'r') as f:
        lines = f.readlines()
        for line in lines[-20:]:
            print(line.strip())

strategy = tf.distribute.get_strategy()
with strategy.scope():
    inputs = tf.keras.Input(shape=(15,3), name='input')
    x = tf.keras.layers.BatchNormalization()(inputs)
    x = tf.keras.layers.LSTM(64, return_sequences=False, name='lstm_main_ncudnn', activation='tanh', recurrent_activation='sigmoid', dropout=0.1, recurrent_dropout=0.1)(x)
    x = tf.keras.layers.Dropout(0.3)(x)
    x = tf.keras.layers.Dense(32, activation='relu')(x)
    x = tf.keras.layers.BatchNormalization()(x)
    x = tf.keras.layers.Dropout(0.3)(x)
    outputs = tf.keras.layers.Dense(3, activation='softmax', name='output')(x)
    model = tf.keras.Model(inputs, outputs, name='l4_ncudnn_lstm_model')
    model.compile(optimizer=tf.keras.optimizers.Adam(1e-3), loss='sparse_categorical_crossentropy', metrics=['accuracy'])
print('‚úÖ Mod√®le L4 (non-cuDNN) cr√©√©')

X_train = X_train.astype(np.float32); y_train = y_train.astype(np.int32)
if X_train.shape[0] == 0:
    print('‚ö†Ô∏è Dataset vide. G√©n√©ration synth√©tique minimale...')
    X_train = np.random.randn(1024, 15, 3).astype(np.float32)
    y_train = np.random.randint(0, 3, size=(1024,)).astype(np.int32)

scaler = (StandardScaler().fit(X_train.reshape(-1,3)))
Xs = scaler.transform(X_train.reshape(-1,3)).reshape(X_train.shape)
callbacks = [
    tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True, monitor='val_loss', verbose=1),
    tf.keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=3, monitor='val_loss', verbose=1)
]
hist = model.fit(Xs, y_train, epochs=10, batch_size=64, validation_split=0.2, callbacks=callbacks, verbose=1)
print('‚úÖ Entra√Ænement termin√©')

import matplotlib.pyplot as plt
os.makedirs(f'{base_path}/models', exist_ok=True)
model.save(f'{base_path}/models/pattern_model_l4_ncudnn.keras')
import pickle as pkl
with open(f'{base_path}/models/pattern_scaler.pkl','wb') as f: pkl.dump(scaler,f)
print('üíæ Mod√®le/scaler sauvegard√©s')
try:
    plt.figure(figsize=(12,4));
    if 'accuracy' in hist.history:
        plt.subplot(1,2,1); plt.plot(hist.history['accuracy']); 
        if 'val_accuracy' in hist.history: plt.plot(hist.history['val_accuracy']); plt.title('Accuracy'); plt.legend(['train','val'])
    if 'loss' in hist.history:
        plt.subplot(1,2,2); plt.plot(hist.history['loss']);
        if 'val_loss' in hist.history: plt.plot(hist.history['val_loss']); plt.title('Loss'); plt.legend(['train','val']); plt.tight_layout(); plt.show()
except Exception:
    pass

update_progress('cell_5_sentiment_training')
