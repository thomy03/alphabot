#!/usr/bin/env python3
"""
Test du Hybrid Orchestrator - AlphaBot
Script de test pour valider l'intÃ©gration complÃ¨te du systÃ¨me hybride
"""

import asyncio
import logging
import sys
import os
from datetime import datetime
from typing import List, Dict, Any

# Ajouter le chemin du projet
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from alphabot.core.hybrid_orchestrator import (
    HybridOrchestrator, 
    HybridWorkflowType,
    get_hybrid_orchestrator
)
from alphabot.ml import get_ml_info

# Configuration du logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('hybrid_orchestrator_test.log')
    ]
)

logger = logging.getLogger(__name__)


async def test_ml_components():
    """Tester les composants ML disponibles"""
    logger.info("ğŸ§  Test des composants ML...")
    
    try:
        ml_info = get_ml_info()
        logger.info(f"âœ… ML Info: {ml_info}")
        
        # Tester l'import des composants
        from alphabot.ml.pattern_detector import MLPatternDetector
        from alphabot.ml.sentiment_analyzer import SentimentDLAnalyzer
        from alphabot.ml.rag_integrator import RAGIntegrator
        
        logger.info("âœ… Tous les composants ML importÃ©s avec succÃ¨s")
        
        # Initialiser chaque composant
        pattern_detector = MLPatternDetector()
        sentiment_analyzer = SentimentDLAnalyzer()
        rag_integrator = RAGIntegrator()
        
        logger.info("âœ… Tous les composants ML initialisÃ©s avec succÃ¨s")
        
        return True
        
    except Exception as e:
        logger.error(f"âŒ Ã‰chec du test des composants ML: {e}")
        return False


async def test_hybrid_orchestrator_basic():
    """Tester les fonctionnalitÃ©s de base de l'orchestrateur hybride"""
    logger.info("ğŸš€ Test de base de l'orchestrateur hybride...")
    
    try:
        # Initialiser l'orchestrateur
        orchestrator = get_hybrid_orchestrator(enable_ml=True)
        logger.info("âœ… Orchestrateur hybride initialisÃ©")
        
        # VÃ©rifier les mÃ©triques de performance
        metrics = orchestrator.get_performance_metrics()
        logger.info(f"ğŸ“Š MÃ©triques initiales: {metrics}")
        
        # VÃ©rifier les composants disponibles
        ml_components = metrics['ml_components_available']
        logger.info(f"ğŸ”§ Composants ML disponibles: {ml_components}")
        
        return True
        
    except Exception as e:
        logger.error(f"âŒ Ã‰chec du test de base: {e}")
        return False


async def test_core_analysis():
    """Tester l'analyse Core System (sans ML)"""
    logger.info("ğŸ¯ Test de l'analyse Core System...")
    
    try:
        orchestrator = get_hybrid_orchestrator(enable_ml=False)  # DÃ©sactiver ML pour ce test
        
        # Symboles de test
        test_symbols = ['AAPL', 'MSFT', 'GOOGL']
        
        # ExÃ©cuter l'analyse Core
        decisions = await orchestrator.analyze_portfolio_hybrid(
            test_symbols, 
            HybridWorkflowType.CORE_ANALYSIS
        )
        
        logger.info(f"âœ… Analyse Core terminÃ©e pour {len(decisions)} symboles")
        
        # Afficher les rÃ©sultats
        for symbol, decision in decisions.items():
            logger.info(f"  {symbol}: {decision.action} (confiance: {decision.confidence:.2f})")
        
        return len(decisions) > 0
        
    except Exception as e:
        logger.error(f"âŒ Ã‰chec du test Core Analysis: {e}")
        return False


async def test_ml_enhanced_analysis():
    """Tester l'analyse ML Enhanced"""
    logger.info("ğŸ§  Test de l'analyse ML Enhanced...")
    
    try:
        orchestrator = get_hybrid_orchestrator(enable_ml=True)
        
        # Symboles de test
        test_symbols = ['AAPL', 'MSFT']
        
        # ExÃ©cuter l'analyse ML Enhanced
        decisions = await orchestrator.analyze_portfolio_hybrid(
            test_symbols, 
            HybridWorkflowType.ML_ENHANCED
        )
        
        logger.info(f"âœ… Analyse ML Enhanced terminÃ©e pour {len(decisions)} symboles")
        
        # Afficher les rÃ©sultats dÃ©taillÃ©s
        for symbol, decision in decisions.items():
            logger.info(f"  {symbol}:")
            logger.info(f"    Action: {decision.action}")
            logger.info(f"    Confiance: {decision.confidence:.2f}")
            logger.info(f"    Composants ML utilisÃ©s: {decision.ml_components_used}")
            if decision.ml_pattern_score:
                logger.info(f"    Score Pattern: {decision.ml_pattern_score:.2f}")
            if decision.ml_sentiment_score:
                logger.info(f"    Score Sentiment: {decision.ml_sentiment_score:.2f}")
            if decision.rag_confidence:
                logger.info(f"    Confiance RAG: {decision.rag_confidence:.2f}")
        
        return len(decisions) > 0
        
    except Exception as e:
        logger.error(f"âŒ Ã‰chec du test ML Enhanced Analysis: {e}")
        return False


async def test_performance_metrics():
    """Tester le suivi des mÃ©triques de performance"""
    logger.info("ğŸ“Š Test des mÃ©triques de performance...")
    
    try:
        orchestrator = get_hybrid_orchestrator(enable_ml=True)
        
        # ExÃ©cuter quelques analyses pour gÃ©nÃ©rer des mÃ©triques
        test_symbols = ['AAPL', 'MSFT']
        
        # Analyse Core
        await orchestrator.analyze_portfolio_hybrid(
            test_symbols, 
            HybridWorkflowType.CORE_ANALYSIS
        )
        
        # Analyse ML Enhanced
        await orchestrator.analyze_portfolio_hybrid(
            test_symbols, 
            HybridWorkflowType.ML_ENHANCED
        )
        
        # RÃ©cupÃ©rer les mÃ©triques
        metrics = orchestrator.get_performance_metrics()
        
        logger.info("ğŸ“ˆ MÃ©triques de performance:")
        for key, value in metrics.items():
            logger.info(f"  {key}: {value}")
        
        # VÃ©rifier que les mÃ©triques ont Ã©tÃ© mises Ã  jour
        assert metrics['core_decisions'] > 0, "Les dÃ©cisions Core devraient Ãªtre comptabilisÃ©es"
        assert metrics['ml_enabled'] == True, "Le ML devrait Ãªtre activÃ©"
        
        logger.info("âœ… MÃ©triques de fonctionnement correctement")
        return True
        
    except Exception as e:
        logger.error(f"âŒ Ã‰chec du test des mÃ©triques: {e}")
        return False


async def test_weekly_rebalance():
    """Tester le rebalancement hebdomadaire"""
    logger.info("ğŸ”„ Test du rebalancement hebdomadaire...")
    
    try:
        orchestrator = get_hybrid_orchestrator(enable_ml=True)
        
        # Symboles de test
        test_symbols = ['AAPL', 'MSFT', 'GOOGL']
        
        # ExÃ©cuter le rebalancement
        result = await orchestrator.execute_weekly_rebalance(test_symbols)
        
        logger.info(f"âœ… Rebalancement terminÃ©: {result['status']}")
        logger.info(f"   DÃ©cisions: {result.get('decisions_count', 0)}")
        logger.info(f"   Trades exÃ©cutÃ©s: {result.get('trades_executed', 0)}")
        logger.info(f"   DÃ©cisions ML: {result.get('ml_enhanced', 0)}")
        
        return result['status'] == 'completed'
        
    except Exception as e:
        logger.error(f"âŒ Ã‰chec du test de rebalancement: {e}")
        return False


async def run_all_tests():
    """ExÃ©cuter tous les tests"""
    logger.info("ğŸš€ DÃ©marrage des tests complets du Hybrid Orchestrator")
    logger.info("=" * 60)
    
    tests = [
        ("Composants ML", test_ml_components),
        ("Orchestrateur Base", test_hybrid_orchestrator_basic),
        ("Analyse Core", test_core_analysis),
        ("Analyse ML Enhanced", test_ml_enhanced_analysis),
        ("MÃ©triques Performance", test_performance_metrics),
        ("Rebalancement Hebdomadaire", test_weekly_rebalance)
    ]
    
    results = {}
    
    for test_name, test_func in tests:
        logger.info(f"\nğŸ§ª Test: {test_name}")
        logger.info("-" * 40)
        
        try:
            success = await test_func()
            results[test_name] = success
            
            if success:
                logger.info(f"âœ… {test_name}: SUCCÃˆS")
            else:
                logger.error(f"âŒ {test_name}: Ã‰CHEC")
                
        except Exception as e:
            logger.error(f"âŒ {test_name}: ERREUR - {e}")
            results[test_name] = False
    
    # RÃ©sumÃ© final
    logger.info("\n" + "=" * 60)
    logger.info("ğŸ“‹ RÃ‰SUMÃ‰ DES TESTS")
    logger.info("=" * 60)
    
    passed = sum(results.values())
    total = len(results)
    
    for test_name, success in results.items():
        status = "âœ… SUCCÃˆS" if success else "âŒ Ã‰CHEC"
        logger.info(f"{test_name:<30} {status}")
    
    logger.info("-" * 60)
    logger.info(f"Total: {passed}/{total} tests rÃ©ussis")
    
    if passed == total:
        logger.info("ğŸ‰ TOUS LES TESTS ONT RÃ‰USSI!")
        return True
    else:
        logger.error(f"âš ï¸  {total - passed} tests ont Ã©chouÃ©")
        return False


if __name__ == "__main__":
    # ExÃ©cuter les tests
    success = asyncio.run(run_all_tests())
    
    # Code de sortie
    sys.exit(0 if success else 1)
