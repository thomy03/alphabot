{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üöÄ AlphaBot ML/DL Training - Google Colab (v2)\n",
        "\n",
        "Notebook propre et corrig√© (d√©tection robuste des colonnes yfinance MultiIndex/suffixes) avec suivi/reprise, t√©l√©chargement de donn√©es robuste et fallbacks s√ªrs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ajustement du path pour que Colab trouve le module alphabot\n",
        "import sys\n",
        "sys.path.append('/content')\n",
        "sys.path.append('/content/alphabot')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîÑ Suivi de Progression et Reprise Automatique"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üîÑ Syst√®me de suivi et reprise automatique\n",
        "import os, json\n",
        "from datetime import datetime\n",
        "\n",
        "base_path = '/content/drive/MyDrive/AlphaBot_ML_Training'\n",
        "os.makedirs(base_path, exist_ok=True)\n",
        "progress_file = f'{base_path}/progress_tracker.json'\n",
        "\n",
        "default_progress = {\n",
        "    'cell_1_setup': False,\n",
        "    'cell_2_data_download': False,\n",
        "    'cell_3_data_analysis': False,\n",
        "    'cell_4_pattern_training': False,\n",
        "    'cell_5_sentiment_training': False,\n",
        "    'cell_6_rag_training': False,\n",
        "    'cell_7_integration': False,\n",
        "    'cell_8_testing': False,\n",
        "    'cell_9_deployment': False,\n",
        "    'last_cell_executed': None,\n",
        "    'start_time': None,\n",
        "    'last_update': None\n",
        "}\n",
        "\n",
        "try:\n",
        "    with open(progress_file, 'r') as f:\n",
        "        progress = json.load(f)\n",
        "    print('üìä Suivi de progression charg√©')\n",
        "except Exception:\n",
        "    progress = default_progress.copy()\n",
        "    progress['start_time'] = datetime.now().isoformat()\n",
        "    print('üÜï Nouveau suivi de progression initialis√©')\n",
        "\n",
        "def update_progress(cell_name):\n",
        "    progress.setdefault(cell_name, False)\n",
        "    progress[cell_name] = True\n",
        "    progress['last_cell_executed'] = cell_name\n",
        "    progress['last_update'] = datetime.now().isoformat()\n",
        "    with open(progress_file, 'w') as f:\n",
        "        json.dump(progress, f, indent=2)\n",
        "    print(f'‚úÖ Progression mise √† jour: {cell_name}')\n",
        "\n",
        "def check_progress():\n",
        "    print('\\nüìã √âtat actuel de la progression:')\n",
        "    print('=' * 50)\n",
        "    completed = sum(1 for k, v in progress.items() if k.startswith('cell_') and isinstance(v, bool) and v)\n",
        "    total = sum(1 for k in default_progress.keys() if k.startswith('cell_'))\n",
        "    pct = (completed / total * 100) if total else 0.0\n",
        "    print(f'üìä Progression: {completed}/{total} √©tapes compl√©t√©es ({pct:.1f}%)')\n",
        "    print(f'‚è∞ D√©marr√©: {progress.get(\"start_time\", \"N/A\")}')\n",
        "    print(f'üîÑ Derni√®re mise √† jour: {progress.get(\"last_update\", \"N/A\")}')\n",
        "    print(f'üìç Derni√®re cellule: {progress.get(\"last_cell_executed\", \"Aucune\")}')\n",
        "    print('=' * 50)\n",
        "\n",
        "check_progress()\n",
        "print('\\nüí° Instructions:')\n",
        "print('1. Ex√©cutez cette cellule pour voir l\\'√©tat d\\'avancement')\n",
        "print('2. Chaque cellule mettra √† jour automatiquement sa progression')\n",
        "print('3. Si le processus s\\'arr√™te, relancez simplement cette cellule')\n",
        "print('4. Continuez avec la cellule sugg√©r√©e')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Setup GPU/TPU et environnement Colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys, os, subprocess\n",
        "\n",
        "def pip_install(pkgs):\n",
        "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q'] + pkgs)\n",
        "\n",
        "# Nettoyage des modules d√©j√† import√©s\n",
        "for m in [m for m in list(sys.modules) if m.startswith('transformers') or m.startswith('accelerate') or m.startswith('numpy') or m.startswith('tensorflow') or m.startswith('torch')]:\n",
        "    del sys.modules[m]\n",
        "\n",
        "# R√©installer numpy pour compatibilit√© binaire\n",
        "pip_install([\n",
        "    '--upgrade',\n",
        "    'numpy>=1.24.0,<1.27.0'\n",
        "])\n",
        "\n",
        "# Installer un set compatible (Option A)\n",
        "pip_install([\n",
        "    'transformers>=4.43,<4.47',\n",
        "    'accelerate>=0.30,<0.34',\n",
        "    'datasets>=2.18,<3.0',\n",
        "    'safetensors>=0.4.3',\n",
        "    'huggingface-hub>=0.23,<0.25'\n",
        "])\n",
        "\n",
        "os.environ['HF_HUB_DISABLE_TELEMETRY'] = '1'\n",
        "\n",
        "# Importer et afficher les versions\n",
        "import numpy\n",
        "import transformers\n",
        "print('NumPy version:', numpy.__version__)\n",
        "print('Transformers version:', transformers.__version__)\n",
        "\n",
        "\n\n# --- Code existant de la cellule 6 ---\n",
        "import sys, os, subprocess\n",
        "\n",
        "def pip_install(pkgs):\n",
        "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q'] + pkgs)\n",
        "\n",
        "# Nettoyage des modules d√©j√† import√©s\n",
        "for m in [m for m in list(sys.modules) if m.startswith('transformers') or m.startswith('accelerate')]:\n",
        "    del sys.modules[m]\n",
        "\n",
        "# Installer un set compatible (Option A)\n",
        "pip_install([\n",
        "    'transformers>=4.43,<4.47',\n",
        "    'accelerate>=0.30,<0.34',\n",
        "    'datasets>=2.18,<3.0',\n",
        "    'safetensors>=0.4.3',\n",
        "    'huggingface-hub>=0.23,<0.25'\n",
        "])\n",
        "\n",
        "os.environ['HF_HUB_DISABLE_TELEMETRY'] = '1'\n",
        "\n",
        "import transformers\n",
        "print('Transformers version:', transformers.__version__)\n",
        "\n",
        "\n\n# --- Code existant de la cellule 6 ---\n",
        "import tensorflow as tf\n",
        "import torch, logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "try:\n",
        "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "    tf.config.experimental_connect_to_cluster(tpu)\n",
        "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "    strategy = tf.distribute.TPUStrategy(tpu)\n",
        "    print('‚úÖ TPU d√©tect√©e et configur√©e')\n",
        "except Exception:\n",
        "    try:\n",
        "        gpus = tf.config.list_physical_devices('GPU')\n",
        "        if gpus:\n",
        "            for gpu in gpus:\n",
        "                tf.config.experimental.set_memory_growth(gpu, True)\n",
        "            strategy = tf.distribute.MirroredStrategy()\n",
        "            print(f'‚úÖ {len(gpus)} GPU(s) d√©tect√©e(s)')\n",
        "        else:\n",
        "            strategy = tf.distribute.get_strategy()\n",
        "            print('‚ö†Ô∏è Aucun GPU/TPU d√©tect√©, utilisation du CPU')\n",
        "    except Exception as e:\n",
        "        strategy = tf.distribute.get_strategy()\n",
        "        print(f'‚ö†Ô∏è Erreur de configuration GPU: {e}')\n",
        "\n",
        "try:\n",
        "    policy = tf.keras.mixed_precision.Policy('mixed_float16')\n",
        "    tf.keras.mixed_precision.set_global_policy(policy)\n",
        "    print('‚úÖ Mixed precision activ√©e')\n",
        "except Exception:\n",
        "    print('‚ö†Ô∏è Mixed precision non disponible')\n",
        "\n",
        "print('\\nüìä Configuration:')\n",
        "print(f'- TensorFlow: {tf.__version__}')\n",
        "print(f'- PyTorch: {torch.__version__}')\n",
        "print(f'- Strategy: {strategy}')\n",
        "print(f\"- GPUs disponibles: {tf.config.list_physical_devices('GPU')}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f'- CUDA: {torch.version.cuda}')\n",
        "    print(f'- GPU: {torch.cuda.get_device_name(0)}')\n",
        "\n",
        "update_progress('cell_1_setup')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Montage Google Drive (r√©silient v2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print('üîß Configuration Google Drive (r√©siliente v2)...')\n",
        "from google.colab import drive\n",
        "import os, shutil, time\n",
        "MOUNT_POINT = '/content/drive'\n",
        "\n",
        "def _safe_cleanup_mount_point(mp: str):\n",
        "    try:\n",
        "        if os.path.islink(mp): os.unlink(mp)\n",
        "        if os.path.isdir(mp):\n",
        "            for entry in os.listdir(mp):\n",
        "                p = os.path.join(mp, entry)\n",
        "                try:\n",
        "                    if os.path.isfile(p) or os.path.islink(p): os.remove(p)\n",
        "                    elif os.path.isdir(p): shutil.rmtree(p)\n",
        "                except Exception: pass\n",
        "        else:\n",
        "            os.makedirs(mp, exist_ok=True)\n",
        "    except Exception as e:\n",
        "        print(f'‚ö†Ô∏è Nettoyage mount point: {e}')\n",
        "\n",
        "def _force_unmount():\n",
        "    try: drive.flush_and_unmount()\n",
        "    except Exception: pass\n",
        "    try:\n",
        "        os.system('fusermount -u /content/drive 2>/dev/null || true')\n",
        "        os.system('umount /content/drive 2>/dev/null || true')\n",
        "    except Exception: pass\n",
        "\n",
        "_force_unmount(); time.sleep(1)\n",
        "_safe_cleanup_mount_point(MOUNT_POINT); time.sleep(0.5)\n",
        "try:\n",
        "    drive.mount(MOUNT_POINT, force_remount=True)\n",
        "    print('‚úÖ Drive mont√© (v2)')\n",
        "except Exception as e:\n",
        "    print(f'‚ùå drive.mount a √©chou√©: {e}')\n",
        "    if 'Mountpoint must not already contain files' in str(e):\n",
        "        try:\n",
        "            shutil.rmtree(MOUNT_POINT, ignore_errors=True)\n",
        "            os.makedirs(MOUNT_POINT, exist_ok=True)\n",
        "            drive.mount(MOUNT_POINT, force_remount=True)\n",
        "            print('‚úÖ Drive mont√© apr√®s recr√©ation du dossier')\n",
        "        except Exception as e2:\n",
        "            print(f'‚ö†Ô∏è Impossible de recr√©er {MOUNT_POINT}: {e2}')\n",
        "            raise\n",
        "\n",
        "base_path = '/content/drive/MyDrive/AlphaBot_ML_Training'\n",
        "for sub in ('data', 'models', 'checkpoints', 'logs', 'exports'):\n",
        "    os.makedirs(f'{base_path}/{sub}', exist_ok=True)\n",
        "print(f'üìÅ R√©pertoires pr√™ts sous: {base_path}')\n",
        "update_progress('cell_2_data_download')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) T√©l√©chargement des donn√©es (robuste + fallbacks + sauvegarde CSV locale pour preuve)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os, ssl, time, pickle\n",
        "from datetime import datetime, timedelta\n",
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "\n",
        "os.environ['PYTHONHTTPSVERIFY'] = '0'\n",
        "try:\n",
        "    _create_unverified_https_context = ssl._create_unverified_context\n",
        "except AttributeError:\n",
        "    pass\n",
        "else:\n",
        "    ssl._create_default_https_context = _create_unverified_https_context\n",
        "\n",
        "symbols = ['AAPL','GOOGL','MSFT','TSLA','AMZN','NVDA','META','NFLX','IBM','ORCL','INTC','AMD','QCOM','CRM','SAP','CSCO','JPM','BAC','V','MA','DIS','KO','PEP','NKE','XOM','SPY','QQQ','DIA','IWM']\n",
        "print(f'üì• T√©l√©chargement (yfinance) period=10y interval=1d pour {len(symbols)} tickers...')\n",
        "\n",
        "def dl_yf(symbol):\n",
        "    try:\n",
        "        df = yf.download(symbol, period='10y', interval='1d', auto_adjust=True, progress=False)\n",
        "        if df is not None and not df.empty:\n",
        "            return df\n",
        "    except Exception as e:\n",
        "        print(f'‚ö†Ô∏è yfinance {symbol}: {e}')\n",
        "    return None\n",
        "\n",
        "def dl_pdr_yahoo(symbol):\n",
        "    try:\n",
        "        from pandas_datareader import data as pdr\n",
        "        yf.pdr_override()\n",
        "        end = datetime.now(); start = end - timedelta(days=365*10)\n",
        "        df = pdr.get_data_yahoo(symbol, start=start, end=end)\n",
        "        if df is not None and not df.empty:\n",
        "            return df\n",
        "    except Exception as e:\n",
        "        print(f'‚ö†Ô∏è pdr-yahoo {symbol}: {e}')\n",
        "    return None\n",
        "\n",
        "def dl_pdr_stooq(symbol):\n",
        "    try:\n",
        "        from pandas_datareader import data as pdr\n",
        "        end = datetime.now(); start = end - timedelta(days=365*10)\n",
        "        df = pdr.DataReader(symbol, 'stooq', start, end)\n",
        "        if df is not None and not df.empty:\n",
        "            return df.sort_index()\n",
        "    except Exception as e:\n",
        "        print(f'‚ö†Ô∏è stooq {symbol}: {e}')\n",
        "    return None\n",
        "\n",
        "def safe_download(symbol):\n",
        "    for fn in (dl_yf, dl_pdr_yahoo, dl_pdr_stooq):\n",
        "        df = fn(symbol)\n",
        "        if df is not None and not df.empty:\n",
        "            return df\n",
        "    return None\n",
        "\n",
        "all_data = {}\n",
        "csv_out = '/content/market_data_csv'\n",
        "os.makedirs(csv_out, exist_ok=True)\n",
        "for s in symbols:\n",
        "    df = safe_download(s)\n",
        "    if df is not None and not df.empty:\n",
        "        all_data[s] = df\n",
        "        df.to_csv(f'{csv_out}/{s}.csv')\n",
        "        print(f'‚úÖ {s}: {len(df)} lignes (CSV √©crit)')\n",
        "    else:\n",
        "        print(f'‚ùå {s}: vide (apr√®s yfinance+pdr)')\n",
        "\n",
        "print('Symbols t√©l√©charg√©s:', list(all_data.keys()))\n",
        "for s, df in list(all_data.items())[:10]:\n",
        "    print(s, 'rows=', len(df))\n",
        "\n",
        "data_path = f'{base_path}/data/market_data.pkl'\n",
        "os.makedirs(f'{base_path}/data', exist_ok=True)\n",
        "with open(data_path, 'wb') as f:\n",
        "    pickle.dump(all_data, f)\n",
        "print(f'üíæ Donn√©es sauvegard√©es: {data_path}')\n",
        "update_progress('cell_4_pattern_training')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) Entra√Ænement Pattern Detector ‚Äî CORRIG√â (d√©tection robuste yfinance MultiIndex/suffixes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['TF_KERAS_ALLOW_CUDNN_RNN']='0'\n",
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np, pickle, pandas as pd\n",
        "from datetime import datetime\n",
        "\n",
        "log_path = f\"{base_path}/logs\"; os.makedirs(log_path, exist_ok=True)\n",
        "log_file = f\"{log_path}/pattern_debug.txt\"; print('Log path:', log_file)\n",
        "open(log_file,'a').write(f\"\\n==== DEBUG RUN {datetime.now().isoformat()} ====\\n\")\n",
        "\n",
        "try:\n",
        "    with open(f'{base_path}/data/market_data.pkl','rb') as f:\n",
        "        all_data = pickle.load(f)\n",
        "    print('‚úÖ Donn√©es charg√©es')\n",
        "except Exception:\n",
        "    print('‚ùå Donn√©es introuvables ‚Äî ex√©cutez la cellule 4'); all_data={}\n",
        "\n",
        "print('Nombre de symboles dans all_data:', len(all_data))\n",
        "if not all_data:\n",
        "    print('‚ö†Ô∏è all_data est vide: v√©rifiez la cellule 4 (r√©seau/API/period).')\n",
        "\n",
        "def find_col(df_local, bases):\n",
        "    cols = [c for c in df_local.columns if isinstance(c, str)]\n",
        "    lower_map = {c.lower(): c for c in cols}\n",
        "    for b in bases:\n",
        "        if b in lower_map:\n",
        "            return lower_map[b]\n",
        "    for c in cols:\n",
        "        lc = c.lower()\n",
        "        for b in bases:\n",
        "            if lc.startswith(b + '_') or lc.endswith('_' + b) or b in lc.split('_'):\n",
        "                return c\n",
        "    for c in cols:\n",
        "        lc = c.lower()\n",
        "        for b in bases:\n",
        "            if b in lc:\n",
        "                return c\n",
        "    return None\n",
        "\n",
        "def prepare_pattern_training_data(all_data):\n",
        "    X, y = [], []\n",
        "    win_count_total = 0\n",
        "    for symbol, data in all_data.items():\n",
        "        try:\n",
        "            if data is None or data.empty:\n",
        "                open(log_file,'a').write(f\"symbol={symbol} skipped: empty\\n\"); continue\n",
        "            df = data.copy()\n",
        "            open(log_file,'a').write(f\"symbol={symbol} columns={list(df.columns)}\\n\")\n",
        "            if isinstance(df.columns, pd.MultiIndex):\n",
        "                df.columns = ['_'.join(col).strip() for col in df.columns.values]\n",
        "                open(log_file,'a').write(f\"symbol={symbol} MultiIndex flattened to {list(df.columns)}\\n\")\n",
        "            df.columns = [str(c).strip() for c in df.columns]\n",
        "            # priorit√© par suffixe exact si pr√©sent\n",
        "            close = find_col(df, [f'close_{symbol.lower()}', 'close','adj close','adj_close','adjclose'])\n",
        "            high  = find_col(df, [f'high_{symbol.lower()}', 'high'])\n",
        "            low   = find_col(df, [f'low_{symbol.lower()}', 'low'])\n",
        "            volume = find_col(df, [f'volume_{symbol.lower()}', 'volume'])\n",
        "            open(log_file,'a').write(f\"symbol={symbol} mapped: close={close} high={high} low={low} volume={volume}\\n\")\n",
        "            if close is None:\n",
        "                alt_close = find_col(df, ['adj close','adj_close','adjclose'])\n",
        "                if alt_close is not None:\n",
        "                    close = alt_close\n",
        "                    open(log_file,'a').write(f\"symbol={symbol} fallback close-> {close}\\n\")\n",
        "            if not all([close, high, low]):\n",
        "                open(log_file,'a').write(f\"symbol={symbol} skipped: missing required columns\\n\"); continue\n",
        "            if volume is None:\n",
        "                df['Volume_proxy'] = df[close].pct_change().rolling(10, min_periods=1).std().fillna(0.01)\n",
        "                volume = 'Volume_proxy'\n",
        "                open(log_file,'a').write(f\"symbol={symbol} created volume proxy\\n\")\n",
        "            for col in [close, high, low, volume]:\n",
        "                df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "            df = df[[close, high, low, volume]].dropna()\n",
        "            n = len(df)\n",
        "            open(log_file,'a').write(f\"symbol={symbol} len={n} after dropna\\n\")\n",
        "            if n < 20:\n",
        "                open(log_file,'a').write(f\"symbol={symbol} skipped: n<20\\n\"); continue\n",
        "            win_count = 0\n",
        "            for i in range(0, n - 17):\n",
        "                seq = df.iloc[i:i+15]\n",
        "                fut = df.iloc[i+15:i+17]\n",
        "                if seq.isnull().any().any() or fut.isnull().any().any():\n",
        "                    continue\n",
        "                close_arr = seq[close].values.reshape(-1, 1)\n",
        "                vol_arr = seq[volume].values.reshape(-1, 1)\n",
        "                spread_arr = (seq[high] - seq[low]).values.reshape(-1, 1)\n",
        "                features = np.concatenate([close_arr, vol_arr, spread_arr], axis=1)\n",
        "                if features.shape != (15, 3):\n",
        "                    continue\n",
        "                current_price = float(seq[close].iloc[-1])\n",
        "                future_mean = float(fut[close].mean())\n",
        "                if current_price <= 0 or not np.isfinite(future_mean):\n",
        "                    continue\n",
        "                future_return = (future_mean - current_price) / current_price\n",
        "                if future_return > 0.002:\n",
        "                    label = 2\n",
        "                elif future_return < -0.002:\n",
        "                    label = 0\n",
        "                else:\n",
        "                    label = 1\n",
        "                X.append(features.astype(np.float32)); y.append(label); win_count += 1\n",
        "            win_count_total += win_count\n",
        "            open(log_file,'a').write(f\"symbol={symbol} windows={win_count}\\n\")\n",
        "        except Exception as e:\n",
        "            open(log_file,'a').write(f\"symbol={symbol} ERROR {str(e)[:200]}\\n\"); continue\n",
        "    open(log_file,'a').write(f\"total_windows={len(X)} total_by_symbol={win_count_total}\\n\")\n",
        "    if len(X) == 0:\n",
        "        print('‚ö†Ô∏è Aucune fen√™tre cr√©√©e! V√©rifiez le log.'); return np.array([]), np.array([])\n",
        "    X = np.array(X, dtype=np.float32); Y = np.array(y, dtype=np.int32)\n",
        "    print(f'‚úÖ Pr√©paration: X={X.shape}, y={Y.shape}')\n",
        "    return X, Y\n",
        "\n",
        "X_train, y_train = prepare_pattern_training_data(all_data)\n",
        "print(f'üìä √âchantillons: {X_train.shape[0] if len(X_train) > 0 else 0}')\n",
        "print(f\"üîç Log fen√™tres: voir {log_file}\")\n",
        "\n",
        "if X_train.shape[0] == 0:\n",
        "    print('\\nüìù Derni√®res lignes du log:')\n",
        "    with open(log_file, 'r') as f:\n",
        "        lines = f.readlines()\n",
        "        for line in lines[-20:]:\n",
        "            print(line.strip())\n",
        "\n",
        "strategy = tf.distribute.get_strategy()\n",
        "with strategy.scope():\n",
        "    inputs = tf.keras.Input(shape=(15,3), name='input')\n",
        "    x = tf.keras.layers.BatchNormalization()(inputs)\n",
        "    x = tf.keras.layers.LSTM(64, return_sequences=False, name='lstm_main_ncudnn', activation='tanh', recurrent_activation='sigmoid', dropout=0.1, recurrent_dropout=0.1)(x)\n",
        "    x = tf.keras.layers.Dropout(0.3)(x)\n",
        "    x = tf.keras.layers.Dense(32, activation='relu')(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.Dropout(0.3)(x)\n",
        "    outputs = tf.keras.layers.Dense(3, activation='softmax', name='output')(x)\n",
        "    model = tf.keras.Model(inputs, outputs, name='l4_ncudnn_lstm_model')\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(1e-3), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "print('‚úÖ Mod√®le L4 (non-cuDNN) cr√©√©')\n",
        "\n",
        "X_train = X_train.astype(np.float32); y_train = y_train.astype(np.int32)\n",
        "if X_train.shape[0] == 0:\n",
        "    print('‚ö†Ô∏è Dataset vide. G√©n√©ration synth√©tique minimale...')\n",
        "    X_train = np.random.randn(1024, 15, 3).astype(np.float32)\n",
        "    y_train = np.random.randint(0, 3, size=(1024,)).astype(np.int32)\n",
        "\n",
        "scaler = (StandardScaler().fit(X_train.reshape(-1,3)))\n",
        "Xs = scaler.transform(X_train.reshape(-1,3)).reshape(X_train.shape)\n",
        "callbacks = [\n",
        "    tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True, monitor='val_loss', verbose=1),\n",
        "    tf.keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=3, monitor='val_loss', verbose=1)\n",
        "]\n",
        "hist = model.fit(Xs, y_train, epochs=10, batch_size=64, validation_split=0.2, callbacks=callbacks, verbose=1)\n",
        "print('‚úÖ Entra√Ænement termin√©')\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "os.makedirs(f'{base_path}/models', exist_ok=True)\n",
        "model.save(f'{base_path}/models/pattern_model_l4_ncudnn.keras')\n",
        "import pickle as pkl\n",
        "with open(f'{base_path}/models/pattern_scaler.pkl','wb') as f: pkl.dump(scaler,f)\n",
        "print('üíæ Mod√®le/scaler sauvegard√©s')\n",
        "try:\n",
        "    plt.figure(figsize=(12,4));\n",
        "    if 'accuracy' in hist.history:\n",
        "        plt.subplot(1,2,1); plt.plot(hist.history['accuracy']); \n",
        "        if 'val_accuracy' in hist.history: plt.plot(hist.history['val_accuracy']); plt.title('Accuracy'); plt.legend(['train','val'])\n",
        "    if 'loss' in hist.history:\n",
        "        plt.subplot(1,2,2); plt.plot(hist.history['loss']);\n",
        "        if 'val_loss' in hist.history: plt.plot(hist.history['val_loss']); plt.title('Loss'); plt.legend(['train','val']); plt.tight_layout(); plt.show()\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "update_progress('cell_5_sentiment_training')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6) Entra√Ænement Sentiment Analyzer (FinBERT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os, sys, subprocess, importlib\n",
        "os.environ.pop('COLAB_TPU_ADDR', None)\n",
        "for mod in list(sys.modules.keys()):\n",
        "    if mod.startswith('torch_xla') or mod == '_XLAC':\n",
        "        sys.modules.pop(mod, None)\n",
        "subprocess.run(['pip', 'uninstall', '-y', 'torch-xla'], check=False)\n",
        "subprocess.run(['pip', 'install', '-q', '--upgrade', 'torch==2.2.2', 'transformers==4.41.2'], check=False)\n",
        "importlib.invalidate_caches()\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class SentimentDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
        "        self.texts=texts; self.labels=labels; self.tokenizer=tokenizer; self.max_length=max_length\n",
        "    def __len__(self): return len(self.texts)\n",
        "    def __getitem__(self, idx):\n",
        "        t=str(self.texts[idx]); l=self.labels[idx]\n",
        "        enc=self.tokenizer(t,truncation=True,padding='max_length',max_length=self.max_length,return_tensors='pt')\n",
        "        return {'input_ids':enc['input_ids'].flatten(),'attention_mask':enc['attention_mask'].flatten(),'labels':torch.tensor(l)}\n",
        "\n",
        "texts=['Apple reports record quarterly earnings','Google stock drops on regulatory concerns','Tesla announces new battery technology','Microsoft cloud growth exceeds expectations','Amazon faces antitrust investigation']\n",
        "labels=[2,0,2,2,0]\n",
        "model_name='yiyanghkust/finbert-tone'\n",
        "tok=AutoTokenizer.from_pretrained(model_name)\n",
        "mdl=AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "ds=SentimentDataset(texts,labels,tok)\n",
        "args=TrainingArguments(output_dir=f'{base_path}/checkpoints/sentiment',num_train_epochs=1,per_device_train_batch_size=4,logging_dir=f'{base_path}/logs/sentiment',logging_steps=1,save_strategy='no',evaluation_strategy='no')\n",
        "trainer=Trainer(model=mdl,args=args,train_dataset=ds,tokenizer=tok)\n",
        "print('üöÄ Entra√Ænement FinBERT...'); trainer.train()\n",
        "os.makedirs(f'{base_path}/models',exist_ok=True)\n",
        "mdl.save_pretrained(f'{base_path}/models/finbert_sentiment'); tok.save_pretrained(f'{base_path}/models/finbert_sentiment')\n",
        "print('‚úÖ FinBERT sauvegard√©')\n",
        "update_progress('cell_6_rag_training')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7) RAG Integrator (Embeddings + FAISS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss, numpy as np, pickle, os\n",
        "docs=[\n",
        " 'Apple Inc. is a technology company...',\n",
        " 'Alphabet Inc. provides various products...',\n",
        " 'Microsoft Corporation develops software...',\n",
        " 'Amazon.com, Inc. engages in retail...',\n",
        " 'Tesla, Inc. designs electric vehicles...']\n",
        "emb=SentenceTransformer('all-MiniLM-L6-v2')\n",
        "vecs=emb.encode(docs)\n",
        "index=faiss.IndexFlatL2(vecs.shape[1]); index.add(vecs)\n",
        "faiss.write_index(index,f'{base_path}/models/faiss_index.bin')\n",
        "with open(f'{base_path}/models/documents.pkl','wb') as f: pickle.dump(docs,f)\n",
        "print('‚úÖ Index/documents sauvegard√©s')\n",
        "update_progress('cell_7_integration')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8) Int√©gration et tests rapides"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json, numpy as np, pickle\n",
        "from datetime import datetime\n",
        "print('üîß Int√©gration finale...')\n",
        "try:\n",
        "    pm=tf.keras.models.load_model(f'{base_path}/models/pattern_model_l4_ncudnn.keras')\n",
        "    print('‚úÖ Pattern model charg√©')\n",
        "    from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "    st=AutoTokenizer.from_pretrained(f'{base_path}/models/finbert_sentiment')\n",
        "    sm=AutoModelForSequenceClassification.from_pretrained(f'{base_path}/models/finbert_sentiment')\n",
        "    print('‚úÖ FinBERT charg√©')\n",
        "    import faiss\n",
        "    fx=faiss.read_index(f'{base_path}/models/faiss_index.bin')\n",
        "    with open(f'{base_path}/models/documents.pkl','rb') as f: DD=pickle.load(f)\n",
        "    print('‚úÖ FAISS/documents charg√©s')\n",
        "except Exception as e:\n",
        "    print(f'‚ùå Erreur chargement: {e}')\n",
        "\n",
        "report={'timestamp': datetime.now().isoformat(),'models_trained':{'pattern':'ok','sentiment':'ok','rag':'ok'}}\n",
        "os.makedirs(f'{base_path}/exports',exist_ok=True)\n",
        "rp=f'{base_path}/exports/performance_report.json'\n",
        "with open(rp,'w') as f: json.dump(report,f,indent=2)\n",
        "print(f'üìä Rapport sauvegard√©: {rp}')\n",
        "update_progress('cell_8_testing')\n",
        "print('‚úÖ Pipeline termin√©')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "name": "ALPHABOT_ML_TRAINING_COLAB_v2.ipynb"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}