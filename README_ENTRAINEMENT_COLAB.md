# üöÄ AlphaBot ML/DL Training - Guide Google Colab

## üìã Vue d'ensemble

Ce guide explique comment entra√Æner les mod√®les Machine Learning et Deep Learning d'AlphaBot sur Google Colab en utilisant les ressources GPU/TPU optimis√©es.

## üéØ Objectifs

- **Entra√Æner 3 composants ML/DL** :
  - Pattern Detector (LSTM + CNN)
  - Sentiment Analyzer (FinBERT + RoBERTa)
  - RAG Integrator (Embeddings + FAISS)

- **Optimisations Colab** :
  - GPU/TPU acceleration
  - Mixed precision training
  - Memory management
  - Automatic checkpoints
  - Timeout protection

## üìÅ Structure des fichiers

```
Tradingbot_V2/
‚îú‚îÄ‚îÄ ALPHABOT_ML_TRAINING_COLAB.ipynb    # Notebook principal Colab
‚îú‚îÄ‚îÄ colab_utils.py                      # Utilitaires Colab optimis√©s
‚îú‚îÄ‚îÄ drive_manager.py                    # Gestion Google Drive
‚îú‚îÄ‚îÄ alphabot/ml/                        # Composants ML/DL
‚îÇ   ‚îú‚îÄ‚îÄ pattern_detector.py             # D√©tecteur de patterns
‚îÇ   ‚îú‚îÄ‚îÄ sentiment_analyzer.py           # Analyseur de sentiment
‚îÇ   ‚îî‚îÄ‚îÄ rag_integrator.py               # Int√©grateur RAG
‚îî‚îÄ‚îÄ README_ENTRAINEMENT_COLAB.md        # Ce guide
```

## üõ†Ô∏è Pr√©requis

### 1. Compte Google avec Drive
- Compte Google valide
- Google Drive activ√©
- 15GB+ d'espace disponible

### 2. Acc√®s Colab
- Acc√®s √† Google Colab (gratuit ou Pro)
- Pour GPU : Colab Pro recommand√© pour sessions plus longues

### 3. D√©pendances locales (optionnel)
Pour tester localement avant Colab :
```bash
pip install tensorflow torch transformers
pip install yfinance pandas numpy scikit-learn
```

## üöÄ D√©marrage rapide

### √âtape 1: Ouvrir Google Colab
1. Aller sur [colab.research.google.com](https://colab.research.google.com)
2. Cr√©er un nouveau notebook
3. Renommer-le "AlphaBot_ML_Training"

### √âtape 2: Configuration GPU
1. Dans le menu : `Runtime` ‚Üí `Change runtime type`
2. S√©lectionner `GPU` ou `TPU`
3. Cliquer `Save`

### √âtape 3: Upload des fichiers
M√©thode 1: Direct upload
```python
from google.colab import files
uploaded = files.upload()
```

M√©thode 2: Clone depuis GitHub (recommand√©)
```python
!git clone https://github.com/votre-username/AlphaBot.git
%cd AlphaBot
```

### √âtape 4: Ex√©cution du notebook
1. Ex√©cuter les cellules dans l'ordre
2. Attendre la fin de chaque cellule avant de passer √† la suivante
3. Surveiller les logs pour les erreurs

## üìä D√©tail du notebook Colab

### CELLULE 1: Setup GPU/TPU optimis√©
- Installation des d√©pendances
- D√©tection GPU/TPU
- Configuration mixed precision
- Memory management

### CELLULE 2: Google Drive setup
- Montage de Google Drive
- Cr√©ation de la structure de dossiers
- V√©rification des permissions

### CELLULE 3: Code AlphaBot setup
- Upload/configuration du code AlphaBot
- Import des modules ML
- V√©rification de l'int√©grit√©

### CELLULE 4: T√©l√©chargement des donn√©es
- T√©l√©chargement des donn√©es de march√© (Yahoo Finance)
- Pr√©paration des datasets
- Visualisation des distributions

### CELLULE 5: Entra√Ænement Pattern Detector
- Configuration LSTM/CNN
- Entra√Ænement avec callbacks optimis√©s
- Sauvegarde des mod√®les

### CELLULE 6: Entra√Ænement Sentiment Analyzer
- Fine-tuning FinBERT/RoBERTa
- Pr√©paration des donn√©es texte
- Entra√Ænement avec monitoring

### CELLULE 7: Entra√Ænement RAG Integrator
- Cr√©ation des embeddings
- Indexation FAISS
- Tests de recherche s√©mantique

### CELLULE 8: Int√©gration et tests
- Int√©gration des mod√®les entra√Æn√©s
- Tests de performance
- Export des r√©sultats

## ‚ö° Optimisations Colab

### GPU Memory Management
```python
# Configuration m√©moire GPU
gpus = tf.config.list_physical_devices('GPU')
if gpus:
    for gpu in gpus:
        tf.config.experimental.set_memory_growth(gpu, True)
```

### Mixed Precision Training
```python
# Activer la pr√©cision mixte
policy = tf.keras.mixed_precision.Policy('mixed_float16')
tf.keras.mixed_precision.set_global_policy(policy)
```

### Batch Size Optimization
```python
# Optimiser le batch size selon la m√©moire disponible
batch_size = optimize_batch_size(
    dataset_size=len(X_train),
    available_memory_gb=12.0,  # GPU RAM
    model_complexity='medium'
)
```

### Checkpoint Management
```python
# Callbacks optimis√©s
callbacks = create_colab_callbacks(
    model_name="pattern_detector",
    save_path="/content/drive/MyDrive/AlphaBot_ML_Training",
    patience=10
)
```

## üìà Monitoring et Performance

### M√©triques suivies
- **Accuracy** : Pr√©cision du mod√®le
- **Loss** : Fonction de perte
- **Val Accuracy** : Pr√©cision sur validation
- **Memory Usage** : Utilisation RAM/GPU
- **Training Time** : Temps d'entra√Ænement

### Visualisation
```python
# Courbes d'apprentissage
plt.figure(figsize=(12, 4))
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Training')
plt.plot(history.history['val_accuracy'], label='Validation')
plt.title('Model Accuracy')
plt.legend()
```

## üíæ Sauvegarde et Gestion

### Google Drive Structure
```
/AlphaBot_ML_Training/
‚îú‚îÄ‚îÄ models/                    # Mod√®les entra√Æn√©s
‚îÇ   ‚îú‚îÄ‚îÄ pattern_detector/
‚îÇ   ‚îú‚îÄ‚îÄ sentiment_analyzer/
‚îÇ   ‚îî‚îÄ‚îÄ rag_integrator/
‚îú‚îÄ‚îÄ data/                     # Donn√©es d'entra√Ænement
‚îú‚îÄ‚îÄ logs/                     # Logs et m√©triques
‚îú‚îÄ‚îÄ checkpoints/              # Checkpoints d'entra√Ænement
‚îú‚îÄ‚îÄ exports/                  # Exports et backups
‚îî‚îÄ‚îÄ configs/                  # Fichiers de configuration
```

### Sauvegarde automatique
```python
# Sauvegarder un mod√®le
drive_manager.save_model(
    model=trained_model,
    model_name="lstm_pattern_v1",
    model_type="pattern_detector",
    metadata={
        "accuracy": 0.89,
        "epochs": 50,
        "batch_size": 32
    }
)
```

## üîß D√©pannage

### Probl√®mes courants

1. **GPU non disponible**
   ```
   Solution: Runtime ‚Üí Change runtime type ‚Üí GPU
   ```

2. **M√©moire insuffisante**
   ```
   Solution: R√©duire batch_size ou activer memory growth
   ```

3. **Timeout Colab**
   ```
   Solution: Activer timeout protection dans les callbacks
   ```

4. **Erreur d'import**
   ```
   Solution: V√©rifier l'installation des d√©pendances
   ```

### Logs utiles
```python
# V√©rifier l'environnement
env_info = get_colab_environment()
print(json.dumps(env_info, indent=2))

# V√©rifier la m√©moire
memory_usage = ColabMemoryMonitor().get_memory_usage()
print(f"RAM utilis√©e: {memory_usage['percent_used']:.1f}%")
```

## üéØ R√©sultats attendus

### Pattern Detector
- **Accuracy cible** : > 85%
- **Latence** : < 50ms
- **M√©moire** : < 100MB

### Sentiment Analyzer
- **Accuracy cible** : > 90%
- **F1-score** : > 0.85
- **Temps d'inf√©rence** : < 100ms

### RAG Integrator
- **Recall@10** : > 0.8
- **Indexation** : < 1s pour 1000 documents
- **M√©moire** : < 500MB

## üìö Prochaines √©tapes

### Apr√®s l'entra√Ænement
1. **T√©l√©charger les mod√®les** depuis Google Drive
2. **Int√©grer dans AlphaBot** local
3. **Tester en paper trading**
4. **D√©ployer en production**

### Am√©liorations possibles
- **Data augmentation** pour plus de donn√©es
- **Hyperparameter tuning** avec Optuna
- **Model ensemble** pour meilleure performance
- **Real-time training** avec streaming data

## üìû Support

Pour toute question ou probl√®me :
1. V√©rifier les logs dans Colab
2. Consulter la documentation dans `docs/`
3. V√©rifier les issues GitHub
4. Contacter le support technique

---

**Note** : Ce guide est optimis√© pour Google Colab Pro. Pour la version gratuite, r√©duire les batch sizes et la dur√©e d'entra√Ænement.
