{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajustement du path pour que Colab trouve le module alphabot\n",
    "import sys\n",
    "sys.path.append('/content')\n",
    "sys.path.append('/content/alphabot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üîÑ Suivi de Progression et Reprise Automatique\n",
    "\n",
    "Cette cellule v√©rifie l'√©tat d'avancement du notebook et permet de reprendre l√† o√π le processus s'est arr√™t√©."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# üîÑ Syst√®me de suivi et reprise automatique\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "# D√©finir le chemin de base\n",
    "base_path = '/content/drive/MyDrive/AlphaBot_ML_Training'\n",
    "os.makedirs(base_path, exist_ok=True)\n",
    "\n",
    "# Fichier de suivi de progression\n",
    "progress_file = f'{base_path}/progress_tracker.json'\n",
    "\n",
    "# √âtat initial des √©tapes\n",
    "default_progress = {\n",
    "    'cell_1_setup': False,\n",
    "    'cell_2_data_download': False,\n",
    "    'cell_3_data_analysis': False,\n",
    "    'cell_4_pattern_training': False,\n",
    "    'cell_5_sentiment_training': False,\n",
    "    'cell_6_rag_training': False,\n",
    "    'cell_7_integration': False,\n",
    "    'cell_8_testing': False,\n",
    "    'cell_9_deployment': False,\n",
    "    'last_cell_executed': None,\n",
    "    'start_time': None,\n",
    "    'last_update': None\n",
    "}\n",
    "\n",
    "# Charger ou initialiser le suivi\n",
    "try:\n",
    "    with open(progress_file, 'r') as f:\n",
    "        progress = json.load(f)\n",
    "    print(\"üìä Suivi de progression charg√©\")\n",
    "except:\n",
    "    progress = default_progress.copy()\n",
    "    progress['start_time'] = datetime.now().isoformat()\n",
    "    print(\"üÜï Nouveau suivi de progression initialis√©\")\n",
    "\n",
    "# Fonction pour mettre √† jour la progression\n",
    "def update_progress(cell_name):\n",
    "    if cell_name not in progress:\n",
    "        if cell_name not in progress:\n",
    "        progress[cell_name] = True\n",
    "    else:\n",
    "        progress[cell_name] = True\n",
    "    else:\n",
    "        if cell_name not in progress:\n",
    "        progress[cell_name] = True\n",
    "    else:\n",
    "        progress[cell_name] = True\n",
    "    progress['last_cell_executed'] = cell_name\n",
    "    progress['last_update'] = datetime.now().isoformat()\n",
    "    \n",
    "    with open(progress_file, 'w') as f:\n",
    "        json.dump(progress, f, indent=2)\n",
    "    \n",
    "    print(f\"‚úÖ Progression mise √† jour: {cell_name}\")\n",
    "\n",
    "# Fonction pour v√©rifier l'√©tat\n",
    "def check_progress():\n",
    "    print(\"\\nüìã √âtat actuel de la progression:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    completed = sum(1 for k,v in progress.items() if isinstance(v, bool) and v)  # Compter uniquement True\n",
    "    total = len([k for k in default_progress.keys() if k.startswith('cell_')])\n",
    "    \n",
    "    \n",
    "    print(f\"üìä Progression: {completed}/{total} √©tapes compl√©t√©es ({completed/total*100:.1f}%)\")\n",
    "    print(f\"‚è∞ D√©marr√©: {progress.get('start_time', 'N/A')}\")\n",
    "    print(f\"üîÑ Derni√®re mise √† jour: {progress.get('last_update', 'N/A')}\")\n",
    "    print(f\"üìç Derni√®re cellule: {progress.get('last_cell_executed', 'Aucune')}\")\n",
    "    \n",
    "    print(\"\\nüìù Statut des √©tapes:\")\n",
    "    steps = [\n",
    "        ('cell_1_setup', '1. Configuration initiale'),\n",
    "        ('cell_2_data_download', '2. T√©l√©chargement des donn√©es'),\n",
    "        ('cell_3_data_analysis', '3. Analyse des donn√©es'),\n",
    "        ('cell_4_pattern_training', '4. Entra√Ænement Pattern Detector'),\n",
    "        ('cell_5_sentiment_training', '5. Entra√Ænement Sentiment Analyzer'),\n",
    "        ('cell_6_rag_training', '6. Entra√Ænement RAG'),\n",
    "        ('cell_7_integration', '7. Int√©gration'),\n",
    "        ('cell_8_testing', '8. Tests'),\n",
    "        ('cell_9_deployment', '9. D√©ploiement')\n",
    "    ]\n",
    "    \n",
    "    for step_key, step_name in steps:\n",
    "        status = \"‚úÖ\" if progress.get(step_key, False) else \"‚è≥\"\n",
    "        print(f\"  {status} {step_name}\")\n",
    "    \n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Sugg√©rer la prochaine √©tape\n",
    "    if not progress['cell_1_setup']:\n",
    "        print(\"\\nüöÄ Prochaine √©tape: Ex√©cuter la cellule 1 (Configuration)\")\n",
    "    elif not progress['cell_2_data_download']:\n",
    "        print(\"\\nüöÄ Prochaine √©tape: Ex√©cuter la cellule 2 (T√©l√©chargement des donn√©es)\")\n",
    "    elif not progress['cell_3_data_analysis']:\n",
    "        print(\"\\nüöÄ Prochaine √©tape: Ex√©cuter la cellule 3 (Analyse des donn√©es)\")\n",
    "    elif not progress['cell_4_pattern_training']:\n",
    "        print(\"\\nüöÄ Prochaine √©tape: Ex√©cuter la cellule 4 (Pattern Detector)\")\n",
    "    elif not progress['cell_5_sentiment_training']:\n",
    "        print(\"\\nüöÄ Prochaine √©tape: Ex√©cuter la cellule 5 (Sentiment Analyzer)\")\n",
    "    elif not progress['cell_6_rag_training']:\n",
    "        print(\"\\nüöÄ Prochaine √©tape: Ex√©cuter la cellule 6 (RAG)\")\n",
    "    elif not progress['cell_7_integration']:\n",
    "        print(\"\\nüöÄ Prochaine √©tape: Ex√©cuter la cellule 7 (Int√©gration)\")\n",
    "    elif not progress['cell_8_testing']:\n",
    "        print(\"\\nüöÄ Prochaine √©tape: Ex√©cuter la cellule 8 (Tests)\")\n",
    "    elif not progress['cell_9_deployment']:\n",
    "        print(\"\\nüöÄ Prochaine √©tape: Ex√©cuter la cellule 9 (D√©ploiement)\")\n",
    "    else:\n",
    "        print(\"\\nüéâ Toutes les √©tapes sont compl√©t√©es !\")\n",
    "\n",
    "# V√©rifier l'√©tat actuel\n",
    "check_progress()\n",
    "\n",
    "# Instructions pour l'utilisateur\n",
    "print(\"\\nüí° Instructions:\")\n",
    "print(\"1. Ex√©cutez cette cellule pour voir l'√©tat d'avancement\")\n",
    "print(\"2. Chaque cellule mettra √† jour automatiquement sa progression\")\n",
    "print(\"3. Si le processus s'arr√™te, relancez simplement cette cellule\")\n",
    "print(\"4. Continuez avec la cellule sugg√©r√©e\")\n",
    "print(\"\\nüîÑ Note: Le syst√®me est con√ßu pour supporter les arr√™ts/red√©marrages\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ AlphaBot ML/DL Training - Google Colab\n",
    "\n",
    "## üìã Vue d'ensemble\n",
    "\n",
    "Ce notebook entra√Æne les mod√®les Machine Learning et Deep Learning d'AlphaBot :\n",
    "- Pattern Detector (LSTM + CNN)\n",
    "- Sentiment Analyzer (FinBERT + RoBERTa)\n",
    "- RAG Integrator (Embeddings + FAISS)\n",
    "\n",
    "## ‚ö° Optimisations\n",
    "- GPU/TPU acceleration\n",
    "- Mixed precision training\n",
    "- Memory management\n",
    "- Automatic checkpoints\n",
    "- Timeout protection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELLULE 1: Setup GPU/TPU optimis√©\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "import json\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Configuration logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# D√©tection GPU/TPU\n",
    "try:\n",
    "    # D√©tecter TPU\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    strategy = tf.distribute.TPUStrategy(tpu)\n",
    "    print('‚úÖ TPU d√©tect√©e et configur√©e')\n",
    "except:\n",
    "    try:\n",
    "        # D√©tecter GPU\n",
    "        gpus = tf.config.list_physical_devices('GPU')\n",
    "        if gpus:\n",
    "            for gpu in gpus:\n",
    "                tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            strategy = tf.distribute.MirroredStrategy()\n",
    "            print(f'‚úÖ {len(gpus)} GPU(s) d√©tect√©e(s)')\n",
    "        else:\n",
    "            strategy = tf.distribute.get_strategy()\n",
    "            print('‚ö†Ô∏è Aucun GPU/TPU d√©tect√©, utilisation du CPU')\n",
    "    except Exception as e:\n",
    "        strategy = tf.distribute.get_strategy()\n",
    "        print(f'‚ö†Ô∏è Erreur de configuration GPU: {e}')\n",
    "\n",
    "# Activer mixed precision\n",
    "try:\n",
    "    policy = tf.keras.mixed_precision.Policy('mixed_float16')\n",
    "    tf.keras.mixed_precision.set_global_policy(policy)\n",
    "    print('‚úÖ Mixed precision activ√©e')\n",
    "except:\n",
    "    print('‚ö†Ô∏è Mixed precision non disponible')\n",
    "\n",
    "# Afficher les infos\n",
    "print(f\"\\nüìä Configuration:\")\n",
    "print(f\"- TensorFlow: {tf.__version__}\")\n",
    "print(f\"- PyTorch: {torch.__version__}\")\n",
    "print(f\"- Strategy: {strategy}\")\n",
    "print(f\"- GPUs disponibles: {tf.config.list_physical_devices('GPU')}\")\n",
    "\n",
    "# V√©rifier CUDA\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"- CUDA disponible: {torch.version.cuda}\")\n",
    "    print(f\"- GPU courant: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"- M√©moire GPU: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîß Configuration Google Drive (r√©siliente v2)...\")\n",
    "from google.colab import drive\n",
    "import os, shutil, time\n",
    "\n",
    "MOUNT_POINT = '/content/drive'\n",
    "\n",
    "def _safe_cleanup_mount_point(mp: str):\n",
    "    try:\n",
    "        # S√©curiser: si bind√© ou symlink, supprimer l'entr√©e\n",
    "        if os.path.islink(mp):\n",
    "            print(\"‚ÑπÔ∏è Le point de montage est un symlink ‚Äî suppression...\")\n",
    "            os.unlink(mp)\n",
    "        # Si dossier existe et contient des fichiers r√©siduels locaux (pas Drive), on nettoie\n",
    "        if os.path.isdir(mp):\n",
    "            for entry in os.listdir(mp):\n",
    "                p = os.path.join(mp, entry)\n",
    "                try:\n",
    "                    if os.path.isfile(p) or os.path.islink(p):\n",
    "                        os.remove(p)\n",
    "                    elif os.path.isdir(p):\n",
    "                        shutil.rmtree(p)\n",
    "                except Exception as e:\n",
    "                    print(f\"‚ö†Ô∏è Ignor√© pendant nettoyage: {p} -> {e}\")\n",
    "        else:\n",
    "            os.makedirs(mp, exist_ok=True)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Probl√®me nettoyage mount point: {e}\")\n",
    "\n",
    "def _force_unmount():\n",
    "    try:\n",
    "        drive.flush_and_unmount()\n",
    "        print(\"‚ÑπÔ∏è flush_and_unmount ex√©cut√©\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ÑπÔ∏è flush_and_unmount non n√©cessaire: {e}\")\n",
    "    # En plus, tenter umount syst√®me si n√©cessaire\n",
    "    try:\n",
    "        os.system('fusermount -u /content/drive 2>/dev/null || true')\n",
    "        os.system('umount /content/drive 2>/dev/null || true')\n",
    "    except Exception as e:\n",
    "        print(f\"‚ÑπÔ∏è umount non n√©cessaire: {e}\")\n",
    "\n",
    "print(\"üîé √âtat initial:\")\n",
    "print(f\" - ismount: {os.path.ismount(MOUNT_POINT)}\")\n",
    "print(f\" - existe: {os.path.exists(MOUNT_POINT)}\")\n",
    "try:\n",
    "    print(f\" - contenu: {os.listdir(MOUNT_POINT) if os.path.isdir(MOUNT_POINT) else 'N/A'}\")\n",
    "except Exception as _:\n",
    "    print(\" - contenu: N/A\")\n",
    "\n",
    "# √âtape 1: forcer un d√©montage (au cas o√π)\n",
    "_force_unmount()\n",
    "time.sleep(1)\n",
    "\n",
    "# √âtape 2: nettoyage du point de montage\n",
    "_safe_cleanup_mount_point(MOUNT_POINT)\n",
    "time.sleep(0.5)\n",
    "\n",
    "# √âtape 3: montage forc√© avec gestion des erreurs\n",
    "try:\n",
    "    drive.mount(MOUNT_POINT, force_remount=True)\n",
    "    print(\"‚úÖ Drive mont√© (v2)\")\n",
    "except Exception as e:\n",
    "    msg = str(e)\n",
    "    print(f\"‚ùå drive.mount a √©chou√©: {msg}\")\n",
    "    if 'Mountpoint must not already contain files' in msg or 'symlink' in msg.lower():\n",
    "        print(\"üîß Correction approfondie: suppression et recr√©ation du dossier de montage\")\n",
    "        try:\n",
    "            # Supprimer compl√®tement et recr√©er /content/drive\n",
    "            if os.path.exists(MOUNT_POINT):\n",
    "                shutil.rmtree(MOUNT_POINT, ignore_errors=True)\n",
    "            os.makedirs(MOUNT_POINT, exist_ok=True)\n",
    "        except Exception as e2:\n",
    "            print(f\"‚ö†Ô∏è Impossible de recr√©er {MOUNT_POINT}: {e2}\")\n",
    "        # Retenter un dernier montage\n",
    "        drive.mount(MOUNT_POINT, force_remount=True)\n",
    "        print(\"‚úÖ Drive mont√© apr√®s recr√©ation du dossier\")\n",
    "    else:\n",
    "        raise\n",
    "\n",
    "# √âtape 4: pr√©parer l'arborescence projet\n",
    "base_path = '/content/drive/MyDrive/AlphaBot_ML_Training'\n",
    "os.makedirs(base_path, exist_ok=True)\n",
    "for sub in ('data', 'models', 'checkpoints', 'logs'):\n",
    "    os.makedirs(f\"{base_path}/{sub}\", exist_ok=True)\n",
    "print(f\"üìÅ R√©pertoires pr√™ts sous: {base_path}\")\n",
    "\n",
    "# V√©rification finale\n",
    "print(\"üîé V√©rification finale:\")\n",
    "print(f\" - ismount: {os.path.ismount(MOUNT_POINT)}\")\n",
    "try:\n",
    "    print(f\" - contenu: {os.listdir(MOUNT_POINT)}\")\n",
    "except Exception:\n",
    "    print(\" - contenu: N/A\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELLULE 3: Code AlphaBot setup\n",
    "# D√©finir le chemin de base si pas d√©j√† d√©fini\n",
    "if 'base_path' not in globals():\n",
    "    base_path = '/content/drive/MyDrive/AlphaBot_ML_Training'\n",
    "    os.makedirs(base_path, exist_ok=True)\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Cloner le d√©p√¥t AlphaBot\n",
    "if not Path('/content/alphabot').exists():\n",
    "    print(\"üì• Clonage du d√©p√¥t AlphaBot...\")\n",
    "    subprocess.run(['git', 'clone', 'https://github.com/thomy03/alphabot.git', '/content/alphabot'], check=True)\n",
    "else:\n",
    "    print(\"üìÇ D√©p√¥t AlphaBot d√©j√† pr√©sent\")\n",
    "\n",
    "# Installer les d√©pendances\n",
    "print(\"üì¶ Installation des d√©pendances...\")\n",
    "try:\n",
    "    subprocess.run(['pip', 'install', '-r', '/content/alphabot/requirements_colab.txt'], check=True)\n",
    "    print(\"‚úÖ D√©pendances install√©es avec succ√®s\")\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(f\"‚ö†Ô∏è Erreur lors de l'installation: {e}\")\n",
    "    print(\"üîß Installation des d√©pendances essentielles manuellement...\")\n",
    "    essential_packages = [\n",
    "        'tensorflow', 'torch', 'transformers', 'sentence-transformers', \n",
    "        'faiss-cpu', 'yfinance', 'pandas', 'numpy', 'scikit-learn',\n",
    "        'matplotlib', 'seaborn', 'tqdm', 'requests'\n",
    "    ]\n",
    "    for package in essential_packages:\n",
    "        try:\n",
    "            result = subprocess.run(['pip', 'install', package], check=True, capture_output=True, text=True)\n",
    "            print(f\"‚úÖ {package} install√©\")\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"‚ùå √âchec installation {package}: {e.stderr.strip() if e.stderr else 'Erreur inconnue'}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå √âchec installation {package}: {str(e)}\")\n",
    "\n",
    "# Importer les modules AlphaBot\n",
    "sys.path.append('/content')\n",
    "sys.path.append('/content/alphabot')\n",
    "\n",
    "# V√©rifier que le dossier alphabot existe\n",
    "import os\n",
    "if not os.path.exists('/content/alphabot/alphabot/ml'):\n",
    "    print(\"‚ùå Dossier alphabot/ml non trouv√©\")\n",
    "    print(\"üìÇ Structure du dossier:\")\n",
    "    if os.path.exists('/content/alphabot'):\n",
    "        for root, dirs, files in os.walk('/content/alphabot'):\n",
    "            level = root.replace('/content/alphabot', '').count(os.sep)\n",
    "            indent = ' ' * 2 * level\n",
    "            print(f\"{indent}{os.path.basename(root)}/\")\n",
    "            subindent = ' ' * 2 * (level + 1)\n",
    "            for file in files[:5]:  # Limiter √† 5 fichiers par dossier\n",
    "                print(f\"{subindent}{file}\")\n",
    "            if len(files) > 5:\n",
    "                print(f\"{subindent}... et {len(files)-5} autres fichiers\")\n",
    "else:\n",
    "    try:\n",
    "        from alphabot.ml.pattern_detector import MLPatternDetector\n",
    "        from alphabot.ml.sentiment_analyzer import SentimentAnalyzer\n",
    "        from alphabot.ml.rag_integrator import RAGIntegrator\n",
    "        print(\"‚úÖ Modules AlphaBot import√©s avec succ√®s\")\n",
    "        \n",
    "        # Initialiser les composants uniquement si l'import a r√©ussi\n",
    "        try:\n",
    "            pattern_detector = MLPatternDetector()\n",
    "            sentiment_analyzer = SentimentAnalyzer()\n",
    "            rag_integrator = RAGIntegrator()\n",
    "            print(\"‚úÖ Composants ML initialis√©s\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erreur d'initialisation: {e}\")\n",
    "            print(\"üîß Les composants seront cr√©√©s plus tard dans le notebook\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erreur d'import: {e}\")\n",
    "        print(\"üîß Cr√©ation des modules de secours...\")\n",
    "        \n",
    "        # Cr√©er des classes de secours pour permettre au notebook de continuer\n",
    "        class MLPatternDetector:\n",
    "            def __init__(self):\n",
    "                print(\"üîß MLPatternDetector de secours cr√©√©\")\n",
    "        \n",
    "        class SentimentAnalyzer:\n",
    "            def __init__(self):\n",
    "                print(\"üîß SentimentAnalyzer de secours cr√©√©\")\n",
    "        \n",
    "        class RAGIntegrator:\n",
    "            def __init__(self):\n",
    "                print(\"üîß RAGIntegrator de secours cr√©√©\")\n",
    "        \n",
    "        # Initialiser les composants de secours\n",
    "        pattern_detector = MLPatternDetector()\n",
    "        sentiment_analyzer = SentimentAnalyzer()\n",
    "        rag_integrator = RAGIntegrator()\n",
    "        print(\"‚úÖ Composants de secours initialis√©s\")\n",
    "\n",
    "# Importer les utilitaires (avec gestion d'erreur)\n",
    "try:\n",
    "    from colab_utils import ColabMemoryMonitor, create_colab_callbacks\n",
    "    from drive_manager import DriveManager\n",
    "    drive_manager = DriveManager(base_path)\n",
    "    memory_monitor = ColabMemoryMonitor()\n",
    "    print(\"‚úÖ Utilitaires import√©s\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Utilitaires non disponibles: {e}\")\n",
    "    # Cr√©er des utilitaires de secours\n",
    "    class DriveManager:\n",
    "        def __init__(self, path):\n",
    "            self.path = path\n",
    "        def save_model(self, **kwargs):\n",
    "            print(f\"üîß Sauvegarde simul√©e dans {self.path}\")\n",
    "    \n",
    "    class ColabMemoryMonitor:\n",
    "        def get_memory_usage(self):\n",
    "            return {\"percent_used\": 50.0}\n",
    "    \n",
    "    drive_manager = DriveManager(base_path)\n",
    "    memory_monitor = ColabMemoryMonitor()\n",
    "    print(\"‚úÖ Utilitaires de secours cr√©√©s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELLULE 4: T√©l√©chargement des donn√©es\n",
    "# D√©finir le chemin de base si pas d√©j√† d√©fini\n",
    "if 'base_path' not in globals():\n",
    "    base_path = '/content/drive/MyDrive/AlphaBot_ML_Training'\n",
    "    os.makedirs(base_path, exist_ok=True)\n",
    "\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "\n",
    "# Configuration\n",
    "symbols = ['AAPL', 'GOOGL', 'MSFT', 'TSLA', 'AMZN']\n",
    "end_date = datetime.now()\n",
    "start_date = end_date - timedelta(days=365*2)  # 2 ans de donn√©es\n",
    "\n",
    "print(f\"üì• T√©l√©chargement des donn√©es pour {symbols}...\")\n",
    "print(f\"üìÖ P√©riode: {start_date.strftime('%Y-%m-%d')} √† {end_date.strftime('%Y-%m-%d')}\")\n",
    "\n",
    "# T√©l√©charger les donn√©es\n",
    "all_data = {}\n",
    "for symbol in symbols:\n",
    "    try:\n",
    "        # Sp√©cifier explicitement auto_adjust pour √©viter le warning\n",
    "        data = yf.download(symbol, start=start_date, end=end_date, auto_adjust=False)\n",
    "        if not data.empty:\n",
    "            all_data[symbol] = data\n",
    "            print(f\"‚úÖ {symbol}: {len(data)} jours de donn√©es\")\n",
    "        else:\n",
    "            print(f\"‚ùå {symbol}: Pas de donn√©es disponibles\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå {symbol}: Erreur de t√©l√©chargement - {e}\")\n",
    "\n",
    "# Sauvegarder les donn√©es\n",
    "import pickle\n",
    "data_path = f\"{base_path}/data/market_data.pkl\"\n",
    "with open(data_path, 'wb') as f:\n",
    "    pickle.dump(all_data, f)\n",
    "\n",
    "print(f\"\\nüíæ Donn√©es sauvegard√©es dans: {data_path}\")\n",
    "print(f\"üìä Total symboles: {len(all_data)}\")\n",
    "\n",
    "# Afficher un exemple\n",
    "if all_data:\n",
    "    sample_symbol = list(all_data.keys())[0]\n",
    "    sample_data = all_data[sample_symbol]\n",
    "    print(f\"\\nüìà Exemple pour {sample_symbol}:\")\n",
    "    print(f\"- Premi√®re date: {sample_data.index[0].strftime('%Y-%m-%d')}\")\n",
    "    print(f\"- Derni√®re date: {sample_data.index[-1].strftime('%Y-%m-%d')}\")\n",
    "    \n",
    "    # Calculer et afficher les statistiques sans warnings\n",
    "    mean_price = float(sample_data['Close'].mean())\n",
    "    volatility = float(sample_data['Close'].pct_change().std() * 100)\n",
    "    \n",
    "    print(f\"- Prix moyen: ${mean_price:.2f}\")\n",
    "    print(f\"- Volatilit√©: {volatility:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üß† Entra√Ænement du Pattern Detector (LSTM + CNN)...\")\n",
    "\n",
    "# Importer les biblioth√®ques n√©cessaires\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pickle\n",
    "from datetime import datetime, timedelta\n",
    "import yfinance as yf\n",
    "\n",
    "# Charger les donn√©es depuis la cellule pr√©c√©dente\n",
    "try:\n",
    "    # Essayer de charger depuis le pickle sauvegard√©\n",
    "    with open(f'{base_path}/data/market_data.pkl', 'rb') as f:\n",
    "        all_data = pickle.load(f)\n",
    "    print(\"‚úÖ Donn√©es charg√©es depuis le pickle\")\n",
    "except:\n",
    "    print(\"üîß Re-t√©l√©chargement des donn√©es...\")\n",
    "    # Re-t√©l√©charger les donn√©es si le pickle n'est pas disponible\n",
    "    symbols = ['AAPL', 'GOOGL', 'MSFT', 'TSLA', 'AMZN']\n",
    "    end_date = datetime.now().strftime('%Y-%m-%d')\n",
    "    start_date = (datetime.now() - timedelta(days=730)).strftime('%Y-%m-%d')\n",
    "    \n",
    "    all_data = {}\n",
    "    for symbol in symbols:\n",
    "        print(f\"üì• T√©l√©chargement des donn√©es pour {symbol}...\")\n",
    "        data = yf.download(symbol, start=start_date, end=end_date, auto_adjust=False)\n",
    "        if not data.empty:\n",
    "            all_data[symbol] = data\n",
    "            print(f\"‚úÖ {symbol}: {len(data)} jours de donn√©es\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è {symbol}: Pas de donn√©es disponibles\")\n",
    "    \n",
    "    # Sauvegarder pour √©viter de re-t√©l√©charger\n",
    "    try:\n",
    "        os.makedirs(f'{base_path}/data', exist_ok=True)\n",
    "        with open(f'{base_path}/data/market_data.pkl', 'wb') as f:\n",
    "            pickle.dump(all_data, f)\n",
    "        print(f\"üíæ Donn√©es sauvegard√©es dans: {base_path}/data/market_data.pkl\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Erreur de sauvegarde: {e}\")\n",
    "\n",
    "print(f\"üìä Total symboles: {len(all_data)}\")\n",
    "\n",
    "# Pr√©parer les donn√©es\n",
    "print(\"üîß Pr√©paration des donn√©es (s√©curis√©e)...\")\n",
    "\n",
    "print(\"üîß Pr√©paration des donn√©es (s√©curis√©e v2 - yfinance compatibles)...\")\n",
    "def prepare_pattern_training_data(all_data):\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "\n",
    "    def normalize_yf_cols(df):\n",
    "        # Aplatissement MultiIndex √©ventuel\n",
    "        if isinstance(df.columns, pd.MultiIndex):\n",
    "            df.columns = ['_'.join([str(c) for c in col if c is not None]) for col in df.columns]\n",
    "        else:\n",
    "            df.columns = [str(c) for c in df.columns]\n",
    "        # Mapping de colonnes standards possibles\n",
    "        candidates = {}\n",
    "        for key in ['Close', 'Adj Close', 'Adj_Close', 'Close_Adj Close']:\n",
    "            candidates['Close'] = candidates.get('Close') or next((c for c in df.columns if c.lower().replace(' ', '').replace('-', '_') == key.lower().replace(' ', '').replace('-', '_')), None)\n",
    "        for key in ['Volume']:\n",
    "            candidates['Volume'] = candidates.get('Volume') or next((c for c in df.columns if c.lower() == key.lower()), None)\n",
    "        for key in ['High']:\n",
    "            candidates['High'] = candidates.get('High') or next((c for c in df.columns if c.lower() == key.lower()), None)\n",
    "        for key in ['Low']:\n",
    "            candidates['Low'] = candidates.get('Low') or next((c for c in df.columns if c.lower() == key.lower()), None)\n",
    "        return candidates\n",
    "\n",
    "    X_train, y_train = [], []\n",
    "    for symbol, data in all_data.items():\n",
    "        try:\n",
    "            if data is None or not hasattr(data, 'empty') or data.empty:\n",
    "                print(f\"‚ö†Ô∏è {symbol}: dataset vide/None, ignor√©\")\n",
    "                continue\n",
    "\n",
    "            data = data.copy()\n",
    "            data = data.sort_index()\n",
    "\n",
    "            # D√©tecter les colonnes r√©elles √† utiliser\n",
    "            cols = normalize_yf_cols(data)\n",
    "            required = ['Close', 'Volume', 'High', 'Low']\n",
    "            if not all(cols.get(k) for k in required):\n",
    "                print(f\"‚ö†Ô∏è {symbol}: colonnes manquantes apr√®s normalisation {cols}, ignor√©\")\n",
    "                continue\n",
    "\n",
    "            close_col = cols['Close']; vol_col = cols['Volume']; hi_col = cols['High']; lo_col = cols['Low']\n",
    "            # Nettoyer NA\n",
    "            data = data.dropna(subset=[close_col, vol_col, hi_col, lo_col])\n",
    "\n",
    "            n = len(data)\n",
    "            if n < 36:\n",
    "                print(f\"‚ÑπÔ∏è {symbol}: pas assez de points ({n}), ignor√©\")\n",
    "                continue\n",
    "\n",
    "            # Fen√™trage\n",
    "            for i in range(0, n - 35):\n",
    "                seq = data.iloc[i:i+30]\n",
    "                next5 = data.iloc[i+30:i+35]\n",
    "                if seq[[close_col, vol_col, hi_col, lo_col]].isnull().any().any():\n",
    "                    continue\n",
    "                if next5[[close_col]].isnull().any().any():\n",
    "                    continue\n",
    "\n",
    "                close = np.asarray(seq[close_col].values, dtype=np.float32).reshape(-1, 1)\n",
    "                volume = np.asarray(seq[vol_col].values, dtype=np.float32).reshape(-1, 1)\n",
    "                spread = np.asarray((seq[hi_col] - seq[lo_col]).values, dtype=np.float32).reshape(-1, 1)\n",
    "                features = np.concatenate([close, volume, spread], axis=1)\n",
    "                if features.shape != (30, 3):\n",
    "                    continue\n",
    "\n",
    "                current_price = float(seq[close_col].iloc[-1])\n",
    "                if current_price == 0:\n",
    "                    continue\n",
    "                future_mean = float(np.mean(next5[close_col].values))\n",
    "                future_return = (future_mean - current_price) / current_price\n",
    "\n",
    "                if future_return > 0.02:\n",
    "                    label = 2\n",
    "                elif future_return < -0.02:\n",
    "                    label = 0\n",
    "                else:\n",
    "                    label = 1\n",
    "\n",
    "                X_train.append(features)\n",
    "                y_train.append(label)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Erreur sur {symbol}, segment ignor√©: {e}\")\n",
    "            continue\n",
    "\n",
    "    X_train = np.array(X_train, dtype=np.float32)\n",
    "    y_train = np.array(y_train, dtype=np.int32)\n",
    "    print(f\"‚úÖ Pr√©paration termin√©e: X={X_train.shape}, y={y_train.shape}\")\n",
    "    return X_train, y_train\n",
    "\n",
    "# Reconstruire X/y avec la nouvelle fonction\n",
    "X_train, y_train = prepare_pattern_training_data(all_data)\n",
    "print(f\"üìä Donn√©es pr√©par√©es: {X_train.shape[0]} √©chantillons\")\n",
    "X_train, y_train = prepare_pattern_training_data(all_data)\n",
    "print(f\"üìä Donn√©es pr√©par√©es: {X_train.shape[0]} √©chantillons\")\n",
    "print(f\"üìä Donn√©es pr√©par√©es: {X_train.shape[0]} √©chantillons\")\n",
    "\n",
    "# Cr√©er le mod√®le GPU optimis√© pour A100\n",
    "print(\"üîß Cr√©ation du mod√®le GPU (compatible L4)...\")\n",
    "\n",
    "# V√©rifier la disponibilit√© du GPU\n",
    "print(f\"üìä GPU disponible: {tf.config.list_physical_devices('GPU')}\")\n",
    "\n",
    "# Configuration simple pour √©viter les crashs\n",
    "# tf.keras.mixed_precision.set_global_policy('mixed_float16')  # D√©sactiv√© pour √©viter les crashs\n",
    "\n",
    "# Cr√©er le mod√®le\n",
    "strategy = tf.distribute.get_strategy()\n",
    "\n",
    "with strategy.scope():\n",
    "    \n",
    "# D√©sactiver l'utilisation CuDNN en for√ßant CPU si n√©cessaire\n",
    "import os\n",
    "os.environ.setdefault('TF_FORCE_GPU_ALLOW_GROWTH', 'true')\n",
    "# Important: √©viter le path CuDNN en fixant un device CPU pour LSTM\n",
    "use_cpu_lstm = True\n",
    "\n",
    "    \n",
    "# D√©sactiver l'utilisation CuDNN en for√ßant CPU si n√©cessaire\n",
    "import os\n",
    "os.environ.setdefault('TF_FORCE_GPU_ALLOW_GROWTH', 'true')\n",
    "# Important: √©viter le path CuDNN en fixant un device CPU pour LSTM\n",
    "use_cpu_lstm = True\n",
    "\n",
    "    # Utiliser un mod√®le plus simple pour √©viter les crashs GPU\n",
    "    inputs = tf.keras.Input(shape=(30, 3), name='input_layer')\n",
    "    \n",
    "    # Normalisation\n",
    "    x = tf.keras.layers.BatchNormalization()(inputs)\n",
    "    \n",
    "    # Une seule couche LSTM\n",
    "    import tensorflow as tf\n",
    "    with tf.device('/CPU:0'):\n",
    "        import tensorflow as tf\n",
    "    with tf.device('/CPU:0'):\n",
    "        x = tf.keras.layers.LSTM(\n",
    "        activation='tanh', recurrent_activation='sigmoid', use_bias=True, unit_forget_bias=True, unroll=False, time_major=False,\n",
    "        activation='tanh', recurrent_activation='sigmoid', use_bias=True, unit_forget_bias=True, unroll=False, time_major=False,\n",
    "        64, \n",
    "        return_sequences=False,\n",
    "        kernel_initializer='glorot_uniform',\n",
    "        recurrent_initializer='orthogonal',\n",
    "        name='lstm_main'\n",
    "    )(x)\n",
    "    x = tf.keras.layers.Dropout(0.3)(x)\n",
    "    \n",
    "    # Couches denses\n",
    "    x = tf.keras.layers.Dense(32, activation='relu')(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Dropout(0.3)(x)\n",
    "    \n",
    "    outputs = tf.keras.layers.Dense(3, activation='softmax', name='output')(x)\n",
    "    \n",
    "    # Cr√©er le mod√®le\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs, name='simplified_gpu_model')\n",
    "    \n",
    "    # Compiler avec des param√®tres simples\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "# Afficher le r√©sum√© du mod√®le\n",
    "print(\"‚úÖ Mod√®le GPU optimis√© cr√©√©:\")\n",
    "model.summary()\n",
    "\n",
    "# Callbacks simplifi√©s pour √©viter les crashs\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        patience=10, \n",
    "        restore_best_weights=True,\n",
    "        monitor='val_loss',\n",
    "        verbose=1\n",
    "    ),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        factor=0.5, \n",
    "        patience=5, \n",
    "        monitor='val_loss',\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "# V√©rifier et pr√©parer les donn√©es\n",
    "print(f\"üìä V√©rification des donn√©es:\")\n",
    "print(f\"  - X_train shape: {X_train.shape}\")\n",
    "print(f\"  - y_train shape: {y_train.shape}\")\n",
    "print(f\"  - X_train dtype: {X_train.dtype}\")\n",
    "print(f\"  - y_train dtype: {y_train.dtype}\")\n",
    "print(f\"  - Valeurs uniques dans y_train: {np.unique(y_train)}\")\n",
    "\n",
    "# S'assurer que les donn√©es sont du bon type pour GPU\n",
    "X_train = X_train.astype(np.float32)\n",
    "y_train = y_train.astype(np.int32)\n",
    "\n",
    "# Normaliser les donn√©es (ou fallback si vide)\n",
    "if X_train.shape[0] == 0:\n",
    "    print(\"‚ö†Ô∏è Aucun √©chantillon r√©el. G√©n√©ration d'un dataset synth√©tique minimal (CPU)...\")\n",
    "    import numpy as np\n",
    "    X_train = np.random.randn(256, 30, 3).astype(np.float32)\n",
    "    y_train = np.random.randint(0, 3, size=(256,)).astype(np.int32)\n",
    "    print(f\"‚úÖ Dataset synth√©tique: X={X_train.shape}, y={y_train.shape}\")\n",
    "# Normaliser les donn√©es pour GPU\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train.reshape(-1, X_train.shape[-1])).reshape(X_train.shape)\n",
    "\n",
    "# Entra√Æner avec des param√®tres optimis√©s pour A100\n",
    "print(\"üöÄ D√©but de l'entra√Ænement GPU (compatible L4)...\")\n",
    "try:\n",
    "    try:\n",
    "        history = model.fit(\n",
    "        X_train_scaled, y_train,\n",
    "        epochs=20,  # R√©duit pour √©viter les crashs\n",
    "        batch_size=64,\n",
    "        validation_split=0.2,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    print(\"‚úÖ Entra√Ænement GPU termin√© avec succ√®s\")\n",
    "    \n",
    "    # Sauvegarder le mod√®le et le scaler\n",
    "    try:\n",
    "        model.save(f'{base_path}/models/simplified_gpu_model.keras')\n",
    "        with open(f'{base_path}/models/simplified_scaler.pkl', 'wb') as f:\n",
    "            pickle.dump(scaler, f)\n",
    "        print(\"‚úÖ Mod√®le GPU et scaler sauvegard√©s\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Erreur de sauvegarde: {e}\")\n",
    "    \n",
    "    # Afficher les courbes d'apprentissage\n",
    "    try:\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        \n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(history.history['accuracy'], label='Training')\n",
    "        if 'val_accuracy' in history.history:\n",
    "            plt.plot(history.history['val_accuracy'], label='Validation')\n",
    "        plt.title('Model Accuracy')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(history.history['loss'], label='Training')\n",
    "        if 'val_loss' in history.history:\n",
    "            plt.plot(history.history['val_loss'], label='Validation')\n",
    "        plt.title('Model Loss')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "\n",
    "        if \"DNN\" in str(e) or \"CuDNN\" in str(e):\n",
    "\n",
    "            print(\"‚ö†Ô∏è DNN/CuDNN non support√© sur ce GPU. Passage √† un mod√®le Dense-only CPU...\")\n",
    "\n",
    "            with tf.distribute.get_strategy().scope():\n",
    "\n",
    "                model = tf.keras.Sequential([\n",
    "\n",
    "                    tf.keras.layers.Input(shape=(30,3)),\n",
    "\n",
    "                    tf.keras.layers.Flatten(),\n",
    "\n",
    "                    tf.keras.layers.Dense(64, activation='relu'),\n",
    "\n",
    "                    tf.keras.layers.Dropout(0.3),\n",
    "\n",
    "                    tf.keras.layers.Dense(32, activation='relu'),\n",
    "\n",
    "                    tf.keras.layers.Dense(3, activation='softmax')\n",
    "\n",
    "                ])\n",
    "\n",
    "                model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "            history = model.fit(\n",
    "\n",
    "                X_train_scaled, y_train,\n",
    "\n",
    "                epochs=10,\n",
    "\n",
    "                batch_size=64,\n",
    "\n",
    "                validation_split=0.2,\n",
    "\n",
    "                verbose=1\n",
    "\n",
    "            )\n",
    "\n",
    "        else:\n",
    "\n",
    "            raise\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Erreur lors de l'affichage des courbes: {e}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erreur lors de l'entra√Ænement GPU: {e}\")\n",
    "    print(\"üîß Analyse de l'erreur:\")\n",
    "    print(f\"  - Type d'erreur: {type(e).__name__}\")\n",
    "    print(f\"  - Message: {str(e)}\")\n",
    "    \n",
    "    # Si erreur CuDNN, essayer une approche CPU\n",
    "    if \"CuDNN\" in str(e) or \"DNN\" in str(e):\n",
    "        print(\"üîß D√©tection d'erreur CuDNN, passage en mode CPU...\")\n",
    "        import os\n",
    "        os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "        \n",
    "        # Recr√©er un mod√®le CPU simple\n",
    "        with tf.distribute.get_strategy().scope():\n",
    "            cpu_model = tf.keras.Sequential([\n",
    "                tf.keras.layers.Input(shape=(30, 3)),\n",
    "                tf.keras.layers.Flatten(),\n",
    "                tf.keras.layers.Dense(128, activation='relu'),\n",
    "                tf.keras.layers.Dropout(0.3),\n",
    "                tf.keras.layers.Dense(64, activation='relu'),\n",
    "                tf.keras.layers.Dropout(0.3),\n",
    "                tf.keras.layers.Dense(32, activation='relu'),\n",
    "                tf.keras.layers.Dense(3, activation='softmax')\n",
    "            ])\n",
    "            \n",
    "            cpu_model.compile(\n",
    "                optimizer='adam',\n",
    "                loss='sparse_categorical_crossentropy',\n",
    "                metrics=['accuracy']\n",
    "            )\n",
    "        \n",
    "        # Entra√Æner le mod√®le CPU\n",
    "        history = cpu_model.fit(\n",
    "            X_train_scaled, y_train,\n",
    "            epochs=15,  # R√©duit pour √©viter les crashs\n",
    "            batch_size=32,\n",
    "            validation_split=0.2,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        model = cpu_model\n",
    "        print(\"‚úÖ Mod√®le CPU de secours entra√Æn√©\")\n",
    "    else:\n",
    "        raise e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELLULE 6: Entra√Ænement Sentiment Analyzer\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# D√©finir le chemin de base\n",
    "base_path = '/content/drive/MyDrive/AlphaBot_ML_Training'\n",
    "os.makedirs(base_path, exist_ok=True)\n",
    "\n",
    "print(\"üí≠ Entra√Ænement du Sentiment Analyzer (FinBERT + RoBERTa)...\")\n",
    "\n",
    "# Cr√©er un dataset de d√©monstration\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Donn√©es de d√©monstration\n",
    "demo_texts = [\n",
    "    \"Apple reports record quarterly earnings\",\n",
    "    \"Google stock drops on regulatory concerns\",\n",
    "    \"Tesla announces new battery technology\",\n",
    "    \"Microsoft cloud growth exceeds expectations\",\n",
    "    \"Amazon faces antitrust investigation\",\n",
    "    \"Meta launches new VR platform\",\n",
    "    \"NVIDIA chips power AI revolution\",\n",
    "    \"Bitcoin reaches new all-time high\",\n",
    "    \"Federal Reserve raises interest rates\",\n",
    "    \"Oil prices surge on supply concerns\"\n",
    "]\n",
    "\n",
    "demo_labels = [2, 0, 2, 2, 0, 2, 2, 2, 0, 0]  # 0=n√©gatif, 1=neutre, 2=positif\n",
    "\n",
    "# Initialiser FinBERT\n",
    "model_name = \"yiyanghkust/finbert-tone\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# Cr√©er le dataset\n",
    "dataset = SentimentDataset(demo_texts, demo_labels, tokenizer)\n",
    "\n",
    "# Configuration de l'entra√Ænement\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f'{base_path}/checkpoints/sentiment',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    warmup_steps=10,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=f'{base_path}/logs/sentiment',\n",
    "    logging_steps=1,\n",
    "    save_steps=10,\n",
    "    evaluation_strategy=\"no\",\n",
    "    save_strategy=\"epoch\"\n",
    ")\n",
    "\n",
    "# Cr√©er le trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# Entra√Æner\n",
    "print(\"üöÄ D√©but de l'entra√Ænement FinBERT...\")\n",
    "trainer.train()\n",
    "\n",
    "# Sauvegarder le mod√®le\n",
    "model.save_pretrained(f'{base_path}/models/finbert_sentiment')\n",
    "tokenizer.save_pretrained(f'{base_path}/models/finbert_sentiment')\n",
    "print(\"‚úÖ Mod√®le FinBERT sauvegard√©\")\n",
    "\n",
    "# Tester le mod√®le\n",
    "print(\"\\nüß™ Test du mod√®le:\")\n",
    "test_texts = [\n",
    "    \"Strong earnings report drives stock higher\",\n",
    "    \"Company faces bankruptcy concerns\",\n",
    "    \"Stable performance in challenging market\"\n",
    "]\n",
    "\n",
    "for text in test_texts:\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    outputs = model(**inputs)\n",
    "    predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "    sentiment_idx = torch.argmax(predictions).item()\n",
    "    sentiment_map = {0: \"N√©gatif\", 1: \"Neutre\", 2: \"Positif\"}\n",
    "    confidence = predictions[0][sentiment_idx].item()\n",
    "    print(f\"'{text[:50]}...' -> {sentiment_map[sentiment_idx]} ({confidence:.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELLULE 7: Entra√Ænement RAG Integrator\n",
    "# D√©finir le chemin de base si pas d√©j√† d√©fini\n",
    "if 'base_path' not in globals():\n",
    "    base_path = '/content/drive/MyDrive/AlphaBot_ML_Training'\n",
    "    os.makedirs(base_path, exist_ok=True)\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"üîç Entra√Ænement du RAG Integrator (Embeddings + FAISS)...\")\n",
    "\n",
    "# Cr√©er des documents de d√©monstration\n",
    "demo_documents = [\n",
    "    \"Apple Inc. is a technology company that designs, manufactures, and markets smartphones, personal computers, tablets, wearables, and accessories worldwide.\",\n",
    "    \"Alphabet Inc. provides various products and platforms in the United States, Europe, the Middle East, Africa, the Asia-Pacific, Canada, and Latin America.\",\n",
    "    \"Microsoft Corporation develops, licenses, and supports software, services, devices, and solutions worldwide.\",\n",
    "    \"Amazon.com, Inc. engages in the retail sale of consumer products and subscriptions in North America and internationally.\",\n",
    "    \"Tesla, Inc. designs, develops, manufactures, leases, and sells electric vehicles, and energy generation and storage systems.\",\n",
    "    \"Meta Platforms, Inc. develops products that enable people to connect and share with friends and family through mobile devices, personal computers, virtual reality headsets, and wearables.\",\n",
    "    \"NVIDIA Corporation provides graphics, computing, and networking solutions in the United States, Taiwan, China, and internationally.\",\n",
    "    \"Netflix, Inc. provides entertainment services worldwide. It offers a TV shows, movies, and games.\",\n",
    "    \"The Goldman Sachs Group, Inc. provides a range of financial services worldwide.\",\n",
    "    \"JPMorgan Chase & Co. provides financial services worldwide.\",\n",
    "    \"The Bank of America Corporation provides banking and financial products and services for consumers, small businesses, and institutions.\",\n",
    "    \"Walmart Inc. engages in the operation of retail, wholesale, and other units worldwide.\",\n",
    "    \"The Procter & Gamble Company provides branded consumer packaged goods worldwide.\",\n",
    "    \"The Coca-Cola Company is a beverage company.\",\n",
    "    \"PepsiCo, Inc. manufactures, markets, and distributes various beverages and convenient foods worldwide.\",\n",
    "    \"The Home Depot, Inc. operates as a home improvement retailer.\",\n",
    "    \"The Boeing Company operates in the aerospace industry.\",\n",
    "    \"The Exxon Mobil Corporation explores for and produces crude oil and natural gas.\",\n",
    "    \"The Chevron Corporation operates through its Upstream and Downstream segments.\",\n",
    "    \"The Johnson & Johnson engages in the research and development, manufacture, and sale of various products in the healthcare field.\",\n",
    "    \"The Visa Inc. operates as a payments technology company worldwide.\"\n",
    "]\n",
    "\n",
    "# Initialiser le mod√®le d'embeddings\n",
    "print(\"üì• Chargement du mod√®le d'embeddings...\")\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Cr√©er les embeddings\n",
    "print(\"üî¢ Cr√©ation des embeddings...\")\n",
    "document_embeddings = embedding_model.encode(demo_documents)\n",
    "print(f\"‚úÖ Embeddings cr√©√©s: {document_embeddings.shape}\")\n",
    "\n",
    "# Cr√©er l'index FAISS\n",
    "dimension = document_embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(document_embeddings)\n",
    "\n",
    "print(f\"‚úÖ Index FAISS cr√©√© avec {index.ntotal} documents\")\n",
    "\n",
    "# Sauvegarder l'index et les documents\n",
    "faiss.write_index(index, f'{base_path}/models/faiss_index.bin')\n",
    "\n",
    "import pickle\n",
    "with open(f'{base_path}/models/documents.pkl', 'wb') as f:\n",
    "    pickle.dump(demo_documents, f)\n",
    "\n",
    "print(\"üíæ Index et documents sauvegard√©s\")\n",
    "\n",
    "# Tester la recherche s√©mantique\n",
    "print(\"\\nüß™ Test de recherche s√©mantique:\")\n",
    "test_queries = [\n",
    "    \"electric vehicle company\",\n",
    "    \"social media platform\",\n",
    "    \"banking services\",\n",
    "    \"beverage company\",\n",
    "    \"technology giant\"\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    # Cr√©er l'embedding de la requ√™te\n",
    "    query_embedding = embedding_model.encode([query])[0].reshape(1, -1)\n",
    "    \n",
    "    # Rechercher les documents les plus similaires\n",
    "    k = 3\n",
    "    distances, indices = index.search(query_embedding, k)\n",
    "    \n",
    "    print(f\"\\nüîç Requ√™te: '{query}'\")\n",
    "    for i, (dist, idx) in enumerate(zip(distances[0], indices[0])):\n",
    "        doc_preview = demo_documents[idx][:80] + \"...\"\n",
    "        print(f\"  {i+1}. {doc_preview} (distance: {dist:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELLULE 8: Int√©gration et tests\n",
    "# D√©finir le chemin de base si pas d√©j√† d√©fini\n",
    "if 'base_path' not in globals():\n",
    "    base_path = '/content/drive/MyDrive/AlphaBot_ML_Training'\n",
    "    os.makedirs(base_path, exist_ok=True)\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "\n",
    "print(\"üîß Int√©gration des mod√®les et tests finaux...\")\n",
    "\n",
    "# Charger tous les mod√®les\n",
    "try:\n",
    "    # Charger le mod√®le LSTM\n",
    "    lstm_model = tf.keras.models.load_model(f'{base_path}/models/lstm_pattern_model.h5')\n",
    "    print(\"‚úÖ Mod√®le LSTM charg√©\")\n",
    "    \n",
    "    # Charger le mod√®le FinBERT\n",
    "    from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "    sentiment_tokenizer = AutoTokenizer.from_pretrained(f'{base_path}/models/finbert_sentiment')\n",
    "    sentiment_model = AutoModelForSequenceClassification.from_pretrained(f'{base_path}/models/finbert_sentiment')\n",
    "    print(\"‚úÖ Mod√®le FinBERT charg√©\")\n",
    "    \n",
    "    # Charger l'index FAISS et les documents\n",
    "    faiss_index = faiss.read_index(f'{base_path}/models/faiss_index.bin')\n",
    "    with open(f'{base_path}/models/documents.pkl', 'rb') as f:\n",
    "        documents = pickle.load(f)\n",
    "    print(\"‚úÖ Index FAISS et documents charg√©s\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erreur de chargement des mod√®les: {e}\")\n",
    "\n",
    "# Test d'int√©gration complet\n",
    "print(\"\\nüß™ Test d'int√©gration complet:\")\n",
    "\n",
    "# 1. Test de d√©tection de patterns\n",
    "print(\"\\n1. Test Pattern Detector:\")\n",
    "test_symbol = 'AAPL'\n",
    "if test_symbol in all_data:\n",
    "    test_data = all_data[test_symbol].tail(50)\n",
    "    \n",
    "    # Pr√©parer les features\n",
    "    prices = test_data['Close'].values\n",
    "    returns = np.diff(prices) / prices[:-1]\n",
    "    \n",
    "    seq_prices = prices[-30:] / prices[-30]\n",
    "    seq_returns = returns[-30:]\n",
    "    seq_volume = test_data['Volume'].values[-30:] / np.mean(test_data['Volume'].values[-30:])\n",
    "    \n",
    "    features = np.column_stack([seq_prices, seq_returns, seq_volume]).reshape(1, 30, 3)\n",
    "    \n",
    "    # Pr√©diction\n",
    "    prediction = lstm_model.predict(features, verbose=0)\n",
    "    predicted_class = np.argmax(prediction[0])\n",
    "    confidence = np.max(prediction[0])\n",
    "    \n",
    "    class_map = {0: 'DOWN', 1: 'SIDEWAYS', 2: 'UP'}\n",
    "    print(f\"   Pr√©diction: {class_map[predicted_class]} (confiance: {confidence:.2f})\")\n",
    "\n",
    "# 2. Test d'analyse de sentiment\n",
    "print(\"\\n2. Test Sentiment Analyzer:\")\n",
    "test_news = [\n",
    "    \"Apple announces breakthrough AI technology\",\n",
    "    \"Market volatility concerns investors\",\n",
    "    \"Tech sector shows steady growth\"\n",
    "]\n",
    "\n",
    "for news in test_news:\n",
    "    inputs = sentiment_tokenizer(news, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    outputs = sentiment_model(**inputs)\n",
    "    predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "    sentiment_idx = torch.argmax(predictions).item()\n",
    "    sentiment_map = {0: \"N√©gatif\", 1: \"Neutre\", 2: \"Positif\"}\n",
    "    confidence = predictions[0][sentiment_idx].item()\n",
    "    print(f\"   '{news[:40]}...' -> {sentiment_map[sentiment_idx]} ({confidence:.2f})\")\n",
    "\n",
    "# 3. Test RAG\n",
    "print(\"\\n3. Test RAG Integrator:\")\n",
    "rag_queries = [\n",
    "    \"Which company focuses on electric vehicles?\",\n",
    "    \"Find information about social media companies\",\n",
    "    \"What companies are in the banking sector?\"\n",
    "]\n",
    "\n",
    "for query in rag_queries:\n",
    "    query_embedding = embedding_model.encode([query])[0].reshape(1, -1)\n",
    "    k = 2\n",
    "    distances, indices = faiss_index.search(query_embedding, k)\n",
    "    \n",
    "    print(f\"   Query: '{query}'\")\n",
    "    for i, (dist, idx) in enumerate(zip(distances[0], indices[0])):\n",
    "        doc_preview = documents[idx][:60] + \"...\"\n",
    "        print(f\"     Result {i+1}: {doc_preview}\")\n",
    "\n",
    "# Cr√©er un rapport de performance\n",
    "performance_report = {\n",
    "    \"timestamp\": datetime.now().isoformat(),\n",
    "    \"models_trained\": {\n",
    "        \"lstm_pattern_detector\": {\n",
    "            \"status\": \"success\",\n",
    "            \"accuracy\": float(history.history['val_accuracy'][-1]) if 'history' in locals() else 0.0,\n",
    "            \"epochs_trained\": len(history.history['accuracy']) if 'history' in locals() else 0\n",
    "        },\n",
    "        \"finbert_sentiment\": {\n",
    "            \"status\": \"success\",\n",
    "            \"epochs_trained\": 3,\n",
    "            \"model_size\": \"base\"\n",
    "        },\n",
    "        \"rag_integrator\": {\n",
    "            \"status\": \"success\",\n",
    "            \"documents_indexed\": len(documents),\n",
    "            \"embedding_dimension\": document_embeddings.shape[1]\n",
    "        }\n",
    "    },\n",
    "    \"system_info\": {\n",
    "        \"tensorflow_version\": tf.__version__,\n",
    "        \"torch_version\": torch.__version__,\n",
    "        \"gpu_available\": len(tf.config.list_physical_devices('GPU')) > 0,\n",
    "        \"tpu_available\": 'tpu' in locals()\n",
    "    },\n",
    "    \"training_duration\": \"N/A\",\n",
    "    \"data_used\": {\n",
    "        \"symbols\": list(all_data.keys()),\n",
    "        \"total_data_points\": sum(len(data) for data in all_data.values())\n",
    "    }\n",
    "}\n",
    "\n",
    "# Sauvegarder le rapport\n",
    "report_path = f'{base_path}/exports/performance_report.json'\n",
    "with open(report_path, 'w') as f:\n",
    "    json.dump(performance_report, f, indent=2)\n",
    "\n",
    "print(f\"\\nüìä Rapport de performance sauvegard√©: {report_path}\")\n",
    "print(\"‚úÖ Entra√Ænement ML/DL termin√© avec succ√®s!\")\n",
    "\n",
    "# R√©sum√© final\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"üéâ R√âSUM√â DE L'ENTRA√éNEMENT\")\n",
    "print(\"=\"*50)\n",
    "print(f\"üìÖ Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"üß† Mod√®les entra√Æn√©s: 3 (LSTM, FinBERT, RAG)\")\n",
    "print(f\"üìä Donn√©es utilis√©es: {len(all_data)} symboles\")\n",
    "print(f\"üíæ Mod√®les sauvegard√©s dans: {base_path}/models/\")\n",
    "print(f\"üìã Rapport disponible: {report_path}\")\n",
    "print(\"\\nüöÄ Prochaines √©tapes:\")\n",
    "print(\"   1. Pousser les modifications sur GitHub\")\n",
    "print(\"   2. Lancer les tests locaux\")\n",
    "print(\"   3. D√©ployer en production\")\n",
    "print(\"=\"*50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}